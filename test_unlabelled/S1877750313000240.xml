<?xml version="1.0" encoding="UTF-8"?>
<xocs:doc xsi:schemaLocation="http://www.elsevier.com/xml/xocs/dtd http://schema.elsevier.com/dtds/document/fulltext/xcr/xocs-article.xsd" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.elsevier.com/xml/ja/dtd" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd"><xocs:meta><xocs:content-family>serial</xocs:content-family><xocs:content-type>JL</xocs:content-type><xocs:cid>280179</xocs:cid><xocs:ssids><xocs:ssid type="alllist">291210</xocs:ssid><xocs:ssid type="subj">291773</xocs:ssid><xocs:ssid type="subj">291791</xocs:ssid><xocs:ssid type="subj">291867</xocs:ssid><xocs:ssid type="subj">291869</xocs:ssid><xocs:ssid type="subj">291870</xocs:ssid><xocs:ssid type="subj">291873</xocs:ssid><xocs:ssid type="content">31</xocs:ssid><xocs:ssid type="oa">90</xocs:ssid></xocs:ssids><xocs:srctitle>Journal of Computational Science</xocs:srctitle><xocs:normalized-srctitle>JOURNALCOMPUTATIONALSCIENCE</xocs:normalized-srctitle><xocs:orig-load-date yyyymmdd="20130326">2013-03-26</xocs:orig-load-date><xocs:ew-transaction-id>2014-09-30T20:23:24</xocs:ew-transaction-id><xocs:eid>1-s2.0-S1877750313000240</xocs:eid><xocs:pii-formatted>S1877-7503(13)00024-0</xocs:pii-formatted><xocs:pii-unformatted>S1877750313000240</xocs:pii-unformatted><xocs:doi>10.1016/j.jocs.2013.03.002</xocs:doi><xocs:item-stage>S300</xocs:item-stage><xocs:item-version-number>S300.2</xocs:item-version-number><xocs:item-weight>FULL-TEXT</xocs:item-weight><xocs:hub-eid>1-s2.0-S1877750313X00055</xocs:hub-eid><xocs:timestamp yyyymmdd="20140930">2014-09-30T22:23:12.367785-04:00</xocs:timestamp><xocs:dco>0</xocs:dco><xocs:tomb>0</xocs:tomb><xocs:date-search-begin>20130901</xocs:date-search-begin><xocs:date-search-end>20130930</xocs:date-search-end><xocs:year-nav>2013</xocs:year-nav><xocs:indexeddate epoch="1364256000">2013-03-26T00:00:00Z</xocs:indexeddate><xocs:articleinfo>articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor highlightsabst primabst ref vitae</xocs:articleinfo><xocs:issns><xocs:issn-primary-formatted>1877-7503</xocs:issn-primary-formatted><xocs:issn-primary-unformatted>18777503</xocs:issn-primary-unformatted></xocs:issns><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>EPSRCPP</xocs:funding-body-id><xocs:crossmark is-crossmark="0">false</xocs:crossmark><xocs:vol-first>4</xocs:vol-first><xocs:volume-list><xocs:volume>4</xocs:volume></xocs:volume-list><xocs:iss-first>5</xocs:iss-first><xocs:issue-list><xocs:issue>5</xocs:issue></xocs:issue-list><xocs:vol-iss-suppl-text>Volume 4, Issue 5</xocs:vol-iss-suppl-text><xocs:sort-order>14</xocs:sort-order><xocs:first-fp>412</xocs:first-fp><xocs:last-lp>422</xocs:last-lp><xocs:pages><xocs:first-page>412</xocs:first-page><xocs:last-page>422</xocs:last-page></xocs:pages><xocs:cover-date-orig><xocs:start-date>201309</xocs:start-date></xocs:cover-date-orig><xocs:cover-date-text>September 2013</xocs:cover-date-text><xocs:cover-date-start>2013-09-01</xocs:cover-date-start><xocs:cover-date-end>2013-09-30</xocs:cover-date-end><xocs:cover-date-year>2013</xocs:cover-date-year><xocs:hub-sec><xocs:hub-sec-title>Regular papers</xocs:hub-sec-title></xocs:hub-sec><xocs:document-type>article</xocs:document-type><xocs:document-subtype>fla</xocs:document-subtype><xocs:copyright-line>Copyright Â© 2013 Elsevier B.V.</xocs:copyright-line><xocs:normalized-article-title>ANALYSINGMODELLINGPERFORMANCEHEMELBLATTICEBOLTZMANNSIMULATIONENVIRONMENT</xocs:normalized-article-title><xocs:normalized-first-auth-surname>GROEN</xocs:normalized-first-auth-surname><xocs:normalized-first-auth-initial>D</xocs:normalized-first-auth-initial><xocs:item-toc><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>1</xocs:item-toc-label><xocs:item-toc-section-title>Introduction</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>1.1</xocs:item-toc-label><xocs:item-toc-section-title>Overview of HemeLB</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>1.2</xocs:item-toc-label><xocs:item-toc-section-title>Related work</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2</xocs:item-toc-label><xocs:item-toc-section-title>Performance analysis</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.1</xocs:item-toc-label><xocs:item-toc-section-title>Performance of LB computations</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.2</xocs:item-toc-label><xocs:item-toc-section-title>Visualisation performance</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.3</xocs:item-toc-label><xocs:item-toc-section-title>Steering performance</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.4</xocs:item-toc-label><xocs:item-toc-section-title>Performance comparison with other codes</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3</xocs:item-toc-label><xocs:item-toc-section-title>Modelling the performance of HemeLB</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.1</xocs:item-toc-label><xocs:item-toc-section-title>Parameter extraction</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.1.1</xocs:item-toc-label><xocs:item-toc-section-title>Characterising maximum neighbour count</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.1.2</xocs:item-toc-label><xocs:item-toc-section-title>Characterising communication volume</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.1.3</xocs:item-toc-label><xocs:item-toc-section-title>Characterising load imbalances</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.2</xocs:item-toc-label><xocs:item-toc-section-title>LB calculations</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.3</xocs:item-toc-label><xocs:item-toc-section-title>Visualisation</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4</xocs:item-toc-label><xocs:item-toc-section-title>Model validation</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.1</xocs:item-toc-label><xocs:item-toc-section-title>Validation on HECToR</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.2</xocs:item-toc-label><xocs:item-toc-section-title>Validation on SuperMUC</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>5</xocs:item-toc-label><xocs:item-toc-section-title>Discussion</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:acknowledgment"><xocs:item-toc-section-title>Acknowledgements</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:bibliography"><xocs:item-toc-section-title>References</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc><xocs:references><xocs:ref-info refid="sbref0005"><xocs:ref-normalized-surname>MAZZEO</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>894</xocs:ref-first-fp><xocs:ref-last-lp>914</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0010"><xocs:ref-normalized-surname>MAZZEO</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>355</xocs:ref-first-fp><xocs:ref-last-lp>370</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0015"><xocs:ref-normalized-surname>SADIQ</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>3199</xocs:ref-first-fp><xocs:ref-last-lp>3219</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="oref0020"/><xocs:ref-info refid="oref0025"/><xocs:ref-info refid="sbref0030"><xocs:ref-normalized-surname>POHL</xocs:ref-normalized-surname><xocs:ref-pub-year>2004</xocs:ref-pub-year><xocs:ref-first-fp>21</xocs:ref-first-fp><xocs:ref-normalized-initial>T</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2004ACMIEEECONFERENCESUPERCOMPUTINGSC04</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>PERFORMANCEEVALUATIONPARALLELLARGESCALELATTICEBOLTZMANNAPPLICATIONSTHREESUPERCOMPUTINGARCHITECTURES</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sbref0035"><xocs:ref-normalized-surname>GELLER</xocs:ref-normalized-surname><xocs:ref-pub-year>2006</xocs:ref-pub-year><xocs:ref-first-fp>888</xocs:ref-first-fp><xocs:ref-last-lp>897</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0040"><xocs:ref-normalized-surname>WILLIAMS</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>1</xocs:ref-first-fp><xocs:ref-last-lp>12</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>2011INTERNATIONALCONFERENCEFORHIGHPERFORMANCECOMPUTINGNETWORKINGSTORAGEANALYSISSC</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>EXTRACTINGULTRASCALELATTICEBOLTZMANNPERFORMANCEVIAHIERARCHICALDISTRIBUTEDAUTOTUNING</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sbref0045"><xocs:ref-normalized-surname>SCHONHERR</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>3730</xocs:ref-first-fp><xocs:ref-last-lp>3743</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="oref0050"/><xocs:ref-info refid="sbref0055"><xocs:ref-normalized-surname>OBRECHT</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>3628</xocs:ref-first-fp><xocs:ref-last-lp>3638</xocs:ref-last-lp><xocs:ref-normalized-initial>C</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0060"><xocs:ref-normalized-surname>OBRECHT</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>295</xocs:ref-first-fp><xocs:ref-last-lp>303</xocs:ref-last-lp><xocs:ref-normalized-initial>C</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0065"><xocs:ref-normalized-surname>MYRE</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>332</xocs:ref-first-fp><xocs:ref-last-lp>350</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0070"><xocs:ref-normalized-surname>GRAY</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>167</xocs:ref-first-fp><xocs:ref-last-lp>174</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>ADVANCESINPARALLELCOMPUTINGVOL22APPLICATIONSTOOLSTECHNIQUESROADEXASCALECOMPUTING</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>LATTICEBOLTZMANNFORLARGESCALEGPUSYSTEMS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sbref0075"><xocs:ref-normalized-surname>DONATH</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>3</xocs:ref-first-fp><xocs:ref-last-lp>11</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0080"><xocs:ref-normalized-surname>HEUVELINE</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-first-fp>1071</xocs:ref-first-fp><xocs:ref-last-lp>1080</xocs:ref-last-lp><xocs:ref-normalized-initial>V</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0085"><xocs:ref-normalized-surname>FIETZ</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>818</xocs:ref-first-fp><xocs:ref-last-lp>829</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>EUROPAR2012PARALLELPROCESSINGVOL7484LECTURENOTESINCOMPUTERSCIENCE</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>OPTIMIZEDHYBRIDPARALLELLATTICEBOLTZMANNFLUIDFLOWSIMULATIONSCOMPLEXGEOMETRIES</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sbref0090"><xocs:ref-normalized-surname>HARVEY</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>056702</xocs:ref-first-fp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0095"><xocs:ref-normalized-surname>WILLIAMS</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-first-fp>762</xocs:ref-first-fp><xocs:ref-last-lp>777</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0100"><xocs:ref-normalized-surname>BIFERALE</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>1075</xocs:ref-first-fp><xocs:ref-last-lp>1082</xocs:ref-last-lp><xocs:ref-normalized-initial>L</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="oref0105"/><xocs:ref-info refid="oref0110"/><xocs:ref-info refid="sbref0115"><xocs:ref-normalized-surname>AXNER</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>4895</xocs:ref-first-fp><xocs:ref-last-lp>4911</xocs:ref-last-lp><xocs:ref-normalized-initial>L</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0120"><xocs:ref-normalized-surname>BHATNAGAR</xocs:ref-normalized-surname><xocs:ref-pub-year>1954</xocs:ref-pub-year><xocs:ref-first-fp>511</xocs:ref-first-fp><xocs:ref-last-lp>525</xocs:ref-last-lp><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0125"><xocs:ref-normalized-surname>HARTING</xocs:ref-normalized-surname><xocs:ref-pub-year>2004</xocs:ref-pub-year><xocs:ref-first-fp>1703</xocs:ref-first-fp><xocs:ref-last-lp>1722</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sbref0130"><xocs:ref-normalized-surname>CHOPARD</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>12</xocs:ref-first-fp><xocs:ref-normalized-initial>B</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>VEUROPEANCONFERENCECOMPUTATIONALFLUIDDYNAMICSECCOMASCFD2010VOL453</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>ALATTICEBOLTZMANNMODELINGBLOODFLOWINCEREBRALANEURYSMS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="oref0135"/><xocs:ref-info refid="sbref0140"><xocs:ref-normalized-surname>BERNASCHI</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-first-fp>1495</xocs:ref-first-fp><xocs:ref-last-lp>1502</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="oref0145"/><xocs:ref-info refid="oref0150"/><xocs:ref-info refid="oref0155"/></xocs:references><xocs:attachment-metadata-doc><xocs:attachment-set-type>item</xocs:attachment-set-type><xocs:pii-formatted>S1877-7503(13)00024-0</xocs:pii-formatted><xocs:pii-unformatted>S1877750313000240</xocs:pii-unformatted><xocs:eid>1-s2.0-S1877750313000240</xocs:eid><xocs:doi>10.1016/j.jocs.2013.03.002</xocs:doi><xocs:cid>280179</xocs:cid><xocs:timestamp>2014-09-30T22:23:12.367785-04:00</xocs:timestamp><xocs:path>/280179/1-s2.0-S1877750313X00055/1-s2.0-S1877750313000240/</xocs:path><xocs:cover-date-start>2013-09-01</xocs:cover-date-start><xocs:cover-date-end>2013-09-30</xocs:cover-date-end><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>EPSRCPP</xocs:funding-body-id><xocs:attachments><xocs:web-pdf><xocs:attachment-eid>1-s2.0-S1877750313000240-main.pdf</xocs:attachment-eid><xocs:filename>main.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:pdf-optimized>true</xocs:pdf-optimized><xocs:filesize>1434013</xocs:filesize><xocs:web-pdf-purpose>MAIN</xocs:web-pdf-purpose><xocs:web-pdf-page-count>11</xocs:web-pdf-page-count><xocs:web-pdf-images><xocs:web-pdf-image><xocs:attachment-eid>1-s2.0-S1877750313000240-main_1.png</xocs:attachment-eid><xocs:filename>main_1.png</xocs:filename><xocs:extension>png</xocs:extension><xocs:filesize>130202</xocs:filesize><xocs:pixel-height>849</xocs:pixel-height><xocs:pixel-width>656</xocs:pixel-width><xocs:attachment-type>IMAGE-WEB-PDF</xocs:attachment-type><xocs:pdf-page-num>1</xocs:pdf-page-num></xocs:web-pdf-image></xocs:web-pdf-images></xocs:web-pdf><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-si5.gif</xocs:attachment-eid><xocs:file-basename>si5</xocs:file-basename><xocs:filename>si5.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>526</xocs:filesize><xocs:pixel-height>16</xocs:pixel-height><xocs:pixel-width>169</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-si4.gif</xocs:attachment-eid><xocs:file-basename>si4</xocs:file-basename><xocs:filename>si4.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>763</xocs:filesize><xocs:pixel-height>34</xocs:pixel-height><xocs:pixel-width>204</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-si3.gif</xocs:attachment-eid><xocs:file-basename>si3</xocs:file-basename><xocs:filename>si3.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>477</xocs:filesize><xocs:pixel-height>34</xocs:pixel-height><xocs:pixel-width>97</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-si2.gif</xocs:attachment-eid><xocs:file-basename>si2</xocs:file-basename><xocs:filename>si2.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>1162</xocs:filesize><xocs:pixel-height>39</xocs:pixel-height><xocs:pixel-width>255</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-si1.gif</xocs:attachment-eid><xocs:file-basename>si1</xocs:file-basename><xocs:filename>si1.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>706</xocs:filesize><xocs:pixel-height>38</xocs:pixel-height><xocs:pixel-width>121</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr9.jpg</xocs:attachment-eid><xocs:file-basename>gr9</xocs:file-basename><xocs:filename>gr9.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>22928</xocs:filesize><xocs:pixel-height>272</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr8.jpg</xocs:attachment-eid><xocs:file-basename>gr8</xocs:file-basename><xocs:filename>gr8.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>15982</xocs:filesize><xocs:pixel-height>286</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr7.jpg</xocs:attachment-eid><xocs:file-basename>gr7</xocs:file-basename><xocs:filename>gr7.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>20193</xocs:filesize><xocs:pixel-height>277</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr6.jpg</xocs:attachment-eid><xocs:file-basename>gr6</xocs:file-basename><xocs:filename>gr6.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>28935</xocs:filesize><xocs:pixel-height>281</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr5.jpg</xocs:attachment-eid><xocs:file-basename>gr5</xocs:file-basename><xocs:filename>gr5.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>24322</xocs:filesize><xocs:pixel-height>276</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr4.jpg</xocs:attachment-eid><xocs:file-basename>gr4</xocs:file-basename><xocs:filename>gr4.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>20405</xocs:filesize><xocs:pixel-height>268</xocs:pixel-height><xocs:pixel-width>367</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr3.jpg</xocs:attachment-eid><xocs:file-basename>gr3</xocs:file-basename><xocs:filename>gr3.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>24549</xocs:filesize><xocs:pixel-height>271</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr2.jpg</xocs:attachment-eid><xocs:file-basename>gr2</xocs:file-basename><xocs:filename>gr2.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>35288</xocs:filesize><xocs:pixel-height>332</xocs:pixel-height><xocs:pixel-width>565</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr16.jpg</xocs:attachment-eid><xocs:file-basename>gr16</xocs:file-basename><xocs:filename>gr16.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>18159</xocs:filesize><xocs:pixel-height>282</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr15.jpg</xocs:attachment-eid><xocs:file-basename>gr15</xocs:file-basename><xocs:filename>gr15.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>24946</xocs:filesize><xocs:pixel-height>285</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr14.jpg</xocs:attachment-eid><xocs:file-basename>gr14</xocs:file-basename><xocs:filename>gr14.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>23545</xocs:filesize><xocs:pixel-height>285</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr13.jpg</xocs:attachment-eid><xocs:file-basename>gr13</xocs:file-basename><xocs:filename>gr13.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>23171</xocs:filesize><xocs:pixel-height>273</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr12.jpg</xocs:attachment-eid><xocs:file-basename>gr12</xocs:file-basename><xocs:filename>gr12.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>19630</xocs:filesize><xocs:pixel-height>289</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr11.jpg</xocs:attachment-eid><xocs:file-basename>gr11</xocs:file-basename><xocs:filename>gr11.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>22612</xocs:filesize><xocs:pixel-height>270</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr10.jpg</xocs:attachment-eid><xocs:file-basename>gr10</xocs:file-basename><xocs:filename>gr10.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>23036</xocs:filesize><xocs:pixel-height>270</xocs:pixel-height><xocs:pixel-width>376</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr1.jpg</xocs:attachment-eid><xocs:file-basename>gr1</xocs:file-basename><xocs:filename>gr1.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>35943</xocs:filesize><xocs:pixel-height>331</xocs:pixel-height><xocs:pixel-width>565</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx6.jpg</xocs:attachment-eid><xocs:file-basename>fx6</xocs:file-basename><xocs:filename>fx6.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>12659</xocs:filesize><xocs:pixel-height>151</xocs:pixel-height><xocs:pixel-width>113</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx5.jpg</xocs:attachment-eid><xocs:file-basename>fx5</xocs:file-basename><xocs:filename>fx5.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>15411</xocs:filesize><xocs:pixel-height>151</xocs:pixel-height><xocs:pixel-width>113</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx4.jpg</xocs:attachment-eid><xocs:file-basename>fx4</xocs:file-basename><xocs:filename>fx4.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>19643</xocs:filesize><xocs:pixel-height>151</xocs:pixel-height><xocs:pixel-width>113</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx3.jpg</xocs:attachment-eid><xocs:file-basename>fx3</xocs:file-basename><xocs:filename>fx3.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>11537</xocs:filesize><xocs:pixel-height>151</xocs:pixel-height><xocs:pixel-width>113</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx2.jpg</xocs:attachment-eid><xocs:file-basename>fx2</xocs:file-basename><xocs:filename>fx2.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>14147</xocs:filesize><xocs:pixel-height>151</xocs:pixel-height><xocs:pixel-width>113</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx1.jpg</xocs:attachment-eid><xocs:file-basename>fx1</xocs:file-basename><xocs:filename>fx1.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>17941</xocs:filesize><xocs:pixel-height>151</xocs:pixel-height><xocs:pixel-width>113</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr9.sml</xocs:attachment-eid><xocs:file-basename>gr9</xocs:file-basename><xocs:filename>gr9.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>5055</xocs:filesize><xocs:pixel-height>158</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr8.sml</xocs:attachment-eid><xocs:file-basename>gr8</xocs:file-basename><xocs:filename>gr8.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>3374</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>216</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr7.sml</xocs:attachment-eid><xocs:file-basename>gr7</xocs:file-basename><xocs:filename>gr7.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>4688</xocs:filesize><xocs:pixel-height>161</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr6.sml</xocs:attachment-eid><xocs:file-basename>gr6</xocs:file-basename><xocs:filename>gr6.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>6546</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr5.sml</xocs:attachment-eid><xocs:file-basename>gr5</xocs:file-basename><xocs:filename>gr5.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>5600</xocs:filesize><xocs:pixel-height>161</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr4.sml</xocs:attachment-eid><xocs:file-basename>gr4</xocs:file-basename><xocs:filename>gr4.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>5034</xocs:filesize><xocs:pixel-height>160</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr3.sml</xocs:attachment-eid><xocs:file-basename>gr3</xocs:file-basename><xocs:filename>gr3.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>5349</xocs:filesize><xocs:pixel-height>158</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr2.sml</xocs:attachment-eid><xocs:file-basename>gr2</xocs:file-basename><xocs:filename>gr2.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>5894</xocs:filesize><xocs:pixel-height>129</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr16.sml</xocs:attachment-eid><xocs:file-basename>gr16</xocs:file-basename><xocs:filename>gr16.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>4060</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>218</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr15.sml</xocs:attachment-eid><xocs:file-basename>gr15</xocs:file-basename><xocs:filename>gr15.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>4895</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>216</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr14.sml</xocs:attachment-eid><xocs:file-basename>gr14</xocs:file-basename><xocs:filename>gr14.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>4838</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>216</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr13.sml</xocs:attachment-eid><xocs:file-basename>gr13</xocs:file-basename><xocs:filename>gr13.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>4828</xocs:filesize><xocs:pixel-height>159</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr12.sml</xocs:attachment-eid><xocs:file-basename>gr12</xocs:file-basename><xocs:filename>gr12.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>4187</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>214</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr11.sml</xocs:attachment-eid><xocs:file-basename>gr11</xocs:file-basename><xocs:filename>gr11.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>5013</xocs:filesize><xocs:pixel-height>157</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr10.sml</xocs:attachment-eid><xocs:file-basename>gr10</xocs:file-basename><xocs:filename>gr10.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>5031</xocs:filesize><xocs:pixel-height>157</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-gr1.sml</xocs:attachment-eid><xocs:file-basename>gr1</xocs:file-basename><xocs:filename>gr1.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>6378</xocs:filesize><xocs:pixel-height>128</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx6.sml</xocs:attachment-eid><xocs:file-basename>fx6</xocs:file-basename><xocs:filename>fx6.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>10016</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>123</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx5.sml</xocs:attachment-eid><xocs:file-basename>fx5</xocs:file-basename><xocs:filename>fx5.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>9823</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>123</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx4.sml</xocs:attachment-eid><xocs:file-basename>fx4</xocs:file-basename><xocs:filename>fx4.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>10829</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>123</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx3.sml</xocs:attachment-eid><xocs:file-basename>fx3</xocs:file-basename><xocs:filename>fx3.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>9657</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>123</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx2.sml</xocs:attachment-eid><xocs:file-basename>fx2</xocs:file-basename><xocs:filename>fx2.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>10895</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>123</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S1877750313000240-fx1.sml</xocs:attachment-eid><xocs:file-basename>fx1</xocs:file-basename><xocs:filename>fx1.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>11559</xocs:filesize><xocs:pixel-height>164</xocs:pixel-height><xocs:pixel-width>123</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment></xocs:attachments></xocs:attachment-metadata-doc><xocs:refkeys><xocs:refkey3>GROENX2013X412</xocs:refkey3><xocs:refkey4lp>GROENX2013X412X422</xocs:refkey4lp><xocs:refkey4ai>GROENX2013X412XD</xocs:refkey4ai><xocs:refkey5>GROENX2013X412X422XD</xocs:refkey5></xocs:refkeys><xocs:open-access><xocs:oa-article-status is-open-access="1" is-open-archive="0">Full</xocs:oa-article-status><xocs:oa-access-effective-date>2014-04-04T12:52:05Z</xocs:oa-access-effective-date><xocs:oa-sponsor><xocs:oa-sponsor-type>FundingBody</xocs:oa-sponsor-type><xocs:oa-sponsor-name>Engineering and Physical Sciences Research Council</xocs:oa-sponsor-name></xocs:oa-sponsor><xocs:oa-user-license>http://creativecommons.org/licenses/by/3.0/</xocs:oa-user-license></xocs:open-access></xocs:meta><xocs:serial-item><article version="5.2" xml:lang="en" docsubtype="fla" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd"><item-info><jid>JOCS</jid><aid>193</aid><ce:pii>S1877-7503(13)00024-0</ce:pii><ce:doi>10.1016/j.jocs.2013.03.002</ce:doi><ce:copyright type="full-transfer" year="2013">Elsevier B.V.</ce:copyright></item-info><ce:floats><ce:figure id="fig0005"><ce:label>Fig. 1</ce:label><ce:caption id="cap0005"><ce:simple-para id="spar0045" view="all">Graphical overview of the bifurcation geometry in the HemeLB Setup Tool. We used this geometry to generate the Bifurcation and Large Bifurcation simulation domains. Inlets are shown by green planes, outlets by red planes. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.)</ce:simple-para></ce:caption><ce:link locator="gr1"/></ce:figure><ce:figure id="fig0010"><ce:label>Fig. 2</ce:label><ce:caption id="cap0010"><ce:simple-para id="spar0050" view="all">Graphical overview of the network geometry in the HemeLB Setup Tool. We used this geometry to generate the Network, Large Network and Small Network simulation domains.</ce:simple-para></ce:caption><ce:link locator="gr2"/></ce:figure><ce:figure id="fig0015"><ce:label>Fig. 3</ce:label><ce:caption id="cap0015"><ce:simple-para id="spar0055" view="all">Lattice site updates per second (SUPS) as a function of the number of cores used for simulations run on the HECToR Cray XE6 machine. We run simulations using each of the six simulation domains (Cylinder, Network, Bifurcation, Large Bifurcation, Large Network and Small Network).</ce:simple-para></ce:caption><ce:link locator="gr3"/></ce:figure><ce:figure id="fig0020"><ce:label>Fig. 4</ce:label><ce:caption id="cap0020"><ce:simple-para id="spar0060" view="all">Site updates per second (SUPS) per core averaged over all cores used in the simulation (excluding the one used for steering) as a function of the number of sites per core, for six LB problems.</ce:simple-para></ce:caption><ce:link locator="gr4"/></ce:figure><ce:figure id="fig0025"><ce:label>Fig. 5</ce:label><ce:caption id="cap0025"><ce:simple-para id="spar0065" view="all">Time spent to simulate 100 time steps as a function of the number of cores used for four settings: no snapshots and no images (blue), images only (green), snapshots only (black) and snapshots and images (red). We averaged the results from runs including any form of snapshot or image writing over three executions, and included a standard deviation error bar with each data point. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.)</ce:simple-para></ce:caption><ce:link locator="gr5"/></ce:figure><ce:figure id="fig0030"><ce:label>Fig. 6</ce:label><ce:caption id="cap0030"><ce:simple-para id="spar0070" view="all">Overhead in seconds relative to the simulation time without images rendered as a function of the number of LB steps per image rendered and written. The simulation with 0 images rendered took 31.4, 16.1 and 7.81<ce:hsp sp="0.25"/>s on respectively 512, 1024 and 2048 cores. We averaged the measurement of the runs over three executions. Error bars are the resulting standard deviations. The prediction of our performance model, presented in Section<ce:hsp sp="0.25"/><ce:cross-ref id="crf0005" refid="sec0075">3.3</ce:cross-ref>, is given by the thick solid red curve. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.)</ce:simple-para></ce:caption><ce:link locator="gr6"/></ce:figure><ce:figure id="fig0035"><ce:label>Fig. 7</ce:label><ce:caption id="cap0035"><ce:simple-para id="spar0075" view="all">Performance impact of running HemeLB with a connected steering client. We show results for 1024 and 2048 cores without steering client (plotted at frame-rate zero), with the client used only for image streaming (images) and with the client used both for image streaming and steering the HemeLB simulation (both).</ce:simple-para></ce:caption><ce:link locator="gr7"/></ce:figure><ce:figure id="fig0040"><ce:label>Fig. 8</ce:label><ce:caption id="cap0040"><ce:simple-para id="spar0080" view="all">Maximum number of neighbours as a function of the core count. Here we selected and fitted our model to the Network simulation domain, which has the highest neighbour count due to its sparseness. We ran 3 decomposition routines for each core count included in the figure, plotting the highest neighbour count separately for each instance. The neighbour count approximation used in our performance model is given by the dotted line.</ce:simple-para></ce:caption><ce:link locator="gr8"/></ce:figure><ce:figure id="fig0045"><ce:label>Fig. 9</ce:label><ce:caption id="cap0045"><ce:simple-para id="spar0085" view="all">Number of bytes sent per LB simulation step as a function of the number of lattice sites per core for Cylindrical geometries (measurements have been done using the Cylinder simulation domain). The fits we use for Cylindrical, Bifurcation and Network geometries in our performance model are given respectively by the red, blue and black dashed lines. Error bars show one standard deviation for the distribution across cores. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.)</ce:simple-para></ce:caption><ce:link locator="gr9"/></ce:figure><ce:figure id="fig0050"><ce:label>Fig. 10</ce:label><ce:caption id="cap0050"><ce:simple-para id="spar0090" view="all">As in <ce:cross-ref id="crf0010" refid="fig0045">Fig. 9</ce:cross-ref> but for the Bifurcation geometry (using the Bifurcation simulation domain).</ce:simple-para></ce:caption><ce:link locator="gr10"/></ce:figure><ce:figure id="fig0055"><ce:label>Fig. 11</ce:label><ce:caption id="cap0055"><ce:simple-para id="spar0095" view="all">As in <ce:cross-ref id="crf0015" refid="fig0045">Fig. 9</ce:cross-ref> but for the network geometry (using the network simulation domain).</ce:simple-para></ce:caption><ce:link locator="gr11"/></ce:figure><ce:figure id="fig0060"><ce:label>Fig. 12</ce:label><ce:caption id="cap0060"><ce:simple-para id="spar0100" view="all">Imbalance of the number of sites per core (i.e., our measure for calculation load imbalance) as a function of the number of cores for the three geometries. The value on the <ce:italic>y</ce:italic>-axis is the relative calculation overhead caused by load imbalance. These values are deterministic for a given core count and ParMETIS version.</ce:simple-para></ce:caption><ce:link locator="gr12"/></ce:figure><ce:figure id="fig0065"><ce:label>Fig. 13</ce:label><ce:caption id="cap0065"><ce:simple-para id="spar0105" view="all">Imbalance in the number of bytes sent (i.e., our measure for communication load imbalance) as a function of the number of cores for the three geometries. The value on the <ce:italic>y</ce:italic>-axis is the relative communication overhead caused by load imbalance. These values are deterministic for a given core count and ParMETIS version.</ce:simple-para></ce:caption><ce:link locator="gr13"/></ce:figure><ce:figure id="fig0070"><ce:label>Fig. 14</ce:label><ce:caption id="cap0070"><ce:simple-para id="spar0110" view="all">Wall-clock time spent to simulate 100 time steps as a function of the number of cores used for the Cylinder, Bifurcation and Large Bifurcation simulation domains. These validation runs were done using the HECToR supercomputer. Predictions by our performance model are indicated by the dashed lines.</ce:simple-para></ce:caption><ce:link locator="gr14"/></ce:figure><ce:figure id="fig0075"><ce:label>Fig. 15</ce:label><ce:caption id="cap0075"><ce:simple-para id="spar0115" view="all">As <ce:cross-ref id="crf0020" refid="fig0070">Fig. 14</ce:cross-ref>, but for the Network, Small Network and Large Network simulation domains. These validation runs were done using the HECToR supercomputer.</ce:simple-para></ce:caption><ce:link locator="gr15"/></ce:figure><ce:figure id="fig0080"><ce:label>Fig. 16</ce:label><ce:caption id="cap0080"><ce:simple-para id="spar0120" view="all">Wall-clock time spent to simulate 100 time steps as a function of the number of cores used for the Bifurcation simulation domains, using the SuperMUC supercomputer. Predictions by our performance model are indicated by the dashed line.</ce:simple-para></ce:caption><ce:link locator="gr16"/></ce:figure><ce:table id="tbl0005" frame="topbot" colsep="0" rowsep="0" xmlns="http://www.elsevier.com/xml/common/cals/dtd"><ce:label>Table 1</ce:label><ce:caption id="cap0085"><ce:simple-para id="spar0125" view="all">Overview of the simulation domains used in our experiments. The percentage of the simulated box that consists of active fluid sites is given by the fluid fraction. Non-active fluid sites do not count towards the number of lattice sites in the simulation.</ce:simple-para></ce:caption><tgroup cols="3" align="center"><colspec colnum="1" colname="col1" colsep="0" align="left"/><colspec colnum="2" colname="col2" colsep="0" align="char" char="."/><colspec colnum="3" colname="col3" colsep="0" align="char" char="."/><thead valign="top"><row rowsep="1"><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Name</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"># of lattice sites</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Fluid fraction</entry></row></thead><tbody><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Bifurcation</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">19,808,107</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">11%</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Cylinder</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">15,607,040</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">65%</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Network</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">18,836,545</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">5.1%</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Large Bifurcation</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">81,132,544</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">11%</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Large Network</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">44,650,496</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">5.1%</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Small Network</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">77,182</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">5.1%</entry></row></tbody></tgroup></ce:table><ce:table id="tbl0010" frame="topbot" colsep="0" rowsep="0" xmlns="http://www.elsevier.com/xml/common/cals/dtd"><ce:label>Table 2</ce:label><ce:caption id="cap0090"><ce:simple-para id="spar0130" view="all">Performance impact of running HemeLB with a connected steering client, simulating the Cylinder simulation domain using 1024 and 2048 cores. Here the mode is the method of running HemeLB, which can be without client (none), with the client used only for image streaming (images) or with the client used both for image streaming and steering the HemeLB simulation (both).</ce:simple-para></ce:caption><tgroup cols="6" align="center"><colspec colnum="1" colname="col1" colsep="0" align="char" char="."/><colspec colnum="2" colname="col2" colsep="0" align="left"/><colspec colnum="3" colname="col3" colsep="0" align="left"/><colspec colnum="4" colname="col4" colsep="0" align="char" char="."/><colspec colnum="5" colname="col5" colsep="0" align="char" char="."/><colspec colnum="6" colname="col6" colsep="0" align="char" char="."/><thead valign="top"><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>p</italic></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Mode</entry><entry namest="col3" nameend="col4" rowsep="1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Frame-rate (1/s)</entry><entry colname="col5" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">MSUPS per core</entry><entry colname="col6" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Mean LB steps per image</entry></row><row rowsep="1"><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd"/><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"/><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Requested</entry><entry colname="col4" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Achieved</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd"/><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd"/></row></thead><tbody><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1024</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">None</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">â</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">â</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.39</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">â</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1024</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Both</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">2.0</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2.0</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.28</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">41.5</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1024</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Images</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">2.0</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2.1</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.25</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">39.3</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1024</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Both</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">5.0</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">4.4</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.11</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">16.5</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1024</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Images</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">5.0</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">4.8</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.02</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">13.8</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1024</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Both</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Max</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">5.9</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">0.84</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">11.3</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1024</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Images</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Max</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">8.2</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">0.76</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">6.0</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2048</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">None</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">â</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">â</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.46</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">â</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2048</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Images</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">2.0</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2.1</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.26</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">77.2</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2048</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Both</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">2.0</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2.2</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.32</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">78.6</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2048</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Both</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">5.0</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">4.6</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.15</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">32.2</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2048</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Images</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">5.0</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">4.8</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">0.99</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">26.9</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2048</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Images</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Max</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">9.5</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">0.59</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">8.0</entry></row><row><entry colname="col1" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2048</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Both</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Max</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">10.6</entry><entry colname="col5" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">0.66</entry><entry colname="col6" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">8.1</entry></row></tbody></tgroup></ce:table><ce:table id="tbl0015" frame="topbot" colsep="0" rowsep="0" xmlns="http://www.elsevier.com/xml/common/cals/dtd"><ce:label>Table 3</ce:label><ce:caption id="cap0095"><ce:simple-para id="spar0135" view="all">Technical specifications of 12 LB simulations in our code comparison. We provide the name of the LB application used in the first column (including the source), followed by respectively the architecture used for the simulations and the number of cores used for the run.</ce:simple-para></ce:caption><tgroup cols="3" align="center"><colspec colnum="1" colname="col1" colsep="0" align="left"/><colspec colnum="2" colname="col2" colsep="0" align="left"/><colspec colnum="3" colname="col3" colsep="0" align="char" char="."/><thead valign="top"><row rowsep="1"><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Name</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Architecture (peak GFLOPS/core)</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Cores</entry></row></thead><tbody><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">HemeLB (Cylinder)</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">AMD Opteron 6276 (9.2)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">4096</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">HemeLB (Network)</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">AMD Opteron 6276 (9.2)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">32</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">HemeLB (Large Network)</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">AMD Opteron 6276 (9.2)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">512</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LB3Dv7 (Shamardin p.c.)</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">AMD Opteron 6276 (9.2)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">32</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LB3Dv7-3phase (Shamardin p.c.)</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">AMD Opteron 6276 (9.2)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">128</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LBMHD <cross-ref id="crf0025" refid="bib0040">[8]</cross-ref></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">AMD Opteron 1356 (9.2)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">8192</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LBMHD <cross-ref id="crf0030" refid="bib0040">[8]</cross-ref></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">AMD Opteron 6172 (8.4)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">49,152</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LUDWIG <cross-ref id="crf0035" refid="bib0070">[14]</cross-ref></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">AMD Opteron 6276 (9.2)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">384</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Palabos <cross-ref id="crf0040" refid="bib0135">[27]</cross-ref></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">AMD Opteron 8356 (9.2)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">4</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">HYPO4D (Groen p.c.)</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">BlueGene/P (3.4)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">512</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LBMHD <cross-ref id="crf0045" refid="bib0040">[8]</cross-ref></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">BlueGene/P (3.4)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">8196</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Palabos <cross-ref id="crf0050" refid="bib0135">[27]</cross-ref></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">BlueGene/P (3.4)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">256</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">MUPHY <cross-ref id="crf0055" refid="bib0140">[28]</cross-ref></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">BlueGene/L (2.8)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">32</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">OpenLB <cross-ref id="crf0060" refid="bib0085">[17]</cross-ref></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Intel Xeon X5355 (10.64)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">8</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Palabos <cross-ref id="crf0065" refid="bib0135">[27]</cross-ref></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Intel Xeon X5550 (10.64)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">4</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">HemeLB (Bifurcation, Sect <cross-ref id="crf0070" refid="sec0090">4.2</cross-ref>)</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Xeon E5-2680 (21.6)</entry><entry colname="col3" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">128</entry></row></tbody></tgroup></ce:table><ce:table id="tbl0020" frame="topbot" colsep="0" rowsep="0" xmlns="http://www.elsevier.com/xml/common/cals/dtd"><ce:label>Table 4</ce:label><ce:caption id="cap0100"><ce:simple-para id="spar0140" view="all">Performance comparison of 12 LB simulations in our code comparison. We provide the name of the LB application used in the first column, followed by the number of lattice sites for each run, the directionality, and the obtained performance per core. We give the per core calculation performance in millions of site updates per second (MSUPS). In the case of LBMHD we assumed 1300 FLOPs per lattice operation, as mentioned in Williams et al. <ce:cross-refs id="crfs0005" refid="bib0095 bib0040">[19,8]</ce:cross-refs>. Runs that use a sparse simulation domain are marked with an asterisk. Three-phase flow runs requires considerably more FLOPs per site update than single-phase flow runs. Here, the OpenLB run used a data set with a fluid fraction of 0.145. The Palabos run on the Opteron relied on shared memory and multi-threading, and did not use MPI.</ce:simple-para></ce:caption><tgroup cols="4" align="center"><colspec colnum="1" colname="col1" colsep="0" align="left"/><colspec colnum="2" colname="col2" colsep="0" align="char" char="."/><colspec colnum="3" colname="col3" colsep="0" align="left"/><colspec colnum="4" colname="col4" colsep="0" align="char" char="."/><thead valign="top"><row rowsep="1"><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Name</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"># of lattice sites</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Directional resolution</entry><entry colname="col4" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">MSUPS per core</entry></row></thead><tbody><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">HemeLB (Cylinder)</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">15,607,040</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q15</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.41</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">HemeLB* (Network)</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">18,836,545</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q15</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.20</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">HemeLB* (Large Network)</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">44,650,496</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q15</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1.19</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LB3Dv7</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">16,777,216</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q19</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">0.30</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LB3Dv7 (3-phase flow)</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">56,623,104</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q19</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">0.084</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LBMHD (w/ magnetism)</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">6,115,295,232</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q27</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">â¼1.42</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LBMHD (w/ magnetism)</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">28,311,552,000</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q27</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">â¼1.15</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LUDWIG</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">339,738,624</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q19</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">â¼3.0</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Palabos (shared memory)</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">64,481,201</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q19</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">2.55</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">HYPO4D</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">452,984,832</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q19</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">0.273</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LBMHD</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1,811,939,328</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q27</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">â¼0.5</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Palabos</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1,003,003,001</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q19</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">0.891</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">LUDWIG</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">16,777,214</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q19</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">0.087</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">MUPHY</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">262,144</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q19</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">0.529</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">OpenLB*</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">1,060,000</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q19</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">â¼0.4</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Palabos</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">64,481,201</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q19</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">7.87</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">HemeLB* (Bifurcation)</entry><entry colname="col2" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">19,808,107</entry><entry colname="col3" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">D3Q15</entry><entry colname="col4" align="char" char="." xmlns="http://www.elsevier.com/xml/common/dtd">3.49</entry></row></tbody></tgroup></ce:table><ce:table id="tbl0025" frame="topbot" colsep="0" rowsep="0" xmlns="http://www.elsevier.com/xml/common/cals/dtd"><ce:label>Table 5</ce:label><ce:caption id="cap0105"><ce:simple-para id="spar0145" view="all">List of constant values used in our performance model. The <ce:italic>Î»</ce:italic> value was measured using a <ce:monospace>ping</ce:monospace> test between nodes on HECToR. The <ce:italic>Ï</ce:italic> value was taken by dividing the MPI point-to-point bandwidth specification on the HECToR website <ce:cross-ref id="crf0075" refid="bib0145">[29]</ce:cross-ref> (at least 5 GB/s) by the number of cores per node (32).</ce:simple-para></ce:caption><tgroup cols="2" align="center"><colspec colnum="1" colname="col1" colsep="0" align="left"/><colspec colnum="2" colname="col2" colsep="0" align="left"/><thead valign="top"><row rowsep="1"><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Constant name</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Value</entry></row></thead><tbody><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>Ï</italic></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">1.57<hsp sp="0.25"/>Ã<hsp sp="0.25"/>10<sup loc="post">6</sup> SUPS per core (calc only)</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>Î»</italic></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">2.5<hsp sp="0.25"/>Ã<hsp sp="0.25"/>10<sup loc="post">â5</sup>[<italic>s</italic>]</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>Ï</italic></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">160<hsp sp="0.25"/>MB/s per core</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>Î¶</italic><inf loc="post">calc</inf></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">1.04</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>Î¶</italic><inf loc="post">comm</inf></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">1.5</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>O</italic><inf loc="post">monitoring</inf></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">0.06</entry></row></tbody></tgroup></ce:table><ce:table id="tbl0030" frame="topbot" colsep="0" rowsep="0" xmlns="http://www.elsevier.com/xml/common/cals/dtd"><ce:label>Table 6</ce:label><ce:caption id="cap0110"><ce:simple-para id="spar0150" view="all">List of fitting functions used in our performance model. Here the total number of lattice sites is given by <ce:italic>N</ce:italic> and the number of cores used by <ce:italic>p</ce:italic>.</ce:simple-para></ce:caption><tgroup cols="2" align="center"><colspec colnum="1" colname="col1" colsep="0" align="left"/><colspec colnum="2" colname="col2" colsep="0" align="left"/><thead valign="top"><row rowsep="1"><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Constant name</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Value</entry></row></thead><tbody><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>S</italic><inf loc="post">cylinder</inf></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">1898<hsp sp="0.25"/>Ã<hsp sp="0.25"/>(<italic>N</italic>/<italic>P</italic>)<sup loc="post">0.482719</sup> bytes per core per step</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>S</italic><inf loc="post">bifurcation</inf></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">942.0<hsp sp="0.25"/>Ã<hsp sp="0.25"/>(<italic>N</italic>/<italic>P</italic>)<sup loc="post">0.595517</sup> bytes per core per step</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>S</italic><inf loc="post">network</inf></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">1176<hsp sp="0.25"/>Ã<hsp sp="0.25"/>(<italic>N</italic>/<italic>P</italic>)<sup loc="post">0.613449</sup> bytes per core per step</entry></row></tbody></tgroup></ce:table><ce:table id="tbl0035" frame="topbot" colsep="0" rowsep="0" xmlns="http://www.elsevier.com/xml/common/cals/dtd"><ce:label>Table 7</ce:label><ce:caption id="cap0115"><ce:simple-para id="spar0155" view="all">List of constant values used in our performance model for SuperMUC. The <ce:italic>Î»</ce:italic> value was measured using a <ce:monospace>ping</ce:monospace> test between nodes on SuperMUC. The <ce:italic>Ï</ce:italic> value was taken by dividing the MPI point-to-point bandwidth specification on the SuperMUC website <ce:cross-ref id="crf0080" refid="bib0145">[29]</ce:cross-ref> (at least 5 GB/s) by the number of cores per node (16).</ce:simple-para></ce:caption><tgroup cols="2" align="center"><colspec colnum="1" colname="col1" colsep="0" align="left"/><colspec colnum="2" colname="col2" colsep="0" align="left"/><thead valign="top"><row rowsep="1"><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Constant name</entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">Value</entry></row></thead><tbody><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>Ï</italic></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">4.2<hsp sp="0.25"/>Ã<hsp sp="0.25"/>10<sup loc="post">6</sup> SUPS per core (calc only)</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>Î»</italic></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">1.83<hsp sp="0.25"/>Ã<hsp sp="0.25"/>10<sup loc="post">â4</sup>[<italic>s</italic>]</entry></row><row><entry colname="col1" align="left" xmlns="http://www.elsevier.com/xml/common/dtd"><italic>Ï</italic></entry><entry colname="col2" align="left" xmlns="http://www.elsevier.com/xml/common/dtd">500<hsp sp="0.25"/>MB/s per core</entry></row></tbody></tgroup></ce:table></ce:floats><head><ce:title id="tit0005">Analysing and modelling the performance of the HemeLB lattice-Boltzmann simulation environment</ce:title><ce:author-group id="aug0005"><ce:author id="aut0005" biographyid="vt0005"><ce:given-name>Derek</ce:given-name><ce:surname>Groen</ce:surname><ce:cross-ref id="crf0085" refid="cor0005"><ce:sup loc="post">â</ce:sup></ce:cross-ref><ce:e-address id="eadd0005" type="email">djgroennl@gmail.com</ce:e-address><ce:e-address id="eadd0010" type="email">d.groen@ucl.ac.uk</ce:e-address></ce:author><ce:author id="aut0010" biographyid="vt0010"><ce:given-name>James</ce:given-name><ce:surname>Hetherington</ce:surname></ce:author><ce:author id="aut0015" biographyid="vt0015"><ce:given-name>Hywel B.</ce:given-name><ce:surname>Carver</ce:surname></ce:author><ce:author id="aut0020" biographyid="vt0020"><ce:given-name>Rupert W.</ce:given-name><ce:surname>Nash</ce:surname></ce:author><ce:author id="aut0025" biographyid="vt0025"><ce:given-name>Miguel O.</ce:given-name><ce:surname>Bernabeu</ce:surname></ce:author><ce:author id="aut0030" biographyid="vt0030"><ce:given-name>Peter V.</ce:given-name><ce:surname>Coveney</ce:surname></ce:author><ce:affiliation id="aff0005"><ce:textfn>Centre for Computational Science, University College London, London, United Kingdom</ce:textfn><sa:affiliation><sa:organization>Centre for Computational Science, University College London</sa:organization><sa:city>London</sa:city><sa:country>United Kingdom</sa:country></sa:affiliation></ce:affiliation><ce:correspondence id="cor0005"><ce:label>â</ce:label><ce:text>Corresponding author.</ce:text></ce:correspondence></ce:author-group><ce:date-received day="29" month="8" year="2012"/><ce:date-revised day="22" month="1" year="2013"/><ce:date-accepted day="17" month="3" year="2013"/><ce:abstract id="abs0005" class="author-highlights" xml:lang="en" view="all"><ce:abstract-sec id="abst0005" view="all"><ce:section-title id="sect0005">Highlights</ce:section-title><ce:simple-para id="spar0005" view="all"><ce:list id="list0005"><ce:list-item id="listitem0005"><ce:label>â¢</ce:label><ce:para id="p0005" view="all">We investigate the scalability of the HemeLB blood flow simulation environment.</ce:para></ce:list-item><ce:list-item id="listitem0010"><ce:label>â¢</ce:label><ce:para id="p0010" view="all">HemeLB scales well up to 32,768 cores, even when applied to sparse geometries.</ce:para></ce:list-item><ce:list-item id="listitem0015"><ce:label>â¢</ce:label><ce:para id="p0015" view="all">HemeLB supports interactive steering and visualisation at a frame rate up to 10.6.</ce:para></ce:list-item><ce:list-item id="listitem0020"><ce:label>â¢</ce:label><ce:para id="p0020" view="all">Our performance model allows advance estimates of the simulation completion time.</ce:para></ce:list-item></ce:list></ce:simple-para></ce:abstract-sec></ce:abstract><ce:abstract id="abs0010" class="author" xml:lang="en" view="all"><ce:section-title id="sect0010">Abstract</ce:section-title><ce:abstract-sec id="abst0010" view="all"><ce:simple-para id="spar0010" view="all">We investigate the performance of the HemeLB lattice-Boltzmann simulator for cerebrovascular blood flow, aimed at providing timely and clinically relevant assistance to neurosurgeons. HemeLB is optimised for sparse geometries, supports interactive use, and scales well to 32,768 cores for problems with â¼81 million lattice sites. We obtain a maximum performance of 29.5 billion site updates per second, with only an 11% slowdown for highly sparse problems (5% fluid fraction). We present steering and visualisation performance measurements and provide a model which allows users to predict the performance, thereby determining how to run simulations with maximum accuracy within time constraints.</ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords id="kwd0005" class="keyword" xml:lang="en" view="all"><ce:section-title id="sect0015">Keywords</ce:section-title><ce:keyword id="kw0005"><ce:text>Lattice-Boltzmann</ce:text></ce:keyword><ce:keyword id="kw0010"><ce:text>Parallel computing</ce:text></ce:keyword><ce:keyword id="kw0015"><ce:text>High-performance computing</ce:text></ce:keyword><ce:keyword id="kw0020"><ce:text>Performance modelling</ce:text></ce:keyword></ce:keywords></head><body view="all"><ce:sections><ce:section id="sec0005" view="all"><ce:label>1</ce:label><ce:section-title id="sect0020">Introduction</ce:section-title><ce:para id="p0025" view="all">Recent progress in imaging and computing technologies has resulted in an increased adoption of computational methods in the life sciences. Using modern imaging methods, we are now able to scan the geometry of individual vessels within patients and map out potential sites for vascular malformations such as intracranial aneurysms. Likewise, recent increases in computational capacity and algorithmic improvements in simulation environments allow us to simulate blood flow in great detail. The HemeLB lattice-Boltzmann application <ce:cross-ref id="crf0090" refid="bib0005">[1]</ce:cross-ref> aims to combine these two developments, thereby allowing medical scans to be used as input for blood flow simulations. It also enables clinicians to run such simulations in real-time, providing runtime visualisation feedback as well as the ability to steer the simulation and its visualisation <ce:cross-ref id="crf0095" refid="bib0010">[2]</ce:cross-ref>. One principal long-term goal for HemeLB is to act as a production toolkit that provides both timely and clinically relevant assistance to surgeons. To achieve this we must not only perform extensive validation and testing for accuracy, reliability, usability and performance, but also ensure that the legal environment and the medical and computational infrastructure are made ready for such use cases <ce:cross-ref id="crf0100" refid="bib0015">[4]</ce:cross-ref>.</ce:para><ce:para id="p0030" view="all">In this work we investigate the performance aspects of the HemeLB environment, taking into account the core lattice-Boltzmann (LB) simulation code and the visualisation and steering facilities. We present performance measurements from a large number of runs using both sparse and non-sparse geometries and the overheads introduced by visualisation and steering. Medical doctors treating patients with intracranial aneurysms are frequently confronted with very short time scales for decision-making. For HemeLB to be useful in such environments, it is therefore not only essential that the code simulates close to real-time, but also that the length of a simulation can be reliably predicted in advance. We demonstrate that it is possible to accurately characterise CPU and network performance at low core counts and integrate this information into a model that predicts performance for arbitrary problem sizes and core counts.</ce:para><ce:section id="sec0010" view="all"><ce:label>1.1</ce:label><ce:section-title id="sect0025">Overview of HemeLB</ce:section-title><ce:para id="p0035" view="all">HemeLB is a massively parallel lattice-Boltzmann simulation framework that allows interactive use, eventually in a medical environment. Segmented angiographic data from patients can be read in by the HemeLB Setup Tool, which allows the user to indicate the geometric domain to be simulated using a graphical user interface. The geometry is then discretised into a regular grid, which is used to run HemeLB simulations. The core HemeLB code, written in C++, consists of a parallelised lattice-Boltzmann application which is optimised for sparse geometries such as vascular networks by use of indirect addressing. We precompute the addresses of neighbouring points within a single one-dimensional array instead of requiring that the points be stored in a dense, three-dimensional array. HemeLB also constructs a load-balanced domain decomposition at runtime, allowing the user to run simulations at varying core counts with the same simulation domain data. HemeLB is highly scalable due to a well-optimised communication strategy and the locality of interactions and communications in the parallelised lattice-Boltzmann algorithm. The File I/O operations are done in parallel using MPI-IO by a group of <ce:italic>reading processes</ce:italic>, which can be adjusted in size using a compile-time parameter.</ce:para><ce:para id="p0040" view="all">The HemeLB Steering Client is a light-weight tool that allows users to connect remotely to their HemeLB simulation, receive real-time visual feedback and modify parameters of the simulation at runtime. Here, the visualisations are generated on-site within HemeLB, using a hand-written ray-tracing kernel <ce:cross-ref id="crf0105" refid="bib0010">[2]</ce:cross-ref>. In our work we run HemeLB with the steering server code enabled. As a result, one core is reserved for steering purposes, whether or not a client is connected, and is thereby excluded from the LB calculations.</ce:para><ce:para id="p0045" view="all">HemeLB relies on ParMETIS version 4.0.2 <ce:cross-ref id="crf0110" refid="bib0020">[4]</ce:cross-ref> to perform its domain decomposition. It constructs an initial guess using a basic graph growing partitioning algorithm (see <ce:cross-ref id="crf0115" refid="bib0005">[1]</ce:cross-ref> for details), which it then passes to ParMETIS for optimisation using the <ce:monospace>ParMETIS_V3_PartKway()</ce:monospace> function. Constructing the initial guess requires less than a second of runtime in all cases, but the ParMETIS optimisation typically adds between 5 and 30<ce:hsp sp="0.25"/>s to the initialisation time. We discuss several technical aspects and performance implications of our decomposition routine in Section<ce:hsp sp="0.25"/><ce:cross-ref id="crf0120" refid="sec0050">3.1</ce:cross-ref>.</ce:para><ce:para id="p0050" view="all">HemeLB uses a coalesced asynchronous communication strategy to optimise its scalability <ce:cross-ref id="crf0125" refid="bib0025">[5]</ce:cross-ref>. This system bundles all communications for each iteration (e.g., exchanges required for the LB algorithm, steering and visualisations) into a single batch of non-blocking communication messages, one for each data exchange of non-zero size between a pair of processes in each direction. As a result, each iteration of HemeLB's core loop has only one <ce:monospace>MPI_Wait</ce:monospace> synchronisation point, minimising the latency overhead of HemeLB simulations. Communication of variable length data is spread over two iterations, the sizes being transferred during the first iteration while the actual exchange takes place during the second one.</ce:para><ce:para id="p0055" view="all">The coalesced communication system is also used for the phased broadcast and reduce operations which are required for the visualisation and steering functionality. Here HemeLB arranges the processes into an <ce:italic>n</ce:italic>-tree and, for broadcasts, sends data from one level of the tree to the level below over successive iterations. For reductions, data is sent up one level of the tree over successive iterations. Hence, both operations can take <ce:italic>O</ce:italic>(log(<ce:italic>p</ce:italic>)) iterations, for <ce:italic>p</ce:italic> cores. In this approach HemeLB does require some additional memory for communication buffers. Additionally, the responsiveness of the steering is constrained, as data arriving in the top-most node takes <ce:italic>O</ce:italic>(log(<ce:italic>p</ce:italic>)) iterations to be spread to all nodes.</ce:para></ce:section><ce:section id="sec0015" view="all"><ce:label>1.2</ce:label><ce:section-title id="sect0030">Related work</ce:section-title><ce:para id="p0060" view="all">A large number of researchers have investigated the performance aspects of various LB simulation codes over the past decade. These investigations have been done without real-time visualisation or steering enabled, and frequently use non-sparse geometries. We present a performance analysis of both sparse geometries and interactive usage modes in this work. Pohl et al. <ce:cross-ref id="crf0130" refid="bib0030">[6]</ce:cross-ref> compared the performance of LB codes across three supercomputer architectures, and concluded that the network and memory performance (bandwidth and latency) are dominant components in establishing a high LB calculation performance. Geller et al. <ce:cross-ref id="crf0135" refid="bib0035">[7]</ce:cross-ref> compared the performance of an LB code with that of several finite element and finite volume solvers, and deduced that LB offers superior efficiency in flow problems with small Mach numbers. Williams et al. <ce:cross-ref id="crf0140" refid="bib0040">[8]</ce:cross-ref> presented a hierarchical autotuning model for parallel lattice-Boltzmann, and report a performance increase of more than a factor 3 in their simulations. Several groups have considered the performance of LB solvers on general-purpose graphics processing unit (GPGPU) architectures. In these studies, they introduced a number of improvements, such as non-uniform grids <ce:cross-ref id="crf0145" refid="bib0045">[9]</ce:cross-ref>, more efficient memory management strategies <ce:cross-refs id="crfs0010" refid="bib0050 bib0055">[10,11]</ce:cross-refs> and LB codes which run across multiple GPUs <ce:cross-refs id="crfs0015" refid="bib0060 bib0065 bib0070">[12â14]</ce:cross-refs>. Other performance investigations include a comparison between different LB implementations <ce:cross-ref id="crf0150" refid="bib0075">[15]</ce:cross-ref>, hybrid parallelisations for multi-core architectures in general <ce:cross-refs id="crfs0020" refid="bib0080 bib0045 bib0085">[16,9,17]</ce:cross-refs> and performance analysis of LB codes on Cell processors <ce:cross-refs id="crfs0025" refid="bib0090 bib0095 bib0100">[18â20]</ce:cross-refs>.</ce:para><ce:para id="p0065" view="all">A few studies within the physiological domain are of special relevance to this work. These include a performance analysis of a blood-flow LB solver using a range of sparse and non-sparse geometries <ce:cross-ref id="crf0155" refid="bib0105">[21]</ce:cross-ref> and a performance prediction model for lattice-Boltzmann solvers <ce:cross-refs id="crfs0030" refid="bib0110 bib0115">[22,23]</ce:cross-refs>. This performance prediction model can be applied largely to our HemeLB application, although HemeLB uses a different decomposition technique and performs real-time rendering and visualisation tasks during the LB simulations. Mazzeo and Coveney <ce:cross-ref id="crf0160" refid="bib0005">[1]</ce:cross-ref> studied the scalability of an earlier version of HemeLB. However, the current performance characteristics of HemeLB are substantially enhanced due to numerous subsequent advances in the code, amongst others: an improved hierarchical, compressed file format; the use of ParMETIS to ensure good load-balance; the coalesced communication patterns to reduce the overhead of rendering; use of compile-time polymorphism to avoid virtual function calls in inner loops.</ce:para></ce:section></ce:section><ce:section id="sec0020" view="all"><ce:label>2</ce:label><ce:section-title id="sect0035">Performance analysis</ce:section-title><ce:para id="p0070" view="all">We benchmarked HemeLB using simulation domains based on three distinct geometries, a vascular network (see <ce:cross-ref id="crf0165" refid="fig0010">Fig. 2</ce:cross-ref><ce:float-anchor refid="fig0010"/>, used to generate three simulation domains), a bifurcation of vessels (see <ce:cross-ref id="crf0170" refid="fig0005">Fig. 1</ce:cross-ref><ce:float-anchor refid="fig0005"/>, used to generate two simulation domains) and a cylinder. Both the network and the bifurcation geometries are sections of an intracranial vasculature model that has been constructed from multiple rotational angiography scans of a patient with an intracranial aneurysm treated at the U.K. National Hospital for Neurology and Neurosurgery. The third and least sparse geometry is an artificially created cylinder. We present an overview of the simulation domains we generated and use in our runs in <ce:cross-ref id="crf0175" refid="tbl0005">Table 1</ce:cross-ref><ce:float-anchor refid="tbl0005"/>. We also provide a brief description of the sparseness of each generated simulation domain. Our runs were impulsively started, applying a pressure gradient across the simulation domain, using Nash in-outlet conditions (Nash et al., in preparation).</ce:para><ce:section id="sec0025" view="all"><ce:label>2.1</ce:label><ce:section-title id="sect0040">Performance of LB computations</ce:section-title><ce:para id="p0075" view="all">We have run blood flow simulations using the simulation domains listed in <ce:cross-ref id="crf0185" refid="tbl0005">Table 1</ce:cross-ref> using up to 32,768 cores on the HECToR Phase 3 supercomputer at EPCC in Edinburgh, United Kingdom. The HECToR machine is a Cray XE6 with 90,112 cores (2.3GHz AMD Opteron 6276), and has a peak performance of 9.2 GFLOP/s per core. Our simulations were done using a 15-directional lattice-Boltzmann kernel (D3Q15), the Lattice BhatnagarâGrossâKrook <ce:cross-ref id="crf0190" refid="bib0120">[24]</ce:cross-ref> model with simple bounce-back boundary conditions and a fixed physical viscosity of 0.004<ce:hsp sp="0.25"/>Pa<ce:hsp sp="0.25"/>s. We present the scalability results for all simulation domains in <ce:cross-ref id="crf0195" refid="fig0015">Fig. 3</ce:cross-ref><ce:float-anchor refid="fig0015"/>. We find that the small network simulation domain scales near-linearly up to 128 cores, despite consisting of only 77,182 lattice sites. All of the medium-sized simulation domains (Bifurcation, Cylinder and Network) scale linearly to 8192 cores. However, the communication overhead and load imbalance reduce the performance on higher core counts. The two largest simulation domains (Large Bifurcation and Large Network) show linear scaling from 512 cores up to 16,384 cores, and significant speedup to 32,768 cores, achieving a maximum performance of 29.5 billion site updates per second (SUPS). The performance obtained at 8192 cores for the medium-sized bifurcation corresponds to 419 timesteps per second, or 646 times slower than real-time for a maximum timestep as limited by incompressibility constraints. The maximum timestep here is estimated by the need to keep the Mach number below 0.05, using a typical blood velocity for vessels of this size of 25 cm/s. At this rate, it takes HemeLB 553<ce:hsp sp="0.25"/>s to simulate one heartbeat with a resolution of around 100 lattice points across a vessel diameter. We present the performance in SUPS per core as a function of the number of sites per core in <ce:cross-ref id="crf0200" refid="fig0020">Fig. 4</ce:cross-ref><ce:float-anchor refid="fig0020"/>, demonstrating that the SUPS per core is largely independent of other factors.</ce:para></ce:section><ce:section id="sec0030" view="all"><ce:label>2.2</ce:label><ce:section-title id="sect0045">Visualisation performance</ce:section-title><ce:para id="p0080" view="all">One of the features that sets HemeLB apart from many other LB codes is its ability to perform <ce:italic>in situ</ce:italic> rendering of the geometry at runtime <ce:cross-ref id="crf0205" refid="bib0010">[2]</ce:cross-ref>, using a parallelised ray-tracing algorithm. The communication needs of the ray-tracing algorithm have been combined with those of the main simulation algorithm, through the coalesced communication strategy, massively improving the scaling when rendering frames. The images rendered by HemeLB can either be stored on disk for future reference or they can be forwarded as a streaming visualisation to the steering client. In this section we present several simulations where we assess the overhead introduced by rendering images, as well as that introduced by writing snapshots of the simulation data. These snapshots store the hydrodynamic variables at each lattice point, recording all information of physical relevance which is useful for visualisation and post-processing. File I/O operations are done in HemeLB using a subset of all processes, the <ce:italic>reading group</ce:italic>. Within this work, we adopted a reading group size of 32 processes, or the number of processes used by HemeLB, whichever was smaller. We have run four types of simulations using the Bifurcation simulation domain, one with snapshots and image-rendering disabled, one where we write snapshots to disk (10 snapshots per 1000 time steps, with each snapshot being 604<ce:hsp sp="0.25"/>MB in size), one where we render and write images to disk (10 rendered images per 1000 time steps, with each image being 180kB in size) and one with both snapshots and images enabled. We have carried out the tests using 256, 512, 1024 and 2048 cores. We present our results in <ce:cross-ref id="crf0210" refid="fig0025">Fig. 5</ce:cross-ref><ce:float-anchor refid="fig0025"/>. Here the overhead for rendering and writing images is marginal, and adds no more than a few percent to the execution time in most cases. Simulations which have snapshot writing enabled are both considerably slower and have more variable performance, due to the high disk activity involved with snapshot writing. When snapshot writing is enabled, the overhead caused by image rendering is difficult to observe, as the standard deviation bars of the performance measurements with and without images overlap. When the simulation writes 10 snapshots over 1000<ce:hsp sp="0.25"/>LB steps, we observe an increase in the wall-clock time of â¼24<ce:hsp sp="0.25"/>s.</ce:para><ce:para id="p0085" view="all">We have also run several simulations of 1000<ce:hsp sp="0.25"/>LB steps where we render and write an image to disk every 5â200<ce:hsp sp="0.25"/>LB steps. The results for these runs (which were done using 1024 and 2048 cores) are given in <ce:cross-ref id="crf0215" refid="fig0030">Fig. 6</ce:cross-ref><ce:float-anchor refid="fig0030"/>. Without rendering the simulations took 31.4, 16.1 and 7.81<ce:hsp sp="0.25"/>s on 512, 1024 and 2048 cores respectively. We observe an overhead of less than 2<ce:hsp sp="0.25"/>s per 1000 LB steps if we render and write no more than 10 images during that period. However, the performance deteriorates somewhat when we write more images, with a maximum measured overhead of â¼6.5<ce:hsp sp="0.25"/>s. We also again observe some jitter in our results, for example in the 1024 core simulation that rendered one image every 50 steps, which we attribute to fluctuations in the file system performance of the machine. Rendering one image per 5 LB steps using 2048 cores corresponds to a frame rate of about 13.6<ce:hsp sp="0.25"/>frames/s, more than sufficient for smooth visualisations of the simulations in real time.</ce:para></ce:section><ce:section id="sec0035" view="all"><ce:label>2.3</ce:label><ce:section-title id="sect0050">Steering performance</ce:section-title><ce:para id="p0090" view="all">The previous subsection isolates the performance impact of the visualisation and rendering, with images written to disk. Here we study the performance impact of the HemeLB steering component, using the Cylinder simulation domain, where images are streamed over the network to a client. In this case, HemeLB produces images as described in Section<ce:hsp sp="0.25"/><ce:cross-ref id="crf0220" refid="sec0030">2.2</ce:cross-ref>, optionally limited by a maximum frame-rate per second. We also look at the performance impact of sending steering messages from the client to the HemeLB steering component. In order to obtain reproducible data, the steering client is set up with a scripted set of simulated user actions (orbiting the view point for image rendering). These results are presented in <ce:cross-ref id="crf0225" refid="tbl0010">Table 2</ce:cross-ref><ce:float-anchor refid="tbl0010"/> and in <ce:cross-ref id="crf0230" refid="fig0035">Fig. 7</ce:cross-ref><ce:float-anchor refid="fig0035"/> and were produced with the steering client running on the HECToR login node. For a frame-rate of 4.6<ce:hsp sp="0.25"/>frames/s, which is usable for scientific steering, with bidirectional communication between client and server, corresponding to 32 LB steps per rendered image, we observe an overhead of 28%.</ce:para></ce:section><ce:section id="sec0040" view="all"><ce:label>2.4</ce:label><ce:section-title id="sect0055">Performance comparison with other codes</ce:section-title><ce:para id="p0095" view="all">In this section we compare the performance of HemeLB with performance measurements of other LB codes as found in the literature. We gathered the number of million lattice site updates per second (MSUPS), the standard measure of LB performance, reported for other implementations. HemeLB is strongly optimised for efficiently handling sparse geometries while most codes are not, making like-for-like comparison difficult. The other applications may not be capable of simulating even moderate complexity domains, such as a cylinder, at all or only at the cost of allocating memory to non-fluid sites. Additionally, the directional resolution affects the number of calculations and memory accesses required per site update, as well as the presence of other special features, such as the additional presence of a D3Q15 magnetic field distribution model in LBMHD <ce:cross-ref id="crf0235" refid="bib0040">[8]</ce:cross-ref>. One particular example is LB3D <ce:cross-ref id="crf0240" refid="bib0125">[25]</ce:cross-ref> version 7, which calculates a number of additional forces, and is strongly optimised for multi-phase flow at the expense of single-phase flow performance. For LB3D we therefore included measurements for both single-phase and multi-phase flow performance.</ce:para><ce:para id="p0100" view="all">We provide the LB performance configurations and results for several well-known LB codes in <ce:cross-refs id="crfs0035" refid="tbl0015 tbl0020">Tables 3 and 4</ce:cross-refs><ce:float-anchor refid="tbl0015"/><ce:float-anchor refid="tbl0020"/>. The MSUPS per core results here are obtained by dividing the total number of lattice site updates by the product of time spent on LB iterations and the number of cores. From each literature source, we picked the result from the run that showed the best MSUPS per core while running on at least one full processor. In the case of HemeLB we picked the best result from the non-sparse Cylinder, as well as from the very sparse Network and Large Network simulation domains, which are the only measurements in the tables using sparse geometries.</ce:para><ce:para id="p0105" view="all">When we examine bulk flow only, the MSUPS per core performance of HemeLB is comparable with that achieved with LBMHD (although LBMHD calculates in 27 directions and HemeLB in 15), and about half of that achieved with Palabos on similar AMD Opteron architectures. The performance of HemeLB, however, is almost entirely preserved when using a very sparse simulation domain as HemeLB does not allocate memory or computational effort for non-active lattice sites, which are by definition common in sparse geometries. LBMHD has no known optimisations for sparse geometries while Palabos features a partial optimisation using the multi-block method <ce:cross-ref id="crf0245" refid="bib0130">[26]</ce:cross-ref>, of which we found no performance data using sparse geometries in the literature. The multi-block method is relatively inefficient because it allocates memory to some of the non-fluid sites and uses data structures that grow in complexity when off-lattice geometries are modelled more accurately. When a code is not designed for sparse geometries, additional optimisations (e.g., cache lookahead) are simpler to implement, hence the performance of a code which supports sparse geometries may not match that of codes which exploit such optimisations. Many of the benchmarks for other LB codes were performed on non-Opteron architectures, making it difficult if not impossible to do a one-on-one comparison. We nevertheless include these results for reference in the lower part of <ce:cross-ref id="crf0250" refid="tbl0015">Table 3</ce:cross-ref>.</ce:para></ce:section></ce:section><ce:section id="sec0045" view="all"><ce:label>3</ce:label><ce:section-title id="sect0060">Modelling the performance of HemeLB</ce:section-title><ce:section id="sec0050" view="all"><ce:label>3.1</ce:label><ce:section-title id="sect0065">Parameter extraction</ce:section-title><ce:para id="p0110" view="all">Before we are able to construct and apply the performance model, we need to extract a number of parameters specific to HemeLB. These parameters include the maximum neighbour count, the communication volume and the calculation and communication load imbalance.</ce:para><ce:section id="sec0055" view="all"><ce:label>3.1.1</ce:label><ce:section-title id="sect0070">Characterising maximum neighbour count</ce:section-title><ce:para id="p0115" view="all">Each process within HemeLB (except for the steering process) models a subsection of the simulation domain, and exchanges information with its neighbours. Here we characterise the maximum neighbour count (<ce:italic>k</ce:italic><ce:inf loc="post">max</ce:inf>), which is an approximation of the maximum number of neighbours a process has in a given simulation.</ce:para><ce:para id="p0120" view="all">To obtain the neighbour counts of each process, we have run the initialisation routine of HemeLB (without any simulation time steps) using 4â16,384 cores. The number of neighbours is dependent not only on core count but also on the geometry of the simulation domain, which makes it non-trivial to fully approximate it in the performance model. Instead, we choose to model close to a worst-case decomposition scenario, selecting the simulation domain with the highest neighbour count, and using the measured values there to determine <ce:italic>k</ce:italic><ce:inf loc="post">max</ce:inf> for any simulation domain. Because ParMETIS does not guarantee a reproducible decomposition, simulations may vary in neighbour counts for a given problem on a given number of cores. We therefore have repeated each measurement three times.</ce:para><ce:para id="p0125" view="all">We present our measurements of the maximum neighbour count as a function of the core count in <ce:cross-ref id="crf0255" refid="fig0040">Fig. 8</ce:cross-ref><ce:float-anchor refid="fig0040"/>. We find that the maximum neighbour count for the network geometry ranges from 7 on 8 cores, up to as high as 94 on 16,384 cores. Based on this data, we created a logarithmic fit, approximating <ce:italic>k</ce:italic><ce:inf loc="post">max</ce:inf> as:</ce:para><ce:para id="p0130" view="all"><ce:display><ce:formula id="eq0005"><ce:label>(1)</ce:label><mml:math altimg="si1.gif" overflow="scroll"><mml:msub><mml:mi>k</mml:mi><mml:mi mathvariant="normal">max</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>log</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mo>log</mml:mo><mml:mn>1.127</mml:mn></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></ce:formula></ce:display></ce:para></ce:section><ce:section id="sec0060" view="all"><ce:label>3.1.2</ce:label><ce:section-title id="sect0075">Characterising communication volume</ce:section-title><ce:para id="p0135" view="all">To model the communication performance of HemeLB we also need information on the amount of data communicated between processes at each step. As the domain decomposition in HemeLB is done at runtime <ce:cross-ref id="crf0260" refid="bib0005">[1]</ce:cross-ref>, we can only know the exact communication data volume after we have launched the simulation. To preserve the predictive power of the performance model, we have instead measured the communication volume for the three types of simulation domains across a range of core counts. After having performed the measurements, we fitted the data to a function of the form <ce:italic>ax</ce:italic><ce:sup loc="post"><ce:italic>b</ce:italic></ce:sup> to gain an approximate estimate while keeping the model relatively straightforward. We present our measurements of the communication volume and our fits for the cylindrical geometries in <ce:cross-ref id="crf0265" refid="fig0045">Fig. 9</ce:cross-ref><ce:float-anchor refid="fig0045"/>, for the bifurcation geometries in <ce:cross-ref id="crf0270" refid="fig0050">Fig. 10</ce:cross-ref><ce:float-anchor refid="fig0050"/>, and for the network geometries in <ce:cross-ref id="crf0275" refid="fig0055">Fig. 11</ce:cross-ref><ce:float-anchor refid="fig0055"/>. Here we find that the communication volume can differ by as much as a factor four between the domain types, making separate fits necessary for each type. We provide the exact formulation for each of the three fits in <ce:cross-ref id="crf0280" refid="tbl0030">Table 6</ce:cross-ref>. Interestingly, these scale with less than (<ce:italic>N</ce:italic>/<ce:italic>P</ce:italic>)<ce:sup loc="post">2/3</ce:sup> as one would expect with, for example, a decomposition into cubes. At large <ce:italic>N</ce:italic>/<ce:italic>P</ce:italic>, i.e., few processes, the sparseness implies that large parts of the single-process volumes are bordered by boundary sites, rather than lattice sites residing on neighbouring processes. We therefore observe a scaling of less than (<ce:italic>N</ce:italic>/<ce:italic>P</ce:italic>)<ce:sup loc="post">2/3</ce:sup>. In the limit of small (<ce:italic>N</ce:italic>/<ce:italic>P</ce:italic>), the measured communication volume does converge to the function <ce:italic>S</ce:italic><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>250<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>(<ce:italic>N</ce:italic>/<ce:italic>P</ce:italic>)<ce:sup loc="post">2/3</ce:sup> when the number of sites per process becomes lower, and the number of cores used higher in the simulations. Because the process-specific volumes are smaller here, the sparseness of the domain has a smaller effect on the measured (maximum) neighbour count.</ce:para></ce:section><ce:section id="sec0065" view="all"><ce:label>3.1.3</ce:label><ce:section-title id="sect0080">Characterising load imbalances</ce:section-title><ce:para id="p0140" view="all">When using sparse geometries, individual processes within HemeLB contain subsets of the simulated system with heterogeneous shapes and sizes. These differences result in two types of load imbalance during the parallel LB calculation: a calculation load imbalance and a communication load imbalance. To obtain a platform-independent measure of the load imbalance in HemeLB, we choose not to include timing results in this procedure. Instead, we examine the number of lattice sites on each core to determine the calculation imbalance and the number of bytes sent by each process to determine the communication imbalance. Both metrics are reproducible on different platforms when using the same version of ParMETIS (4.0.2), although some variations may occur due to the stochastic nature of the ParMETIS decomposition technique.</ce:para><ce:para id="p0145" view="all">In <ce:cross-ref id="crf0285" refid="fig0060">Fig. 12</ce:cross-ref><ce:float-anchor refid="fig0060"/> we show the measured <ce:italic>calculation load imbalance</ce:italic> for three geometries as a function of the core count. We determine this calculation load imbalance by dividing the maximum number of lattice sites on any core within this run by the average number of sites over all cores in the same run. HemeLB is optimised for calculation load balance and we find an imbalance of less than 1.04 for most core counts. However, the calculation load imbalance is higher for both very low and very high core counts. This contributes in part to the superlinear scaling of HemeLB at lower core counts in some cases, and reduces scalability when there are less than 2000 lattice sites per core. Based on these measurements, we assume a calculation load imbalance (<ce:italic>Î¶</ce:italic><ce:inf loc="post">calc</ce:inf>) of 1.04 in our performance model. In <ce:cross-ref id="crf0290" refid="fig0065">Fig. 13</ce:cross-ref><ce:float-anchor refid="fig0065"/> we present the <ce:italic>communication load imbalance</ce:italic>, which we measure by dividing the maximum number of bytes sent by a single core in the run by the average number of bytes sent per core. All the communication measurements are given per step. We observe a large and erratic imbalance in the communication sizes. The ParMETIS domain distribution algorithm co-optimises for both calculation load balance and communication minimisation. However, these results suggest that it does not optimise for communication balance. This communication imbalance does not strongly diminish the code performance unless the performance is already dominated by communication. Within our model we take an approximate average of our measurements, and assume a communication load imbalance (<ce:italic>Î¶</ce:italic><ce:inf loc="post">comm</ce:inf>) of 1.5.</ce:para></ce:section></ce:section><ce:section id="sec0070" view="all"><ce:label>3.2</ce:label><ce:section-title id="sect0085">LB calculations</ce:section-title><ce:para id="p0150" view="all">To model the performance of the core LB simulator code we propose a time-complexity model which is loosely based on <ce:cross-refs id="crfs0040" refid="bib0110 bib0115">[22,23]</ce:cross-refs> but largely simplified. We use a range of parameters which we derived in Section<ce:hsp sp="0.25"/><ce:cross-ref id="crf0295" refid="sec0050">3.1</ce:cross-ref>. In this model we approximate the overall time spent to perform a single simulation step in HemeLB (<ce:italic>T</ce:italic><ce:inf loc="post">step</ce:inf>), using</ce:para><ce:para id="p0155" view="all"><ce:display><ce:formula id="eq0010"><ce:label>(2)</ce:label><mml:math altimg="si2.gif" overflow="scroll"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">step</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>Î¶</mml:mi><mml:mi mathvariant="normal">calc</mml:mi></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">calc</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Î¶</mml:mi><mml:mi mathvariant="normal">comm</mml:mi></mml:msub><mml:mo>Ã</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">comm</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>1.0</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mi mathvariant="normal">monitoring</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></ce:formula></ce:display>where <ce:italic>T</ce:italic><ce:inf loc="post">calc</ce:inf> is the average calculation time per core, (<ce:italic>Î¶</ce:italic><ce:inf loc="post">calc</ce:inf>) is the calculation load imbalance constant, <ce:italic>T</ce:italic><ce:inf loc="post">comm</ce:inf> is the communication time per core, <ce:italic>Î¶</ce:italic><ce:inf loc="post">comm</ce:inf> is the communication load imbalance constant and <ce:italic>O</ce:italic><ce:inf loc="post">monitoring</ce:inf> is the fraction of time spent on monitoring overhead. Throughout our runs we found that â¼6% of the runtime is spent on monitoring, so we define <ce:italic>O</ce:italic><ce:inf loc="post">monitoring</ce:inf><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>0.06. The average calculation time per core is given by</ce:para><ce:para id="p0160" view="all"><ce:display><ce:formula id="eq0015"><ce:label>(3)</ce:label><mml:math altimg="si3.gif" overflow="scroll"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">calc</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>Ï</mml:mi></mml:mfrac></mml:math></ce:formula></ce:display>Here, the total number of lattice sites is given by <ce:italic>N</ce:italic> and the number of cores by <ce:italic>p</ce:italic>. We define the SUPS per core <ce:italic>Ï</ce:italic> as a platform dependent constant for the HECToR machine in <ce:cross-ref id="crf0300" refid="tbl0025">Table 5</ce:cross-ref><ce:float-anchor refid="tbl0025"/>. We measured <ce:italic>Ï</ce:italic> as an average from our HemeLB runs with 32 cores (1 node). The true SUPS capacity per core depends slightly on the number of sites per core, but is in almost all cases within 20% of this average value. We model the time spent on communications, <ce:italic>T</ce:italic><ce:inf loc="post">comm</ce:inf>, using <ce:cross-ref id="crf0305" refid="tbl0030">Table 6</ce:cross-ref><ce:float-anchor refid="tbl0030"/>.</ce:para><ce:para id="p0165" view="all"><ce:display><ce:formula id="eq0020"><ce:label>(4)</ce:label><mml:math altimg="si4.gif" overflow="scroll"><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">comm</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>Ã</mml:mo><mml:mi>Î»</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>x</mml:mi><mml:mo>&gt;</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mi>Ï</mml:mi></mml:mfrac><mml:mo>,</mml:mo></mml:math></ce:formula></ce:display>where <ce:italic>Î»</ce:italic> is the point-to-point latency of MPI communications between nodes in seconds, and <ce:italic>Ï</ce:italic> the average throughput capacity per core in bytes. We assume that the number of messages exchanged per time step increases with the number of processes and we model this as log(<ce:italic>P</ce:italic>). The number of bytes sent out per core per step (<ce:italic>S</ce:italic><ce:inf loc="post">&lt;<ce:italic>x</ce:italic>&gt;</ce:inf>) is dependent on the geometry used as well as the number of sites per core. We have provided basic fits for three geometry layouts with different sparsity (network, bifurcation and cylinder) in <ce:cross-ref id="crf0310" refid="tbl0025">Table 5</ce:cross-ref>. These fits are most accurate for simulations that have between 5000 and 200,000 sites per core.</ce:para></ce:section><ce:section id="sec0075" view="all"><ce:label>3.3</ce:label><ce:section-title id="sect0090">Visualisation</ce:section-title><ce:para id="p0170" view="all">When image rendering and writing is enabled in HemeLB, some overhead is introduced in the execution, and the new time per step (<ce:italic>T</ce:italic><ce:inf loc="post">step_<ce:italic>vis</ce:italic></ce:inf>) becomes</ce:para><ce:para id="p0175" view="all"><ce:display><ce:formula id="eq0025"><ce:label>(5)</ce:label><mml:math altimg="si5.gif" overflow="scroll"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="normal">step</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="italic">vis</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">step</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi mathvariant="normal">images</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:math></ce:formula></ce:display>where <ce:italic>T</ce:italic><ce:inf loc="post">images</ce:inf> is the overhead for rendering and writing images. Because our overhead measurements show a large variability, we use a straightforward fit rather than a detailed sub-model to approximate this overhead. Based on our measurements on 2048 cores, we have derived an approximate fit of <ce:italic>T</ce:italic><ce:inf loc="post">images</ce:inf><ce:hsp sp="0.25"/>=<ce:hsp sp="0.25"/>21.6<ce:italic>k</ce:italic><ce:sup loc="post">â0.76</ce:sup>, with <ce:italic>k</ce:italic> being the number of LB steps per rendered image. We provide a graphical overview of the approximation in <ce:cross-ref id="crf0315" refid="fig0030">Fig. 6</ce:cross-ref>.</ce:para></ce:section></ce:section><ce:section id="sec0080" view="all"><ce:label>4</ce:label><ce:section-title id="sect0095">Model validation</ce:section-title><ce:section id="sec0085" view="all"><ce:label>4.1</ce:label><ce:section-title id="sect0100">Validation on HECToR</ce:section-title><ce:para id="p0180" view="all">We have applied our performance model to calculate the theoretical execution times of the simulations we presented in Section<ce:hsp sp="0.25"/><ce:cross-ref id="crf0320" refid="sec0025">2.1</ce:cross-ref>. The predictions given by the model, as well as the measurements presented earlier, can be found in <ce:cross-ref id="crf0325" refid="fig0070">Fig. 14</ce:cross-ref><ce:float-anchor refid="fig0070"/> for the Cylinder, Bifurcation and Large Bifurcation simulation domains and in <ce:cross-ref id="crf0330" refid="fig0075">Fig. 15</ce:cross-ref><ce:float-anchor refid="fig0075"/> for the Network, Small Network and Large Network simulation domains. The predictions from our model are generally in agreement with our measurements, especially for the larger simulation domains. However, the model does not reproduce the superlinear speedup measured in the results. This is mainly because the model assumes a constant calculation and communication load imbalance, regardless of core count. In contract we measure relatively large calculation and communication load imbalances for runs on less than 32 cores (see <ce:cross-refs id="crfs0045" refid="fig0060 fig0065">Figs. 12 and 13</ce:cross-refs>). In this regime, the measured load imbalances are considerably higher than the ones assumed in our model, and the execution time is consequently slightly higher than in our model predictions.</ce:para></ce:section><ce:section id="sec0090" view="all"><ce:label>4.2</ce:label><ce:section-title id="sect0105">Validation on SuperMUC</ce:section-title><ce:para id="p0185" view="all">To test whether our performance model holds when applied to a different platform, we used a small part of an allocation arranged by MAPPER on the SuperMUC supercomputer at the Leibniz-Rechenzentrum in Garching, Germany. SuperMUC is an IBM System x iDataPlex machine with 147,456 compute cores and a total peak performance of 3.185 PFLOP/s (21.6 GFLOP/s per core). Each node has 16 cores, consists of two Intel Xeon E5-2680 CPUs, and is equipped with 32 GB of memory. The nodes are interconnected with an Infiniband FDR10 network, which divides the supercomputer into <ce:italic>islands</ce:italic>, each of which contains 8192 cores. We use this machine to run HemeLB simulations using the Bifurcation simulation domain, and to compare our measurements to our model predictions. We provide the list of constant values in <ce:cross-ref id="crf0335" refid="tbl0035">Table 7</ce:cross-ref><ce:float-anchor refid="tbl0035"/>. The values of <ce:italic>Î¶</ce:italic><ce:inf loc="post">calc</ce:inf>, <ce:italic>Î¶</ce:italic><ce:inf loc="post">comm</ce:inf> and <ce:italic>O</ce:italic><ce:inf loc="post">monitoring</ce:inf> are the same as those we used for HECToR, as these constants do not depend on the underlying architecture. We obtained a <ce:italic>Ï</ce:italic> value of 500<ce:hsp sp="0.25"/>MB/s per core through direct correspondence with LRZ, and measured a <ce:italic>Î»</ce:italic> of 1.83<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>10<ce:sup loc="post">â4</ce:sup> s by running a ping job between two nodes within the same island, and taking the average from 10 pings. As all small jobs on SuperMUC tend to get scheduled on the same island, it was unfortunately not possible to accurately measure the latency between islands. We obtained the value of <ce:italic>Ï</ce:italic> by running a very short HemeLB simulation on one node and extracting the calculation rate per core, excluding any communications or other overhead (4.2<ce:hsp sp="0.25"/>Ã<ce:hsp sp="0.25"/>10<ce:sup loc="post">6</ce:sup> SUPS).</ce:para><ce:para id="p0190" view="all">We present both our model predictions and our performance measurements in <ce:cross-ref id="crf0340" refid="fig0080">Fig. 16</ce:cross-ref><ce:float-anchor refid="fig0080"/>. Here we find that our simulation runs considerably faster on SuperMUC than on HECToR, achieving 3.49 MSUPS per core when using 128 cores, and 3.00 MSUPS per core when using 2048 cores. Our performance model accurately predicts the runtime for simulations up to 4096 cores, and matches the measured performance even more closely than in the HECToR validation tests. We have performed 2 runs using 8192 cores, one using one island, and on distributed over two islands. Our performance model predicts a time which is higher than the measured time for the single-island run. This may be because the <ce:monospace>ping</ce:monospace> test we used over the Infiniband has given us a somewhat higher <ce:italic>Î»</ce:italic> than the actual point-to-point latency of communications in the MPI layer. Communications between islands experience much higher latency and lower bandwidth (at a 4:1 ratio). As a result, the run performed using two islands is an order of magnitude slower than the run using one island. Understanding the performance across islands would require us to assess the latency and bandwidth characteristics of the inter-island links (which would require a special access mode), and incorporate these in a separate âinter-island" set of the parameters <ce:italic>Î»</ce:italic> and <ce:italic>Ï</ce:italic>.</ce:para></ce:section></ce:section><ce:section id="sec0095" view="all"><ce:label>5</ce:label><ce:section-title id="sect0110">Discussion</ce:section-title><ce:para id="p0195" view="all">We have presented a range of performance measurements for HemeLB, covering the lattice Boltzmann simulation and the visualisation and steering functionalities. For the models studied here, HemeLB scales near-linearly up to 32,768 cores, even for highly sparse simulation domains such as vascular networks. The application achieves close to maximum efficiency when using between 5000 and 500,000 lattice sites per core. We have shown that HemeLB can render and write images once every 100 timesteps with an overhead of â¼10%, sharing streaming images and control with a steering client at 4.6<ce:hsp sp="0.25"/>frames/s with a 28% overhead. We have demonstrated that it is possible to create a model which can estimate the run time of HemeLB simulations in advance. In our validation tests, we find that the predictions are between 70% and 140% of the actual runtime for simulations with at least 5000 lattice sites per core, and that our model remains largely accurate when applied to a different architecture (SuperMUC). We believe that accurate runtime predictions will be useful in the long term when HemeLB is used in a clinical setting, as doctors will be able to select the simulation with the highest accuracy that still meets the deadline for actual treatment.</ce:para><ce:para id="p0200" view="all">To improve the accuracy of HemeLB simulations, as part of the MAPPER project <ce:cross-ref id="crf0345" refid="bib0150">[30]</ce:cross-ref>, we have developed an intercommunication layer that allows the code to exchange boundary information with other simulation codes <ce:cross-ref id="crf0350" refid="bib0155">[31]</ce:cross-ref>. These couplings allow us to incorporate phenomena that are not resolved in HemeLB itself, such as the interaction between the blood flow in the intracranial vasculature and that in the rest of the human body. The boundary exchanges in these coupled simulations occur at high frequency and require rapid response times on both ends. The performance bottlenecks we have identified allow us to take the necessary steps to ensure an optimal performance for multiscale simulations using HemeLB.</ce:para><ce:para id="p0205" view="all">The envisaged use-case for HemeLB, involving deployment within a clinical setting, is made more difficult by typical queuing and scheduling policies for supercomputers. One important benefit of supercomputing lies in enabling results to be produced in a timely fashion. With typical scheduling policies, however, many codes produce results only after a lengthy wait in a queuing system, significantly reducing the value-added of the supercomputing resource relative to a long-running simulation on a smaller machine. The value of supercomputing is particularly apparent when using interactive visualisation and steering <ce:cross-ref id="crf0355" refid="bib0010">[2]</ce:cross-ref>, as this enables complex simulations to be investigated on timescales close to those of human engagement. However, this form of interaction is not possible without an advance reservation facility, enabling one to predict the time when one will be able to interact with the running simulation.</ce:para><ce:para id="p0210" view="all">In particular, in the clinical context, patients and physicians already interact within a complex resource availability and scheduling environment. In this case, advance reservation will be necessary to make computing resources available concurrently with medical equipment, physicians, and patient needs. Furthermore, when HemeLB is used in a clinical context, rapid access to computing resources will become a safety-critical factor. This requires not just advance reservation, but support for urgent computing <ce:cross-ref id="crf0360" refid="bib0015">[4]</ce:cross-ref>. For the use cases we envisage for HemeLB, an urgent computing mechanism will need to be available on supercomputing resources.</ce:para></ce:section></ce:sections><ce:acknowledgment id="ack0005" view="all"><ce:section-title id="sect0115">Acknowledgements</ce:section-title><ce:para id="p0215" view="all">We thank Dr. Lev Shamardin for his assistance in including the LB3D version 7 performance measurements and Dr. Timm Krueger for his valuable comments. Our research has received funding from the CRESTA and MAPPER projects within the <ce:grant-sponsor id="gs0005" xlink:type="simple" xlink:role="http://www.elsevier.com/xml/linking-roles/grant-sponsor">European Community's Seventh Framework Programme (ICT-2011.9.13)</ce:grant-sponsor> under Grant Agreements nos. <ce:grant-number refid="gs0005">287703</ce:grant-number> and <ce:grant-number refid="gs0005">261507</ce:grant-number>, and by <ce:grant-sponsor id="gs0010" xlink:type="simple" xlink:role="http://www.elsevier.com/xml/linking-roles/grant-sponsor">EPSRC</ce:grant-sponsor> grants <ce:grant-number refid="gs0010">EP/I017909/1</ce:grant-number> (<ce:inter-ref id="intr0005" xlink:href="http://www.2020science.net" xlink:type="simple">www.2020science.net</ce:inter-ref>) and <ce:grant-number refid="gs0010">EP/I034602/1</ce:grant-number>. This work made use of HECToR, the UK's national high-performance computing service, which is hosted by UoE HPCx Ltd at the University of Edinburgh, Cray Inc and NAG Ltd, and funded by the Office of Science and Technology through EPSRC's High End Computing Programme. In addition we have made use of the SuperMUC supercomputer, hosted by the Leibniz Rechenzentrum (LRZ) in Garching, Germany. We are grateful to the MAPPER consortium for providing an allocation on SuperMUC.</ce:para></ce:acknowledgment></body><tail view="all"><ce:bibliography id="bibl0005" view="all"><ce:section-title id="sect0120">References</ce:section-title><ce:bibliography-sec id="bibs0005" view="all"><ce:bib-reference id="bib0005"><ce:label>[1]</ce:label><sb:reference id="sbref0005"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.D.</ce:given-name><ce:surname>Mazzeo</ce:surname></sb:author><sb:author><ce:given-name>P.V.</ce:given-name><ce:surname>Coveney</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>HemeLB: a high performance parallel lattice-Boltzmann code for large scale fluid flow in complex geometries</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Computer Physics Communications</sb:maintitle></sb:title><sb:volume-nr>178</sb:volume-nr></sb:series><sb:issue-nr>12</sb:issue-nr><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>894</sb:first-page><sb:last-page>914</sb:last-page></sb:pages><ce:doi>10.1016/j.cpc.2008.02.013</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0010"><ce:label>[2]</ce:label><sb:reference id="sbref0010"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.D.</ce:given-name><ce:surname>Mazzeo</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Manos</ce:surname></sb:author><sb:author><ce:given-name>P.V.</ce:given-name><ce:surname>Coveney</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>In situ ray tracing and computational steering for interactive blood flow simulation</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Computer Physics Communications</sb:maintitle></sb:title><sb:volume-nr>181</sb:volume-nr></sb:series><sb:date>2010</sb:date></sb:issue><sb:pages><sb:first-page>355</sb:first-page><sb:last-page>370</sb:last-page></sb:pages><ce:doi>10.1016/j.cpc.2009.10.013</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0015"><ce:label>[4]</ce:label><sb:reference id="sbref0015"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.K.</ce:given-name><ce:surname>Sadiq</ce:surname></sb:author><sb:author><ce:given-name>M.D.</ce:given-name><ce:surname>Mazzeo</ce:surname></sb:author><sb:author><ce:given-name>S.J.</ce:given-name><ce:surname>Zasada</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Manos</ce:surname></sb:author><sb:author><ce:given-name>I.</ce:given-name><ce:surname>Stoica</ce:surname></sb:author><sb:author><ce:given-name>C.V.</ce:given-name><ce:surname>Gale</ce:surname></sb:author><sb:author><ce:given-name>S.J.</ce:given-name><ce:surname>Watson</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Kellam</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Brew</ce:surname></sb:author><sb:author><ce:given-name>P.V.</ce:given-name><ce:surname>Coveney</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Patient-specific simulation as a basis for clinical decision-making</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</sb:maintitle></sb:title><sb:volume-nr>366</sb:volume-nr></sb:series><sb:issue-nr>1878</sb:issue-nr><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>3199</sb:first-page><sb:last-page>3219</sb:last-page></sb:pages><ce:doi>10.1098/rsta.2008.0100</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0020"><ce:label>[4]</ce:label><ce:other-ref id="oref0020"><ce:textref>ParMETIS â <ce:inter-ref id="intr0010" xlink:href="http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview" xlink:type="simple">http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview</ce:inter-ref> (2012).</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0025"><ce:label>[5]</ce:label><ce:other-ref id="oref0025"><ce:textref>H.B. Carver, D. Groen, J. Hetherington, R.W. Nash, M.O. Bernabeu, P.V. Coveney, Coalesced communication: a design pattern for complex parallel scientific software, <ce:inter-ref id="intr0015" xlink:role="http://www.elsevier.com/xml/linking-roles/preprint" xlink:href="arxiv:1210.4400" xlink:type="simple">arXiv:1210.4400</ce:inter-ref>.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0030"><ce:label>[6]</ce:label><sb:reference id="sbref0030"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Pohl</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Deserno</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Thurey</ce:surname></sb:author><sb:author><ce:given-name>U.</ce:given-name><ce:surname>Rude</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Lammers</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Wellein</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Zeiser</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Performance evaluation of parallel large-scale lattice Boltzmann applications on three supercomputing architectures</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 2004 ACM/IEEE Conference on Supercomputing, SCâ04</sb:maintitle></sb:title><sb:conference>IEEE Computer Society, Washington, DC, USA</sb:conference><sb:date>2004</sb:date></sb:edited-book><sb:pages><sb:first-page>21</sb:first-page></sb:pages><ce:doi>10.1109/SC.2004.37</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0035"><ce:label>[7]</ce:label><sb:reference id="sbref0035"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Geller</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Krafczyk</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>TÃ¶lke</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Turek</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Hron</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Benchmark computations based on lattice-Boltzmann finite element and finite volume methods for laminar flows</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:title><sb:maintitle>Proceedings of the First International Conference for Mesoscopic Methods in Engineering and Science</sb:maintitle></sb:title><sb:series><sb:title><sb:maintitle>Computers and Fluids</sb:maintitle></sb:title><sb:volume-nr>35</sb:volume-nr></sb:series><sb:issue-nr>8â9</sb:issue-nr><sb:date>2006</sb:date></sb:issue><sb:pages><sb:first-page>888</sb:first-page><sb:last-page>897</sb:last-page></sb:pages><ce:doi>10.1016/j.compfluid.2005.08.009</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0040"><ce:label>[8]</ce:label><sb:reference id="sbref0040"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Williams</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Oliker</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Carter</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Shalf</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Extracting ultra-scale lattice Boltzmann performance via hierarchical and distributed auto-tuning</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC)</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book><sb:pages><sb:first-page>1</sb:first-page><sb:last-page>12</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0045"><ce:label>[9]</ce:label><sb:reference id="sbref0045"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>SchÃ¶nherr</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Kucher</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Geier</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Stiebler</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Freudiger</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Krafczyk</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Multi-thread implementations of the lattice Boltzmann method on non-uniform grids for CPUs and GPUs</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Computers &amp; Mathematics with Applications</sb:maintitle></sb:title><sb:volume-nr>61</sb:volume-nr></sb:series><sb:issue-nr>12</sb:issue-nr><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>3730</sb:first-page><sb:last-page>3743</sb:last-page></sb:pages><ce:doi>10.1016/j.camwa.2011.04.012</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0050"><ce:label>[10]</ce:label><ce:other-ref id="oref0050"><ce:textref>J. Habich, C. Feichtinger, H. Kostler, G. Hager, G. Wellein, Performance engineering for the lattice Boltzmann method on GPGPUs: architectural requirements and performance results, Computers &amp; Fluids, <ce:inter-ref id="intr0050" xlink:href="doi:10.1016/j.compfluid.2012.02.013" xlink:type="simple">http://dx.doi.org/10.1016/j.compfluid.2012.02.013</ce:inter-ref>, in press.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0055"><ce:label>[11]</ce:label><sb:reference id="sbref0055"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Obrecht</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Kuznik</ce:surname></sb:author><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Tourancheau</ce:surname></sb:author><sb:author><ce:given-name>J.J.</ce:given-name><ce:surname>Roux</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A new approach to the lattice Boltzmann method for graphics processing units</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:title><sb:maintitle>Proceedings of ICMMES-09, Mesoscopic Methods for Engineering and Science</sb:maintitle></sb:title><sb:series><sb:title><sb:maintitle>Computers and Mathematics with Applications</sb:maintitle></sb:title><sb:volume-nr>61</sb:volume-nr></sb:series><sb:issue-nr>12</sb:issue-nr><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>3628</sb:first-page><sb:last-page>3638</sb:last-page></sb:pages><ce:doi>10.1016/j.camwa.2010.01.054</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0060"><ce:label>[12]</ce:label><sb:reference id="sbref0060"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Obrecht</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Kuznik</ce:surname></sb:author><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Tourancheau</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Roux</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The TheLMA project: multi-GPU implementation of the lattice Boltzmann method</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>International Journal of High Performance Computing Applications</sb:maintitle></sb:title><sb:volume-nr>25</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>295</sb:first-page><sb:last-page>303</sb:last-page></sb:pages><ce:doi>10.1177/1094342011414745</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0065"><ce:label>[13]</ce:label><sb:reference id="sbref0065"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Myre</ce:surname></sb:author><sb:author><ce:given-name>S.D.C.</ce:given-name><ce:surname>Walsh</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Lilja</ce:surname></sb:author><sb:author><ce:given-name>M.O.</ce:given-name><ce:surname>Saar</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Performance analysis of single-phase, multiphase, and multicomponent lattice-Boltzmann fluid flow simulations on GPU clusters</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Concurrency and Computation: Practice and Experience</sb:maintitle></sb:title><sb:volume-nr>23</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>332</sb:first-page><sb:last-page>350</sb:last-page></sb:pages><ce:doi>10.1002/cpe.1645</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0070"><ce:label>[14]</ce:label><sb:reference id="sbref0070"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Gray</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Hart</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Richardson</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Stratford</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Lattice Boltzmann for large-scale GPU systems</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Advances in Parallel Computing, vol. 22: Applications, Tools and Techniques on the Road to Exascale Computing</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book><sb:pages><sb:first-page>167</sb:first-page><sb:last-page>174</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0075"><ce:label>[15]</ce:label><sb:reference id="sbref0075"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Donath</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Iglberger</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Zeiser</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Nitsure</ce:surname></sb:author><sb:author><ce:given-name>U.</ce:given-name><ce:surname>Rude</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Performance comparison of different parallel lattice Boltzmann implementations on multi-core multi-socket systems</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>International Journal of Computational Science and Engineering</sb:maintitle></sb:title><sb:volume-nr>4</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>3</sb:first-page><sb:last-page>11</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0080"><ce:label>[16]</ce:label><sb:reference id="sbref0080"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>V.</ce:given-name><ce:surname>Heuveline</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Krause</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>LÃ¤tt</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Towards a hybrid parallelization of lattice Boltzmann methods</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Computers and Mathematics with Applications</sb:maintitle></sb:title><sb:volume-nr>58</sb:volume-nr></sb:series><sb:issue-nr>5</sb:issue-nr><sb:date>2009</sb:date></sb:issue><sb:pages><sb:first-page>1071</sb:first-page><sb:last-page>1080</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0085"><ce:label>[17]</ce:label><sb:reference id="sbref0085"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Fietz</ce:surname></sb:author><sb:author><ce:given-name>M.J.</ce:given-name><ce:surname>Krause</ce:surname></sb:author><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Schulz</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Sanders</ce:surname></sb:author><sb:author><ce:given-name>V.</ce:given-name><ce:surname>Heuveline</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Optimized hybrid parallel lattice Boltzmann fluid flow simulations on complex geometries</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:editors><sb:editor><ce:given-name>C.</ce:given-name><ce:surname>Kaklamanis</ce:surname></sb:editor><sb:editor><ce:given-name>T.</ce:given-name><ce:surname>Papatheodorou</ce:surname></sb:editor><sb:editor><ce:given-name>P.</ce:given-name><ce:surname>Spirakis</ce:surname></sb:editor></sb:editors><sb:title><sb:maintitle>Euro-Par 2012 Parallel Processing, Vol. 7484 of Lecture Notes in Computer Science</sb:maintitle></sb:title><sb:date>2012</sb:date><sb:publisher><sb:name>Springer</sb:name><sb:location>Berlin/Heidelberg</sb:location></sb:publisher></sb:edited-book><sb:pages><sb:first-page>818</sb:first-page><sb:last-page>829</sb:last-page></sb:pages><ce:doi>10.1007/978-3-642-32820-6_81</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0090"><ce:label>[18]</ce:label><sb:reference id="sbref0090"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.J.</ce:given-name><ce:surname>Harvey</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>de Fabritiis</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Giupponi</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Accuracy of the lattice-Boltzmann method using the cell processor</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Physics Review E</sb:maintitle></sb:title><sb:volume-nr>78</sb:volume-nr></sb:series><sb:issue-nr>5</sb:issue-nr><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>056702</sb:first-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0095"><ce:label>[19]</ce:label><sb:reference id="sbref0095"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Williams</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Carter</ce:surname></sb:author><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Oliker</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Shalf</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Yelick</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Optimization of a lattice Boltzmann computation on state-of-the-art multicore platforms</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of Parallel and Distributed Computing</sb:maintitle></sb:title><sb:volume-nr>69</sb:volume-nr></sb:series><sb:issue-nr>9</sb:issue-nr><sb:date>2009</sb:date></sb:issue><sb:pages><sb:first-page>762</sb:first-page><sb:last-page>777</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0100"><ce:label>[20]</ce:label><sb:reference id="sbref0100"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Biferale</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Mantovani</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Pivanti</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Sbragaglia</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Scagliarini</ce:surname></sb:author><sb:author><ce:given-name>S.F.</ce:given-name><ce:surname>Schifano</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Toschi</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Tripiccione</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Lattice Boltzmann fluid-dynamics on the QPACE supercomputer</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Procedia CS</sb:maintitle></sb:title><sb:volume-nr>1</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2010</sb:date></sb:issue><sb:pages><sb:first-page>1075</sb:first-page><sb:last-page>1082</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0105"><ce:label>[21]</ce:label><ce:other-ref id="oref0105"><ce:textref>J. Bernsdorf, Simulation of complex flows and multi-physics with the lattice-Boltzmann method, Ph.D. thesis, University of Amsterdam (2008).</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0110"><ce:label>[22]</ce:label><ce:other-ref id="oref0110"><ce:textref>L. Axner, High performance computational hemodynamics with the lattice Boltzmann method, Ph.D. thesis, University of Amsterdam (2007).</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0115"><ce:label>[23]</ce:label><sb:reference id="sbref0115"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>L.</ce:given-name><ce:surname>Axner</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Bernsdorf</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Zeiser</ce:surname></sb:author><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Lammers</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Linxweiler</ce:surname></sb:author><sb:author><ce:given-name>A.G.</ce:given-name><ce:surname>Hoekstra</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Performance evaluation of a parallel sparse lattice Boltzmann solver</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Journal of Computational Physics</sb:maintitle></sb:title><sb:volume-nr>227</sb:volume-nr></sb:series><sb:issue-nr>10</sb:issue-nr><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>4895</sb:first-page><sb:last-page>4911</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0120"><ce:label>[24]</ce:label><sb:reference id="sbref0120"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.L.</ce:given-name><ce:surname>Bhatnagar</ce:surname></sb:author><sb:author><ce:given-name>E.P.</ce:given-name><ce:surname>Gross</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Krook</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A model for collision processes in gases. I. Small amplitude processes in charged and neutral one-component systems</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Physical Review</sb:maintitle></sb:title><sb:volume-nr>94</sb:volume-nr></sb:series><sb:date>1954</sb:date></sb:issue><sb:pages><sb:first-page>511</sb:first-page><sb:last-page>525</sb:last-page></sb:pages><ce:doi>10.1103/PhysRev.94.511</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0125"><ce:label>[25]</ce:label><sb:reference id="sbref0125"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Harting</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Venturoli</ce:surname></sb:author><sb:author><ce:given-name>P.V.</ce:given-name><ce:surname>Coveney</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Large-scale grid-enabled lattice Boltzmann simulations of complex fluid flow in porous media and under shear</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Philosophical Transactions of the Royal Society A</sb:maintitle></sb:title><sb:volume-nr>362</sb:volume-nr></sb:series><sb:issue-nr>1821</sb:issue-nr><sb:date>2004</sb:date></sb:issue><sb:pages><sb:first-page>1703</sb:first-page><sb:last-page>1722</sb:last-page></sb:pages><ce:doi>10.1098/rsta.2004.1402</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0130"><ce:label>[26]</ce:label><sb:reference id="sbref0130"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Chopard</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Lagrava</ce:surname></sb:author><sb:author><ce:given-name>O.</ce:given-name><ce:surname>Malaspinas</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Ouared</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Latt</ce:surname></sb:author><sb:author><ce:given-name>K.O.</ce:given-name><ce:surname>Lovblad</ce:surname></sb:author><sb:author><ce:given-name>V.</ce:given-name><ce:surname>Pereira-Mendes</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A lattice Boltzmann modeling of bloodflow in cerebral aneurysms</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:editors><sb:editor><ce:given-name>J.C.F.</ce:given-name><ce:surname>Pereira</ce:surname></sb:editor><sb:editor><ce:given-name>A.</ce:given-name><ce:surname>Seguira</ce:surname></sb:editor></sb:editors><sb:title><sb:maintitle>V: European Conference on Computational Fluid Dynamics, ECCOMAS CFD 2010, vol. 453</sb:maintitle></sb:title><sb:date>2010</sb:date></sb:edited-book><sb:pages><sb:first-page>12</sb:first-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0135"><ce:label>[27]</ce:label><ce:other-ref id="oref0135"><ce:textref>Palabos LBM Wiki â <ce:inter-ref id="intr0020" xlink:href="http://wiki.palabos.org/" xlink:type="simple">http://wiki.palabos.org/</ce:inter-ref> (2011).</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0140"><ce:label>[28]</ce:label><sb:reference id="sbref0140"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Bernaschi</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Melchionna</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Succi</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Fyta</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Kaxiras</ce:surname></sb:author><sb:author><ce:given-name>J.K.</ce:given-name><ce:surname>Sircar</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>MUPHY: a parallel MUlti PHYsics/scale code for high performance bio-fluidic simulations</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Computer Physics Communications</sb:maintitle></sb:title><sb:volume-nr>180</sb:volume-nr></sb:series><sb:date>2009</sb:date></sb:issue><sb:pages><sb:first-page>1495</sb:first-page><sb:last-page>1502</sb:last-page></sb:pages><ce:doi>10.1016/j.cpc.2009.04.001</ce:doi></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0145"><ce:label>[29]</ce:label><ce:other-ref id="oref0145"><ce:textref>HECToR â <ce:inter-ref id="intr0025" xlink:href="http://www.hector.ac.uk" xlink:type="simple">http://www.hector.ac.uk</ce:inter-ref> (2012).</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0150"><ce:label>[30]</ce:label><ce:other-ref id="oref0150"><ce:textref>MAPPER: Multiscale Applications on European e-Infrastructures â <ce:inter-ref id="intr0030" xlink:href="http://www.mapper-project.eu" xlink:type="simple">http://www.mapper-project.eu</ce:inter-ref> (2012).</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0155"><ce:label>[31]</ce:label><ce:other-ref id="oref0155"><ce:textref>D. Groen, J. Borgdorff, C. Bona-Casas, J. Hetherington, R.W. Nash, S.J. Zasada, I. Saverchenko, M. Mamonski, K. Kurowski, M.O. Bernabeu, A.G. Hoekstra, P.V. Coveney, Flexible composition and execution of high performance, high fidelity multiscale biomedical simulations, Interface Focus 3 (2) (2013), 20120087, <ce:inter-ref id="intr0035" xlink:href="doi:10.1098/rsfs.2012.0087" xlink:type="simple">http://dx.doi.org/10.1098/rsfs.2012.0087</ce:inter-ref>.</ce:textref></ce:other-ref></ce:bib-reference></ce:bibliography-sec></ce:bibliography><ce:biography id="vt0005" view="all"><ce:link locator="fx1"/><ce:simple-para id="spar0015" view="all"><ce:bold>Derek Groen</ce:bold> is a post-doctoral research associate in the CCS at University College London, specialised in multiscale simulation and parallel/distributed computing. He has worked with a wide range of applications, including those using lattice-Boltzmann, N-body and molecular dynamics methods, and participated in several EU-funded IT projects. He finished his PhD in 2010 at the University of Amsterdam, where he investigated the performance of N-body simulations run across geographically distributed (supercomputing) infrastructures. Derek currently works on constructing and testing multiscale simulation applications of cerebrovascular blood flow and clayâpolymer nanocomposites.</ce:simple-para></ce:biography><ce:biography id="vt0010" view="all"><ce:link locator="fx2"/><ce:simple-para id="spar0020" view="all"><ce:bold>James Hetherington</ce:bold> is a research software engineer, combining computational science research experience with professional software development skills. He helps UCL to produce maintainable, usable, well-tested scientific software which with a lasting impact, bringing software engineering best practice into computational research. At AMEE UK Limited, he developed systems to make it easier for organisations to understand their environmental impacts, creating AMEE Explorer, winner of a Best of What's New award in Popular Science Magazine in 2010. He has also worked on software for model management at the MathWorks, systems biology at UCL CoMPLEx, and particle physics at the Cavendish Laboratory, Cambridge.</ce:simple-para></ce:biography><ce:biography id="vt0015" view="all"><ce:link locator="fx3"/><ce:simple-para id="spar0025" view="all"><ce:bold>Hywel B. Carver</ce:bold> received his MEng degree in Information Engineering from King's College, Cambridge, with Distinction in 2008. He is now a second-year PhD student, interested in using high-performance computing as a predictive tool in solving clinical problems. His current focus is the application of lattice-Boltzmann to intracranial haemodynamics, as a means of predicting rupture risks in neurovascular aneurysms.</ce:simple-para></ce:biography><ce:biography id="vt0020" view="all"><ce:link locator="fx4"/><ce:simple-para id="spar0030" view="all"><ce:bold>Rupert W. Nash</ce:bold> completed his doctoral studies in the School of Physics at the University of Edinburgh in 2010, studying simple models of swimming particles using lattice-Boltzmann methods. He has since been at the Centre for Computational Science at UCL, working on computational haemodynamics. His research interests are focussed on lattice-Boltzmann methods and high performance computing.</ce:simple-para></ce:biography><ce:biography id="vt0025" view="all"><ce:link locator="fx5"/><ce:simple-para id="spar0035" view="all"><ce:bold>Miguel O. Bernabeu</ce:bold> received his DPhil in computational biology from the University of Oxford, UK, in 2011 and his MSc and BEng from the Universidad PolitÃ©cnica de Valencia, Spain, in 2005 and 2007. He is currently a 2020 Science Research Fellow at the Centre for Computational Science and CoMPLEX, University College London, UK. He has worked in various UK and EU projects developing HPC software for different aspects of cardiovascular modelling and simulation. His research interests include software engineering, parallel computing, and numerical methods with applications to cardiac electrophysiology and brain haemodynamics.</ce:simple-para></ce:biography><ce:biography id="vt0030" view="all"><ce:link locator="fx6"/><ce:simple-para id="spar0040" view="all"><ce:bold>Prof. Peter V. Coveney</ce:bold> holds a Chair in physical chemistry and is Director of the Centre for Computational Science, and an Honorary Professor in Computer Science. He is also professor adjunct within the Medical School at Yale University, and Director of the UCL Computational Life and Medical Sciences Network. Coveney is active in a broad area of interdisciplinary theoretical research including condensed matter physics and chemistry, materials science, life and medical sciences. He has published over 300 papers, books and edited works, including the acclaimed bestsellers The Arrow of Time and Frontiers of Complexity, both with Roger Highfield.</ce:simple-para></ce:biography></tail></article></xocs:serial-item></xocs:doc>