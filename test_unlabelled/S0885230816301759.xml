<full-text-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/article/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:bk="http://www.elsevier.com/xml/bk/dtd"><coredata><prism:url>http://api.elsevier.com/content/article/pii/S0885230816301759</prism:url><dc:identifier>doi:10.1016/j.csl.2016.04.003</dc:identifier><eid>1-s2.0-S0885230816301759</eid><prism:doi>10.1016/j.csl.2016.04.003</prism:doi><pii>S0885-2308(16)30175-9</pii><dc:title>Automatic sentence stress feedback for non-native English learners </dc:title><prism:publicationName>Computer Speech &amp; Language</prism:publicationName><prism:aggregationType>Journal</prism:aggregationType><prism:issn>08852308</prism:issn><prism:volume>41</prism:volume><prism:startingPage>29</prism:startingPage><prism:endingPage>42</prism:endingPage><prism:pageRange>29-42</prism:pageRange><dc:format>text/xml</dc:format><prism:coverDate>2017-01-31</prism:coverDate><prism:coverDisplayDate>January 2017</prism:coverDisplayDate><prism:copyright>Â© 2016 The Authors. Published by Elsevier Ltd.</prism:copyright><prism:publisher>The Authors. Published by Elsevier Ltd.</prism:publisher><dc:creator>Lee, Gary Geunbae</dc:creator><dc:creator>Lee, Ho-Young</dc:creator><dc:creator>Song, Jieun</dc:creator><dc:creator>Kim, Byeongchang</dc:creator><dc:creator>Kang, Sechun</dc:creator><dc:creator>Lee, Jinsik</dc:creator><dc:creator>Hwang, Hyosung</dc:creator><dc:description>AbstractThis paper proposes a sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined. This system provides non-native learners with feedback on sentence stress errors so that they can improve their English rhythm and fluency in a self-study setting. The sentence stress feedback system was devised to predict and detect the sentence stress of any practice sentence. The accuracy of the prediction and detection models was 96.6% and 84.1%, respectively. The stress feedback provision model offers positive or negative stress feedback for each spoken word by comparing the probability of the predicted stress pattern with that of the detected stress pattern. In an experiment that evaluated the educational effect of the proposed system incorporated in our CALL system, significant improvements in accentedness and rhythm were seen with the students who trained with our system but not with those in the control group.</dc:description><openaccess>1</openaccess><openaccessArticle>true</openaccessArticle><openaccessType>Full</openaccessType><openArchiveArticle>false</openArchiveArticle><openaccessSponsorName/><openaccessSponsorType>Author</openaccessSponsorType><openaccessUserLicense>http://creativecommons.org/licenses/by-nc-nd/4.0/</openaccessUserLicense><dcterms:subject>Sentence stress</dcterms:subject><dcterms:subject>Sentence stress feedback system</dcterms:subject><dcterms:subject>Stress prediction model</dcterms:subject><dcterms:subject>Stress detection model</dcterms:subject><dcterms:subject>Stress feedback provision model</dcterms:subject><dcterms:subject>CALL</dcterms:subject><link rel="self" href="http://api.elsevier.com/content/article/pii/S0885230816301759"/><link rel="scidir" href="http://www.sciencedirect.com/science/article/pii/S0885230816301759"/></coredata><objects><object ref="ycsla775-fig-0001" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="96" size="17192">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-ycsla775-fig-0001.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla775-fig-0002" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="168" height="163" size="23100">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-ycsla775-fig-0002.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla775-fig-0003" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="119" size="19348">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-ycsla775-fig-0003.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla775-fig-0004" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="69" size="11711">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-ycsla775-fig-0004.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla775-fig-0005" category="thumbnail" type="IMAGE-THUMBNAIL" multimediatype="GIF image file" mimetype="image/gif" width="219" height="147" size="9692">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-ycsla775-fig-0005.sml?httpAccept=%2A%2F%2A</object><object ref="ycsla775-fig-0001" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="731" height="319" size="78596">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-ycsla775-fig-0001.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla775-fig-0002" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="393" height="382" size="81972">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-ycsla775-fig-0002.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla775-fig-0003" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="731" height="397" size="92742">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-ycsla775-fig-0003.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla775-fig-0004" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="731" height="231" size="45415">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-ycsla775-fig-0004.jpg?httpAccept=%2A%2F%2A</object><object ref="ycsla775-fig-0005" category="standard" type="IMAGE-DOWNSAMPLED" multimediatype="JPEG image file" mimetype="image/jpeg" width="393" height="263" size="27719">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-ycsla775-fig-0005.jpg?httpAccept=%2A%2F%2A</object><object ref="si1" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="458" height="65" size="2828">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-si1.gif?httpAccept=%2A%2F%2A</object><object ref="si2" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="412" height="96" size="4323">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-si2.gif?httpAccept=%2A%2F%2A</object><object ref="si3" category="thumbnail" type="ALTIMG" multimediatype="GIF image file" mimetype="image/gif" width="669" height="60" size="4684">http://api.elsevier.com/content/object/eid/1-s2.0-S0885230816301759-si3.gif?httpAccept=%2A%2F%2A</object></objects><scopus-id>84976637294</scopus-id><scopus-eid>2-s2.0-84976637294</scopus-eid><link rel="abstract" href="http://api.elsevier.com/content/abstract/scopus_id/84976637294"/><originalText><xocs:doc xmlns:xoe="http://www.elsevier.com/xml/xoe/dtd" xsi:schemaLocation="http://www.elsevier.com/xml/xocs/dtd http://be-prod3a/schema/dtds/document/fulltext/xcr/xocs-article.xsd"><xocs:meta><xocs:content-family>serial</xocs:content-family><xocs:content-type>JL</xocs:content-type><xocs:cid>272453</xocs:cid><xocs:ssids><xocs:ssid type="alllist">291210</xocs:ssid><xocs:ssid type="subj">291718</xocs:ssid><xocs:ssid type="subj">291723</xocs:ssid><xocs:ssid type="subj">291743</xocs:ssid><xocs:ssid type="subj">291782</xocs:ssid><xocs:ssid type="subj">291874</xocs:ssid><xocs:ssid type="content">31</xocs:ssid><xocs:ssid type="oa">90</xocs:ssid></xocs:ssids><xocs:srctitle>Computer Speech &amp; Language</xocs:srctitle><xocs:normalized-srctitle>COMPUTERSPEECHLANGUAGE</xocs:normalized-srctitle><xocs:orig-load-date yyyymmdd="20160606">2016-06-06</xocs:orig-load-date><xocs:available-online-date yyyymmdd="20160606">2016-06-06</xocs:available-online-date><xocs:vor-load-date yyyymmdd="20160628">2016-06-28</xocs:vor-load-date><xocs:vor-available-online-date yyyymmdd="20160628">2016-06-28</xocs:vor-available-online-date><xocs:ew-transaction-id>2016-08-09T16:09:23</xocs:ew-transaction-id><xocs:eid>1-s2.0-S0885230816301759</xocs:eid><xocs:pii-formatted>S0885-2308(16)30175-9</xocs:pii-formatted><xocs:pii-unformatted>S0885230816301759</xocs:pii-unformatted><xocs:doi>10.1016/j.csl.2016.04.003</xocs:doi><xocs:item-stage>S250</xocs:item-stage><xocs:item-version-number>S250.1</xocs:item-version-number><xocs:item-weight>FULL-TEXT</xocs:item-weight><xocs:hub-eid>1-s2.0-S0885230816X00046</xocs:hub-eid><xocs:timestamp yyyymmdd="20160809">2016-08-09T11:51:28.352335-04:00</xocs:timestamp><xocs:dco>0</xocs:dco><xocs:tomb>0</xocs:tomb><xocs:date-search-begin>20170101</xocs:date-search-begin><xocs:date-search-end>20170131</xocs:date-search-end><xocs:year-nav>2017</xocs:year-nav><xocs:indexeddate epoch="1465226377">2016-06-06T15:19:37.102405Z</xocs:indexeddate><xocs:articleinfo>articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst primabst ref</xocs:articleinfo><xocs:issns><xocs:issn-primary-formatted>0885-2308</xocs:issn-primary-formatted><xocs:issn-primary-unformatted>08852308</xocs:issn-primary-unformatted></xocs:issns><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>NONE</xocs:funding-body-id><xocs:crossmark is-crossmark="1">true</xocs:crossmark><xocs:vol-first>41</xocs:vol-first><xocs:volume-list><xocs:volume>41</xocs:volume></xocs:volume-list><xocs:suppl>C</xocs:suppl><xocs:vol-iss-suppl-text>Volume 41</xocs:vol-iss-suppl-text><xocs:sort-order>2</xocs:sort-order><xocs:first-fp>29</xocs:first-fp><xocs:last-lp>42</xocs:last-lp><xocs:pages><xocs:first-page>29</xocs:first-page><xocs:last-page>42</xocs:last-page></xocs:pages><xocs:cover-date-orig><xocs:start-date>201701</xocs:start-date></xocs:cover-date-orig><xocs:cover-date-text>January 2017</xocs:cover-date-text><xocs:cover-date-start>2017-01-01</xocs:cover-date-start><xocs:cover-date-end>2017-01-31</xocs:cover-date-end><xocs:cover-date-year>2017</xocs:cover-date-year><xocs:document-type>article</xocs:document-type><xocs:document-subtype>fla</xocs:document-subtype><xocs:copyright-line>Â© 2016 The Authors. Published by Elsevier Ltd.</xocs:copyright-line><xocs:normalized-article-title>AUTOMATICSENTENCESTRESSFEEDBACKFORNONNATIVEENGLISHLEARNERS</xocs:normalized-article-title><xocs:normalized-first-auth-surname>LEE</xocs:normalized-first-auth-surname><xocs:normalized-first-auth-initial>G</xocs:normalized-first-auth-initial><xocs:item-toc><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>1</xocs:item-toc-label><xocs:item-toc-section-title>Introduction</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2</xocs:item-toc-label><xocs:item-toc-section-title>Materials</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.1</xocs:item-toc-label><xocs:item-toc-section-title>The Aix-MARSEC database for sentence stress prediction</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>2.2</xocs:item-toc-label><xocs:item-toc-section-title>The KLEAC database for sentence stress detection</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3</xocs:item-toc-label><xocs:item-toc-section-title>Methods</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.1</xocs:item-toc-label><xocs:item-toc-section-title>Overview of the sentence stress feedback system</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.2</xocs:item-toc-label><xocs:item-toc-section-title>Sentence stress prediction model</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.3</xocs:item-toc-label><xocs:item-toc-section-title>Sentence stress detection model</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>3.4</xocs:item-toc-label><xocs:item-toc-section-title>Sentence stress feedback provision model</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4</xocs:item-toc-label><xocs:item-toc-section-title>Results</xocs:item-toc-section-title><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.1</xocs:item-toc-label><xocs:item-toc-section-title>Evaluation of the sentence stress prediction and detection models</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.2</xocs:item-toc-label><xocs:item-toc-section-title>Determining feedback decision boundaries</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>4.3</xocs:item-toc-label><xocs:item-toc-section-title>Educational effect of the proposed system</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:sections"><xocs:item-toc-label>5</xocs:item-toc-label><xocs:item-toc-section-title>Discussion and conclusion</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:acknowledgment"><xocs:item-toc-section-title>Acknowledgments</xocs:item-toc-section-title></xocs:item-toc-entry><xocs:item-toc-entry ref-elem="ce:bibliography"><xocs:item-toc-section-title>References</xocs:item-toc-section-title></xocs:item-toc-entry></xocs:item-toc><xocs:references><xocs:ref-info refid="sr0010"><xocs:ref-normalized-surname>ABERCROMBIE</xocs:ref-normalized-surname><xocs:ref-pub-year>1964</xocs:ref-pub-year><xocs:ref-first-fp>216</xocs:ref-first-fp><xocs:ref-last-lp>222</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>INHONOURDANIELJONES</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>SYLLABLEQUANTITYENCLITICSINENGLISH</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0015"><xocs:ref-normalized-surname>ABERCROMBIE</xocs:ref-normalized-surname><xocs:ref-pub-year>1967</xocs:ref-pub-year><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>ELEMENTSGENERALPHONETICS</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0020"><xocs:ref-normalized-surname>ANDERSONHSIEH</xocs:ref-normalized-surname><xocs:ref-pub-year>1992</xocs:ref-pub-year><xocs:ref-first-fp>529</xocs:ref-first-fp><xocs:ref-last-lp>555</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0025"><xocs:ref-normalized-surname>BANG</xocs:ref-normalized-surname><xocs:ref-pub-year>2013</xocs:ref-pub-year><xocs:ref-first-fp>83</xocs:ref-first-fp><xocs:ref-last-lp>89</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSSLATE2013</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>AUTOMATICFEEDBACKSYSTEMFORENGLISHSPEAKINGINTEGRATINGPRONUNCIATIONPROSODYASSESSMENTS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0030"><xocs:ref-normalized-surname>BOERSMA</xocs:ref-normalized-surname><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0035"><xocs:ref-normalized-surname>BOLINGER</xocs:ref-normalized-surname><xocs:ref-pub-year>1958</xocs:ref-pub-year><xocs:ref-first-fp>109</xocs:ref-first-fp><xocs:ref-last-lp>149</xocs:ref-last-lp><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0040"><xocs:ref-normalized-surname>BOULADEMAREUIL</xocs:ref-normalized-surname><xocs:ref-pub-year>2006</xocs:ref-pub-year><xocs:ref-first-fp>247</xocs:ref-first-fp><xocs:ref-last-lp>267</xocs:ref-last-lp><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0045"><xocs:ref-normalized-surname>BRADLOW</xocs:ref-normalized-surname><xocs:ref-pub-year>1995</xocs:ref-pub-year><xocs:ref-first-fp>2299</xocs:ref-first-fp><xocs:ref-last-lp>2310</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0050"><xocs:ref-normalized-surname>BRILL</xocs:ref-normalized-surname><xocs:ref-pub-year>1992</xocs:ref-pub-year><xocs:ref-first-fp>112</xocs:ref-first-fp><xocs:ref-last-lp>116</xocs:ref-last-lp><xocs:ref-normalized-initial>E</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSTHIRDCONFERENCEAPPLIEDNATURALLANGUAGEPROCESSINGMORRISTOWN</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>ASIMPLERULEBASEDPARTSPEECHTAGGER</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0055"><xocs:ref-normalized-surname>CHANDEL</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-first-fp>711</xocs:ref-first-fp><xocs:ref-last-lp>716</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>2007IEEEWORKSHOPAUTOMATICSPEECHRECOGNITIONUNDERSTANDING</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>SENSEISPOKENLANGUAGEASSESSMENTFORCALLCENTERAGENTS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0060"><xocs:ref-normalized-surname>CHEN</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>1861</xocs:ref-first-fp><xocs:ref-last-lp>1864</xocs:ref-last-lp><xocs:ref-normalized-initial>L</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSINTERSPEECH2011</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>APPLYINGRHYTHMFEATURESAUTOMATICALLYASSESSNONNATIVESPEECH</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0065"><xocs:ref-normalized-surname>CHENG</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>1589</xocs:ref-first-fp><xocs:ref-last-lp>1592</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSINTERSPEECH2011</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>AUTOMATICASSESSMENTPROSODYINHIGHSTAKESENGLISHTESTS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0070"><xocs:ref-normalized-surname>DAUER</xocs:ref-normalized-surname><xocs:ref-pub-year>1983</xocs:ref-pub-year><xocs:ref-first-fp>51</xocs:ref-first-fp><xocs:ref-last-lp>62</xocs:ref-last-lp><xocs:ref-normalized-initial>R</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0075"><xocs:ref-normalized-surname>DERWING</xocs:ref-normalized-surname><xocs:ref-pub-year>1997</xocs:ref-pub-year><xocs:ref-first-fp>1</xocs:ref-first-fp><xocs:ref-last-lp>16</xocs:ref-last-lp><xocs:ref-normalized-initial>T</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0080"><xocs:ref-normalized-surname>DERWING</xocs:ref-normalized-surname><xocs:ref-pub-year>2003</xocs:ref-pub-year><xocs:ref-first-fp>1</xocs:ref-first-fp><xocs:ref-last-lp>18</xocs:ref-last-lp><xocs:ref-normalized-initial>T</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0085"><xocs:ref-normalized-surname>DERWING</xocs:ref-normalized-surname><xocs:ref-pub-year>1998</xocs:ref-pub-year><xocs:ref-first-fp>393</xocs:ref-first-fp><xocs:ref-last-lp>410</xocs:ref-last-lp><xocs:ref-normalized-initial>T</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0090"><xocs:ref-normalized-surname>ELLIOTT</xocs:ref-normalized-surname><xocs:ref-pub-year>1997</xocs:ref-pub-year><xocs:ref-first-fp>95</xocs:ref-first-fp><xocs:ref-last-lp>108</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0095"><xocs:ref-normalized-surname>GIMSON</xocs:ref-normalized-surname><xocs:ref-pub-year>1980</xocs:ref-pub-year><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>INTRODUCTIONPRONUNCIATIONENGLISH</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0100"><xocs:ref-normalized-surname>HAHN</xocs:ref-normalized-surname><xocs:ref-pub-year>2004</xocs:ref-pub-year><xocs:ref-first-fp>201</xocs:ref-first-fp><xocs:ref-last-lp>223</xocs:ref-last-lp><xocs:ref-normalized-initial>L</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0105"><xocs:ref-normalized-surname>HIRST</xocs:ref-normalized-surname><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0110"><xocs:ref-normalized-surname>HONIG</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-normalized-initial>F</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSSPEECHPROSODY</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>AUTOMATICASSESSMENTNONNATIVEPROSODYFORENGLISHL2</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0115"><xocs:ref-normalized-surname>IMOTO</xocs:ref-normalized-surname><xocs:ref-pub-year>2002</xocs:ref-pub-year><xocs:ref-first-fp>749</xocs:ref-first-fp><xocs:ref-last-lp>752</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSICSLPDENVER</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>MODELINGAUTOMATICDETECTIONENGLISHSENTENCESTRESSFORCOMPUTERASSISTEDENGLISHPROSODYLEARNINGSYSTEM</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0120"><xocs:ref-normalized-surname>ITO</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-first-fp>596</xocs:ref-first-fp><xocs:ref-last-lp>599</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSINTERSPEECH2009</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>EVALUATIONENGLISHINTONATIONBASEDCOMBINATIONMULTIPLEEVALUATIONSCORES</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0125"><xocs:ref-normalized-surname>JASSEM</xocs:ref-normalized-surname><xocs:ref-pub-year>1952</xocs:ref-pub-year><xocs:ref-first-fp>189</xocs:ref-first-fp><xocs:ref-last-lp>194</xocs:ref-last-lp><xocs:ref-normalized-initial>W</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0130"><xocs:ref-normalized-surname>JASSEM</xocs:ref-normalized-surname><xocs:ref-pub-year>1999</xocs:ref-pub-year><xocs:ref-first-fp>33</xocs:ref-first-fp><xocs:ref-last-lp>50</xocs:ref-last-lp><xocs:ref-normalized-initial>W</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0135"><xocs:ref-normalized-surname>JEON</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-first-fp>4565</xocs:ref-first-fp><xocs:ref-last-lp>4568</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSIEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSING2009</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>AUTOMATICPROSODICEVENTSDETECTIONUSINGSYLLABLEBASEDACOUSTICSYNTACTICFEATURES</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0140"><xocs:ref-normalized-surname>JONES</xocs:ref-normalized-surname><xocs:ref-pub-year>1972</xocs:ref-pub-year><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>OUTLINEENGLISHPHONETICS</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0145"><xocs:ref-normalized-surname>JUN</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>53</xocs:ref-first-fp><xocs:ref-last-lp>66</xocs:ref-last-lp><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS1STPRONUNCIATIONINSECONDLANGUAGELEARNINGTEACHINGCONFERENCE</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>FACTORSINRATERSPERCEPTIONSCOMPREHENSIBILITYACCENTEDNESS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0150"><xocs:ref-normalized-surname>KANG</xocs:ref-normalized-surname><xocs:ref-pub-year>2012</xocs:ref-pub-year><xocs:ref-first-fp>432</xocs:ref-first-fp><xocs:ref-last-lp>437</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSIEEEWORKSHOPSPOKENLANGUAGETECHNOLOGYSLT2012MIAMI</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>AUTOMATICPITCHACCENTFEEDBACKSYSTEMFORENGLISHLEARNERSADAPTATIONENGLISHCORPUSSPOKENBYKOREANS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0155"><xocs:ref-normalized-surname>KINGDON</xocs:ref-normalized-surname><xocs:ref-pub-year>1958</xocs:ref-pub-year><xocs:ref-normalized-initial>D</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>GROUNDWORKENGLISHSTRESS</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0160"><xocs:ref-normalized-surname>LAFFERTY</xocs:ref-normalized-surname><xocs:ref-pub-year>2001</xocs:ref-pub-year><xocs:ref-first-fp>282</xocs:ref-first-fp><xocs:ref-last-lp>289</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSEIGHTEENTHINTERNATIONALCONFERENCEMACHINELEARNING2001</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>CONDITIONALRANDOMFIELDSPROBABILISTICMODELSFORSEGMENTINGLABELINGSEQUENCEDATA</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0165"><xocs:ref-normalized-surname>LEE</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>1</xocs:ref-first-fp><xocs:ref-last-lp>22</xocs:ref-last-lp><xocs:ref-normalized-initial>C</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0170"><xocs:ref-normalized-surname>LEE</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>23</xocs:ref-first-fp><xocs:ref-last-lp>25</xocs:ref-last-lp><xocs:ref-normalized-initial>H</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS16THNATIONALCONFERENCEENGLISHPHONETICSOCIETYJAPANSECONDINTERNATIONALCONGRESSPHONETICIANSENGLISH</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>EVALUATIONKOREANLEARNERSENGLISHACCENTUATION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0175"><xocs:ref-normalized-surname>LEE</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>25</xocs:ref-first-fp><xocs:ref-last-lp>58</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0180"><xocs:ref-normalized-surname>LEVY</xocs:ref-normalized-surname><xocs:ref-pub-year>1997</xocs:ref-pub-year><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>CALLCONTEXTCONCEPTUALISATION</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0185"><xocs:ref-normalized-surname>LISCOMBE</xocs:ref-normalized-surname><xocs:ref-pub-year>2007</xocs:ref-pub-year><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROSODYSPEAKERSTATEPARALINGUISTICSPRAGMATICSPROFICIENCY</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0190"><xocs:ref-normalized-surname>LIVELY</xocs:ref-normalized-surname><xocs:ref-pub-year>1993</xocs:ref-pub-year><xocs:ref-first-fp>1242</xocs:ref-first-fp><xocs:ref-last-lp>1255</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0195"><xocs:ref-normalized-surname>LLOYDJAMES</xocs:ref-normalized-surname><xocs:ref-pub-year>1940</xocs:ref-pub-year><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>SPEECHSIGNALSINTELEPHONY</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0200"><xocs:ref-normalized-surname>LOGAN</xocs:ref-normalized-surname><xocs:ref-pub-year>1991</xocs:ref-pub-year><xocs:ref-first-fp>874</xocs:ref-first-fp><xocs:ref-last-lp>886</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0205"><xocs:ref-normalized-surname>MAIER</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-first-fp>600</xocs:ref-first-fp><xocs:ref-last-lp>603</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSINTERSPEECH2009</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>ALANGUAGEINDEPENDENTFEATURESETFORAUTOMATICEVALUATIONPROSODY</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0210"><xocs:ref-normalized-surname>MOSTOW</xocs:ref-normalized-surname><xocs:ref-pub-year>2009</xocs:ref-pub-year><xocs:ref-first-fp>189</xocs:ref-first-fp><xocs:ref-last-lp>196</xocs:ref-last-lp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS14THINTERNATIONALCONFERENCEARTIFICIALINTELLIGENCEINEDUCATIONAIED2009</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>AUTOMATEDASSESSMENTORALREADINGPROSODY</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0215"><xocs:ref-normalized-surname>MUNRO</xocs:ref-normalized-surname><xocs:ref-pub-year>1995</xocs:ref-pub-year><xocs:ref-first-fp>73</xocs:ref-first-fp><xocs:ref-last-lp>97</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0220"><xocs:ref-normalized-surname>PIERREHUMBERT</xocs:ref-normalized-surname><xocs:ref-pub-year>1980</xocs:ref-pub-year><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PHONOLOGYPHONETICSENGLISHINTONATION</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0225"><xocs:ref-normalized-surname>PIKE</xocs:ref-normalized-surname><xocs:ref-pub-year>1945</xocs:ref-pub-year><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>INTONATIONAMERICANENGLISH</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0230"><xocs:ref-normalized-surname>QIAN</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>135</xocs:ref-first-fp><xocs:ref-last-lp>138</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSISCSLP2010</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>AUTOMATICPROSODYPREDICTIONDETECTIONCONDITIONALRANDOMFIELDCRFMODELS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0235"><xocs:ref-normalized-surname>RANGARAJANSRIDHAR</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>797</xocs:ref-first-fp><xocs:ref-last-lp>811</xocs:ref-last-lp><xocs:ref-normalized-initial>V</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0240"><xocs:ref-normalized-surname>ROACH</xocs:ref-normalized-surname><xocs:ref-pub-year>1982</xocs:ref-pub-year><xocs:ref-normalized-initial>P</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>LINGUISTICCONTROVERSIES</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>DISTINCTIONBETWEENSTRESSTIMEDSYLLABLETIMEDLANGUAGES</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0245"><xocs:ref-normalized-surname>SILVERMAN</xocs:ref-normalized-surname><xocs:ref-pub-year>1992</xocs:ref-pub-year><xocs:ref-first-fp>563</xocs:ref-first-fp><xocs:ref-last-lp>566</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSINTERNATIONALCONFERENCESPOKENLANGUAGE92</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>TOBIASTANDARDSCHEMEFORLABELINGPROSODY</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0250"><xocs:ref-normalized-surname>SITARAM</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>24</xocs:ref-first-fp><xocs:ref-last-lp>26</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSSLATE2011</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>VISUALFEEDBACKAREADINGTUTORGIVECHILDRENORALREADINGPROSODY</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0255"><xocs:ref-normalized-surname>SUZUKI</xocs:ref-normalized-surname><xocs:ref-pub-year>2008</xocs:ref-pub-year><xocs:ref-first-fp>83</xocs:ref-first-fp><xocs:ref-last-lp>90</xocs:ref-last-lp><xocs:ref-normalized-initial>M</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0260"><xocs:ref-normalized-surname>TAJIMA</xocs:ref-normalized-surname><xocs:ref-pub-year>1997</xocs:ref-pub-year><xocs:ref-first-fp>10</xocs:ref-first-fp><xocs:ref-last-lp>24</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0265"><xocs:ref-normalized-surname>TEIXEIRA</xocs:ref-normalized-surname><xocs:ref-pub-year>2000</xocs:ref-pub-year><xocs:ref-first-fp>187</xocs:ref-first-fp><xocs:ref-last-lp>190</xocs:ref-last-lp><xocs:ref-normalized-initial>C</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSICSLP</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>PROSODICFEATURESFORAUTOMATICTEXTINDEPENDENTEVALUATIONDEGREENATIVENESSFORLANGUAGELEARNERS</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0270"><xocs:ref-normalized-surname>TEPPERMAN</xocs:ref-normalized-surname><xocs:ref-pub-year>2010</xocs:ref-pub-year><xocs:ref-first-fp>2010</xocs:ref-first-fp><xocs:ref-normalized-initial>J</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSSPEECHPROSODY</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>TESTINGSUPRASEGMENTALENGLISHTHROUGHPARROTING</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0275"><xocs:ref-normalized-surname>WENK</xocs:ref-normalized-surname><xocs:ref-pub-year>1982</xocs:ref-pub-year><xocs:ref-first-fp>193</xocs:ref-first-fp><xocs:ref-last-lp>216</xocs:ref-last-lp><xocs:ref-normalized-initial>B</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0280"><xocs:ref-normalized-surname>WENNERSTROM</xocs:ref-normalized-surname><xocs:ref-pub-year>2000</xocs:ref-pub-year><xocs:ref-first-fp>102</xocs:ref-first-fp><xocs:ref-last-lp>127</xocs:ref-last-lp><xocs:ref-normalized-initial>A</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>ROLEINTONATIONINSECONDLANGUAGEFLUENCYPERSPECTIVESFLUENCY</xocs:ref-normalized-srctitle></xocs:ref-info><xocs:ref-info refid="sr0285"><xocs:ref-normalized-surname>WITT</xocs:ref-normalized-surname><xocs:ref-pub-year>1997</xocs:ref-pub-year><xocs:ref-first-fp>633</xocs:ref-first-fp><xocs:ref-last-lp>636</xocs:ref-last-lp><xocs:ref-normalized-initial>S</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGSICASSP1997</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>LANGUAGELEARNINGBASEDNONNATIVESPEECHRECOGNITION</xocs:ref-normalized-article-title></xocs:ref-info><xocs:ref-info refid="sr0290"><xocs:ref-normalized-surname>YAMASHITA</xocs:ref-normalized-surname><xocs:ref-pub-year>2005</xocs:ref-pub-year><xocs:ref-first-fp>496</xocs:ref-first-fp><xocs:ref-last-lp>501</xocs:ref-last-lp><xocs:ref-normalized-initial>Y</xocs:ref-normalized-initial></xocs:ref-info><xocs:ref-info refid="sr0295"><xocs:ref-normalized-surname>ZECHNER</xocs:ref-normalized-surname><xocs:ref-pub-year>2011</xocs:ref-pub-year><xocs:ref-first-fp>461</xocs:ref-first-fp><xocs:ref-last-lp>466</xocs:ref-last-lp><xocs:ref-normalized-initial>K</xocs:ref-normalized-initial><xocs:ref-normalized-srctitle>PROCEEDINGS2011IEEEWORKSHOPAUTOMATICSPEECHRECOGNITIONUNDERSTANDINGASRU</xocs:ref-normalized-srctitle><xocs:ref-normalized-article-title>EVALUATINGPROSODICFEATURESFORAUTOMATEDSCORINGNONNATIVEREADSPEECH</xocs:ref-normalized-article-title></xocs:ref-info></xocs:references><xocs:refkeys><xocs:refkey3>LEEX2017X29</xocs:refkey3><xocs:refkey4lp>LEEX2017X29X42</xocs:refkey4lp><xocs:refkey4ai>LEEX2017X29XG</xocs:refkey4ai><xocs:refkey5>LEEX2017X29X42XG</xocs:refkey5></xocs:refkeys><xocs:open-access><xocs:oa-article-status is-open-access="1" is-open-archive="0">Full</xocs:oa-article-status><xocs:oa-access-effective-date>2016-06-06T11:21:15Z</xocs:oa-access-effective-date><xocs:oa-sponsor><xocs:oa-sponsor-type>Author</xocs:oa-sponsor-type></xocs:oa-sponsor><xocs:oa-user-license>http://creativecommons.org/licenses/by-nc-nd/4.0/</xocs:oa-user-license></xocs:open-access><xocs:self-archiving><xocs:sa-start-date>2018-06-28T00:00:00Z</xocs:sa-start-date><xocs:sa-embargo-status>UnderEmbargo</xocs:sa-embargo-status><xocs:sa-user-license>http://creativecommons.org/licenses/by-nc-nd/4.0/</xocs:sa-user-license></xocs:self-archiving><xocs:copyright-info><xocs:cp-license-lines><xocs:cp-license-line lang="en">This is an open access article under the CC BY-NC-ND license.</xocs:cp-license-line></xocs:cp-license-lines><xocs:cp-notices><xocs:cp-notice lang="en">Â© 2016 The Authors. Published by Elsevier Ltd.</xocs:cp-notice></xocs:cp-notices></xocs:copyright-info><xocs:attachment-metadata-doc><xocs:attachment-set-type>item</xocs:attachment-set-type><xocs:pii-formatted>S0885-2308(16)30175-9</xocs:pii-formatted><xocs:pii-unformatted>S0885230816301759</xocs:pii-unformatted><xocs:eid>1-s2.0-S0885230816301759</xocs:eid><xocs:doi>10.1016/j.csl.2016.04.003</xocs:doi><xocs:cid>272453</xocs:cid><xocs:timestamp>2016-08-09T11:51:28.352335-04:00</xocs:timestamp><xocs:cover-date-start>2017-01-01</xocs:cover-date-start><xocs:cover-date-end>2017-01-31</xocs:cover-date-end><xocs:sponsored-access-type>UNLIMITED</xocs:sponsored-access-type><xocs:funding-body-id>NONE</xocs:funding-body-id><xocs:attachments><xocs:web-pdf><xocs:attachment-eid>1-s2.0-S0885230816301759-main.pdf</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/MAIN/application/pdf/f40d0c8a02d8881e3a317760cddbf150/main.pdf</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/MAIN/application/pdf/f40d0c8a02d8881e3a317760cddbf150/main.pdf</xocs:ucs-locator><xocs:filename>main.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:pdf-optimized>true</xocs:pdf-optimized><xocs:filesize>1607372</xocs:filesize><xocs:web-pdf-purpose>MAIN</xocs:web-pdf-purpose><xocs:web-pdf-page-count>14</xocs:web-pdf-page-count><xocs:web-pdf-images><xocs:web-pdf-image><xocs:attachment-eid>1-s2.0-S0885230816301759-main_1.png</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/PREVIEW/image/png/48aeb1b81987b96c801fb8de2bf9632f/main_1.png</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/PREVIEW/image/png/48aeb1b81987b96c801fb8de2bf9632f/main_1.png</xocs:ucs-locator><xocs:filename>main_1.png</xocs:filename><xocs:extension>png</xocs:extension><xocs:filesize>47518</xocs:filesize><xocs:pixel-height>849</xocs:pixel-height><xocs:pixel-width>656</xocs:pixel-width><xocs:attachment-type>IMAGE-WEB-PDF</xocs:attachment-type><xocs:pdf-page-num>1</xocs:pdf-page-num></xocs:web-pdf-image></xocs:web-pdf-images></xocs:web-pdf><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-ycsla775-fig-0001.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/ycsla775-fig-0001/THUMBNAIL/image/gif/42149af64afc48a56aa37003c9f54aad/ycsla775-fig-0001.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/ycsla775-fig-0001/THUMBNAIL/image/gif/42149af64afc48a56aa37003c9f54aad/ycsla775-fig-0001.sml</xocs:ucs-locator><xocs:file-basename>ycsla775-fig-0001</xocs:file-basename><xocs:filename>ycsla775-fig-0001.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>17192</xocs:filesize><xocs:pixel-height>96</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-ycsla775-fig-0002.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/ycsla775-fig-0002/THUMBNAIL/image/gif/0646b047c8eb608de58d0d48483e42cd/ycsla775-fig-0002.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/ycsla775-fig-0002/THUMBNAIL/image/gif/0646b047c8eb608de58d0d48483e42cd/ycsla775-fig-0002.sml</xocs:ucs-locator><xocs:file-basename>ycsla775-fig-0002</xocs:file-basename><xocs:filename>ycsla775-fig-0002.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>23100</xocs:filesize><xocs:pixel-height>163</xocs:pixel-height><xocs:pixel-width>168</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-ycsla775-fig-0003.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/ycsla775-fig-0003/THUMBNAIL/image/gif/b1e2bcf28b014b654c5956d0ad884b72/ycsla775-fig-0003.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/ycsla775-fig-0003/THUMBNAIL/image/gif/b1e2bcf28b014b654c5956d0ad884b72/ycsla775-fig-0003.sml</xocs:ucs-locator><xocs:file-basename>ycsla775-fig-0003</xocs:file-basename><xocs:filename>ycsla775-fig-0003.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>19348</xocs:filesize><xocs:pixel-height>119</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-ycsla775-fig-0004.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/ycsla775-fig-0004/THUMBNAIL/image/gif/48d3398ed129407728e6d349f3828e71/ycsla775-fig-0004.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/ycsla775-fig-0004/THUMBNAIL/image/gif/48d3398ed129407728e6d349f3828e71/ycsla775-fig-0004.sml</xocs:ucs-locator><xocs:file-basename>ycsla775-fig-0004</xocs:file-basename><xocs:filename>ycsla775-fig-0004.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>11711</xocs:filesize><xocs:pixel-height>69</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-ycsla775-fig-0005.sml</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/ycsla775-fig-0005/THUMBNAIL/image/gif/2a2e3a6a0711d7690d0d940b0405a8e8/ycsla775-fig-0005.sml</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/ycsla775-fig-0005/THUMBNAIL/image/gif/2a2e3a6a0711d7690d0d940b0405a8e8/ycsla775-fig-0005.sml</xocs:ucs-locator><xocs:file-basename>ycsla775-fig-0005</xocs:file-basename><xocs:filename>ycsla775-fig-0005.sml</xocs:filename><xocs:extension>sml</xocs:extension><xocs:filesize>9692</xocs:filesize><xocs:pixel-height>147</xocs:pixel-height><xocs:pixel-width>219</xocs:pixel-width><xocs:attachment-type>IMAGE-THUMBNAIL</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-ycsla775-fig-0001.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/ycsla775-fig-0001/DOWNSAMPLED/image/jpeg/abb6e7b94ebe3c57ff69b6b4cf6e0e02/ycsla775-fig-0001.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/ycsla775-fig-0001/DOWNSAMPLED/image/jpeg/abb6e7b94ebe3c57ff69b6b4cf6e0e02/ycsla775-fig-0001.jpg</xocs:ucs-locator><xocs:file-basename>ycsla775-fig-0001</xocs:file-basename><xocs:filename>ycsla775-fig-0001.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>78596</xocs:filesize><xocs:pixel-height>319</xocs:pixel-height><xocs:pixel-width>731</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-ycsla775-fig-0002.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/ycsla775-fig-0002/DOWNSAMPLED/image/jpeg/9553d83c34db81ef1a94799f348ba3f9/ycsla775-fig-0002.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/ycsla775-fig-0002/DOWNSAMPLED/image/jpeg/9553d83c34db81ef1a94799f348ba3f9/ycsla775-fig-0002.jpg</xocs:ucs-locator><xocs:file-basename>ycsla775-fig-0002</xocs:file-basename><xocs:filename>ycsla775-fig-0002.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>81972</xocs:filesize><xocs:pixel-height>382</xocs:pixel-height><xocs:pixel-width>393</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-ycsla775-fig-0003.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/ycsla775-fig-0003/DOWNSAMPLED/image/jpeg/d233263de0ec62b6f05b023db6350b17/ycsla775-fig-0003.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/ycsla775-fig-0003/DOWNSAMPLED/image/jpeg/d233263de0ec62b6f05b023db6350b17/ycsla775-fig-0003.jpg</xocs:ucs-locator><xocs:file-basename>ycsla775-fig-0003</xocs:file-basename><xocs:filename>ycsla775-fig-0003.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>92742</xocs:filesize><xocs:pixel-height>397</xocs:pixel-height><xocs:pixel-width>731</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-ycsla775-fig-0004.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/ycsla775-fig-0004/DOWNSAMPLED/image/jpeg/40c214c92818b035ce55d3edd27c9466/ycsla775-fig-0004.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/ycsla775-fig-0004/DOWNSAMPLED/image/jpeg/40c214c92818b035ce55d3edd27c9466/ycsla775-fig-0004.jpg</xocs:ucs-locator><xocs:file-basename>ycsla775-fig-0004</xocs:file-basename><xocs:filename>ycsla775-fig-0004.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>45415</xocs:filesize><xocs:pixel-height>231</xocs:pixel-height><xocs:pixel-width>731</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-ycsla775-fig-0005.jpg</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/ycsla775-fig-0005/DOWNSAMPLED/image/jpeg/218eba5d1c8324c3caef1d2b9d37ef64/ycsla775-fig-0005.jpg</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/ycsla775-fig-0005/DOWNSAMPLED/image/jpeg/218eba5d1c8324c3caef1d2b9d37ef64/ycsla775-fig-0005.jpg</xocs:ucs-locator><xocs:file-basename>ycsla775-fig-0005</xocs:file-basename><xocs:filename>ycsla775-fig-0005.jpg</xocs:filename><xocs:extension>jpg</xocs:extension><xocs:filesize>27719</xocs:filesize><xocs:pixel-height>263</xocs:pixel-height><xocs:pixel-width>393</xocs:pixel-width><xocs:attachment-type>IMAGE-DOWNSAMPLED</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-si1.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/STRIPIN/image/gif/0c9fbb80c19b1c62ccb2e9037ccf47e2/si1.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/STRIPIN/image/gif/0c9fbb80c19b1c62ccb2e9037ccf47e2/si1.gif</xocs:ucs-locator><xocs:file-basename>si1</xocs:file-basename><xocs:filename>si1.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>2828</xocs:filesize><xocs:pixel-height>65</xocs:pixel-height><xocs:pixel-width>458</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-si2.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/STRIPIN/image/gif/3348f1273365f197bfc40a0b6bf2493d/si2.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/STRIPIN/image/gif/3348f1273365f197bfc40a0b6bf2493d/si2.gif</xocs:ucs-locator><xocs:file-basename>si2</xocs:file-basename><xocs:filename>si2.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>4323</xocs:filesize><xocs:pixel-height>96</xocs:pixel-height><xocs:pixel-width>412</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment><xocs:attachment><xocs:attachment-eid>1-s2.0-S0885230816301759-si3.gif</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0885230816301759/STRIPIN/image/gif/f8d6bae289699f8ebb3b6953fa8c3540/si3.gif</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0885230816301759/STRIPIN/image/gif/f8d6bae289699f8ebb3b6953fa8c3540/si3.gif</xocs:ucs-locator><xocs:file-basename>si3</xocs:file-basename><xocs:filename>si3.gif</xocs:filename><xocs:extension>gif</xocs:extension><xocs:filesize>4684</xocs:filesize><xocs:pixel-height>60</xocs:pixel-height><xocs:pixel-width>669</xocs:pixel-width><xocs:attachment-type>ALTIMG</xocs:attachment-type></xocs:attachment></xocs:attachments></xocs:attachment-metadata-doc></xocs:meta><xocs:serial-item><article xmlns="http://www.elsevier.com/xml/ja/dtd" docsubtype="fla" xml:lang="en" version="5.4"><item-info><jid>YCSLA</jid><aid>775</aid><ce:pii>S0885-2308(16)30175-9</ce:pii><ce:doi>10.1016/j.csl.2016.04.003</ce:doi><ce:copyright type="other" year="2016">The Authors</ce:copyright></item-info><ce:floats><ce:figure id="f0010"><ce:label>Fig.â1</ce:label><ce:caption id="ca0015"><ce:simple-para id="sp0025" view="all">A sample from the KLEAC.</ce:simple-para></ce:caption><ce:alt-text role="short" id="atte0010">Fig.â1</ce:alt-text><ce:link id="ln0010" locator="ycsla775-fig-0001"/></ce:figure><ce:figure id="f0015"><ce:label>Fig.â2</ce:label><ce:caption id="ca0025"><ce:simple-para id="sp0035" view="all">Architecture of the proposed system.</ce:simple-para></ce:caption><ce:alt-text role="short" id="atte0015">Fig.â2</ce:alt-text><ce:link id="ln0015" locator="ycsla775-fig-0002"/></ce:figure><ce:figure id="f0020"><ce:label>Fig.â3</ce:label><ce:caption id="ca0030"><ce:simple-para id="sp0040" view="all">A sample of sentence stress feedback.</ce:simple-para></ce:caption><ce:alt-text role="short" id="atte0020">Fig.â3</ce:alt-text><ce:link id="ln0025" locator="ycsla775-fig-0003"/></ce:figure><ce:figure id="f0025"><ce:label>Fig.â4</ce:label><ce:caption id="ca0045"><ce:simple-para id="sp0055" view="all">Correlations between the learners' proficiency levels and the feedback scores determined by <ce:italic>Î¸</ce:italic><ce:inf loc="post">1</ce:inf> and <ce:italic>Î¸</ce:italic><ce:inf loc="post">2</ce:inf>; (a) for an overview, (b) for <ce:italic>Î¸</ce:italic><ce:inf loc="post">1</ce:inf> and (c) for <ce:italic>Î¸</ce:italic><ce:inf loc="post">2.</ce:inf></ce:simple-para></ce:caption><ce:alt-text role="short" id="atte0025">Fig.â4</ce:alt-text><ce:link id="ln0040" locator="ycsla775-fig-0004"/></ce:figure><ce:figure id="f0030"><ce:label>Fig.â5</ce:label><ce:caption id="ca0050"><ce:simple-para id="sp0060" view="all">Distribution of feedback scores over proficiency levels.</ce:simple-para></ce:caption><ce:alt-text role="short" id="atte0030">Fig.â5</ce:alt-text><ce:link id="ln0045" locator="ycsla775-fig-0005"/></ce:figure><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0010"><ce:label>Tableâ1</ce:label><ce:caption id="ca0010"><ce:simple-para id="sp0020" view="all">A comparison of Foot and Narrow Rhythm Unit.</ce:simple-para></ce:caption><ce:alt-text role="short" id="atte0035">Tableâ1</ce:alt-text><tgroup cols="5"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Sentence</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">It's</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Ëalmost</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">im-</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Ëpossible</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Abercrombie</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">[<inf loc="post">F</inf> It's]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col4" namest="col3" align="left">[<inf loc="post">F</inf> almost im-]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">[<inf loc="post">F</inf> -possible]</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Jassem</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">[<inf loc="post">ANA</inf> It's]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">[<inf loc="post">NRU</inf> almost]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">[<inf loc="post">ANA</inf> im-]</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">[<inf loc="post">NRU</inf> -possible]</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0015"><ce:label>Tableâ2</ce:label><ce:caption id="ca0020"><ce:simple-para id="sp0030" view="all">The agreement rates between individual KLEAC labelers and Aix-MARSEC labelers.</ce:simple-para></ce:caption><ce:alt-text role="short" id="atte0040">Tableâ2</ce:alt-text><tgroup cols="6"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Transcribers</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">2</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">4</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="2" align="left">Cohen's Kappa</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Îºâ=â.702</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Îºâ=â.789</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Îºâ=â.617</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Îºâ=â.830</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Îºâ=â.829</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">95% CI: .631 to .774</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">95% CI: .728 to .851</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">95% CI: .539 to .695</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">95% CI: .773 to .886</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">95% CI: .772 to .885</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">pâ&lt;â.001</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">pâ&lt;â.001</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">pâ&lt;â.001</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">pâ&lt;â.001</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">pâ&lt;â.001</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0020"><ce:label>Tableâ3</ce:label><ce:caption id="ca0035"><ce:simple-para id="sp0045" view="all">Accuracy, precision, recall and F-measure of the proposed models.</ce:simple-para></ce:caption><ce:alt-text role="short" id="atte0045">Tableâ3</ce:alt-text><tgroup cols="5"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Models</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Accuracy</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Precision</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Recall</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">F-measure</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Prediction</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">96.6</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">98.3</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">96.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">97.2</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Detection</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">84.1</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">84.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">88.8</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">86.7</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0025"><ce:label>Tableâ4</ce:label><ce:caption id="ca0040"><ce:simple-para id="sp0050" view="all">Comparison of labeled, predicted and detected results.</ce:simple-para></ce:caption><ce:alt-text role="short" id="atte0050">Tableâ4</ce:alt-text><tgroup cols="3"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col3" namest="col1" align="left">a. Prediction</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col3" namest="col2" align="left">Predicted</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Unstressed</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Stressed</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Labeled</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"/></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">âUnstressed</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3670</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">96</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">âStressed</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">224</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5521</entry></row></tbody></tgroup><tgroup cols="3"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col3" namest="col1" align="left">b. Detection</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col3" namest="col2" align="left">Detected</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Unstressed</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Stressed</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Labeled</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left"/></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">âUnstressed</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3047</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">889</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">âStressed</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">626</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4959</entry></row></tbody></tgroup></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0030"><ce:label>Tableâ5</ce:label><ce:caption id="ca0055"><ce:simple-para id="sp0065" view="all">Results of the assessment (mean and standard deviation).</ce:simple-para></ce:caption><ce:alt-text role="short" id="atte0055">Tableâ5</ce:alt-text><tgroup cols="7"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><colspec colname="col5" colnum="5"/><colspec colname="col6" colnum="6"/><colspec colname="col7" colnum="7"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" morerows="1" align="left"/><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col3" namest="col2" align="left">Comprehensibility</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col5" namest="col4" align="left">Accentedness</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" nameend="col7" namest="col6" align="left">Rhythm</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Before</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">After</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Before</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">After</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Before</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">After</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Control group</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5.47 (1.60)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5.72 (1.55)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">4.25 (1.71)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">4.45 (1.61)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">4.15 (1.73)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">4.38 (1.86)</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Experiment group</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5.55 (1.78)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5.75 (1.80)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">4.33 (1.85)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">4.95<cross-ref id="crf0310" refid="tn0010">*</cross-ref> (1.69)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">4.37 (2.03)</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">5.05<cross-ref id="crf0315" refid="tn0010">*</cross-ref> (2.04)</entry></row></tbody></tgroup><ce:table-footnote id="tn0010"><ce:label>*</ce:label><ce:note-para id="np0040" view="all">The asterisk represents statistically significant figures at the 99% level.</ce:note-para></ce:table-footnote></ce:table><ce:table xmlns="http://www.elsevier.com/xml/common/cals/dtd" frame="topbot" id="t0035"><ce:label>Tableâ6</ce:label><ce:caption id="ca0060"><ce:simple-para id="sp0070" view="all">Improvement ratio.</ce:simple-para></ce:caption><ce:alt-text role="short" id="atte0060">Tableâ6</ce:alt-text><tgroup cols="4"><colspec colname="col1" colnum="1"/><colspec colname="col2" colnum="2"/><colspec colname="col3" colnum="3"/><colspec colname="col4" colnum="4"/><thead><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Criterion</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Comprehensibility</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Accentedness</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Rhythm</entry></row></thead><tbody><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Control group</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">4.7%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">5.5%</entry></row><row><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="left">Experiment group</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">3.6%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">14.3%</entry><entry xmlns="http://www.elsevier.com/xml/common/dtd" align="char" char=".">15.5%</entry></row></tbody></tgroup></ce:table></ce:floats><head><ce:title id="tit0010">Automatic sentence stress feedback for non-native English learners</ce:title><ce:author-group id="aug0010"><ce:author id="au0010"><ce:given-name>Gary Geunbae</ce:given-name><ce:surname>Lee</ce:surname><ce:cross-ref id="crf0010" refid="af0010"><ce:sup loc="post">a</ce:sup></ce:cross-ref></ce:author><ce:author id="au0015"><ce:given-name>Ho-Young</ce:given-name><ce:surname>Lee</ce:surname><ce:cross-ref id="crf0015" refid="af0015"><ce:sup loc="post">b</ce:sup></ce:cross-ref><ce:cross-ref id="crf0020" refid="co0010">*</ce:cross-ref><ce:e-address id="eadd0010" type="email">hylee@snu.ac.kr</ce:e-address></ce:author><ce:author id="au0020"><ce:given-name>Jieun</ce:given-name><ce:surname>Song</ce:surname><ce:cross-ref id="crf0025" refid="af0020"><ce:sup loc="post">c</ce:sup></ce:cross-ref></ce:author><ce:author id="au0025"><ce:given-name>Byeongchang</ce:given-name><ce:surname>Kim</ce:surname><ce:cross-ref id="crf0030" refid="af0025"><ce:sup loc="post">d</ce:sup></ce:cross-ref></ce:author><ce:author id="au0030"><ce:given-name>Sechun</ce:given-name><ce:surname>Kang</ce:surname><ce:cross-ref id="crf0035" refid="af0030"><ce:sup loc="post">e</ce:sup></ce:cross-ref></ce:author><ce:author id="au0035"><ce:given-name>Jinsik</ce:given-name><ce:surname>Lee</ce:surname><ce:cross-ref id="crf0040" refid="af0035"><ce:sup loc="post">f</ce:sup></ce:cross-ref></ce:author><ce:author id="au0040"><ce:given-name>Hyosung</ce:given-name><ce:surname>Hwang</ce:surname><ce:cross-ref id="crf0045" refid="af0015"><ce:sup loc="post">b</ce:sup></ce:cross-ref></ce:author><ce:affiliation id="af0010"><ce:label>a</ce:label><ce:textfn id="tx0010">Department of Computer Science and Engineering, Pohang University of Science and Technology, 77 Cheongam-ro. Nam-gu, Pohang, Gyeongbuk, South Korea</ce:textfn><sa:affiliation><sa:organization>Department of Computer Science and Engineering</sa:organization><sa:organization>Pohang University of Science and Technology</sa:organization><sa:address-line>77 Cheongam-ro. Nam-gu</sa:address-line><sa:city>Pohang</sa:city><sa:state>Gyeongbuk</sa:state><sa:country>South Korea</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0015"><ce:label>b</ce:label><ce:textfn id="tx0015">Department of Linguistics, Seoul National University, Daehak-dong, Gwanak-gu, Seoul, South Korea</ce:textfn><sa:affiliation><sa:organization>Department of Linguistics</sa:organization><sa:organization>Seoul National University</sa:organization><sa:address-line>Daehak-dong</sa:address-line><sa:address-line>Gwanak-gu</sa:address-line><sa:city>Seoul</sa:city><sa:country>South Korea</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0020"><ce:label>c</ce:label><ce:textfn id="tx0020">Speech Hearing and Phonetic Sciences, University College London, 2 Wakefield St., London WC1N 1PF, UK</ce:textfn><sa:affiliation><sa:organization>Speech Hearing and Phonetic Sciences</sa:organization><sa:organization>University College London</sa:organization><sa:address-line>2 Wakefield St.</sa:address-line><sa:city>London</sa:city><sa:postal-code>WC1N 1PF</sa:postal-code><sa:country>UK</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0025"><ce:label>d</ce:label><ce:textfn id="tx0025">School of Computer and Information Communication Engineering, Catholic University of Daegu, Hayang-ro, Gyeongsan, Gyeongbuk, South Korea</ce:textfn><sa:affiliation><sa:organization>School of Computer and Information Communication Engineering</sa:organization><sa:organization>Catholic University of Daegu</sa:organization><sa:address-line>Hayang-ro</sa:address-line><sa:city>Gyeongsan</sa:city><sa:state>Gyeongbuk</sa:state><sa:country>South Korea</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0030"><ce:label>e</ce:label><ce:textfn id="tx0030">Software R&amp;D Center, Samsung Electronics, Maetan-dong, Yeongtong-gu, Suwon, Gyeonggi, South Korea</ce:textfn><sa:affiliation><sa:organization>Software R&amp;D Center</sa:organization><sa:organization>Samsung Electronics</sa:organization><sa:address-line>Maetan-dong</sa:address-line><sa:address-line>Yeongtong-gu</sa:address-line><sa:city>Suwon</sa:city><sa:state>Gyeonggi</sa:state><sa:country>South Korea</sa:country></sa:affiliation></ce:affiliation><ce:affiliation id="af0035"><ce:label>f</ce:label><ce:textfn id="tx0035">DMC R&amp;D Center, Samsung Electronics, Maetan-dong, Yeongtong-gu, Suwon, Gyeonggi, South Korea</ce:textfn><sa:affiliation><sa:organization>DMC R&amp;D Center</sa:organization><sa:organization>Samsung Electronics</sa:organization><sa:address-line>Maetan-dong, Yeongtong-gu</sa:address-line><sa:city>Suwon</sa:city><sa:state>Gyeonggi</sa:state><sa:country>South Korea</sa:country></sa:affiliation></ce:affiliation><ce:correspondence id="co0010"><ce:label>*</ce:label><ce:text id="te0010">Corresponding author at: Department of Linguistics, Seoul National University, Daehak-dong, Gwanak-gu, Seoul, South Korea. Tel: +82 10 2744 0584; fax: +82 2 882 2451.</ce:text><sa:affiliation><sa:organization>Department of Linguistics</sa:organization><sa:organization>Seoul National University</sa:organization><sa:address-line>Daehak-dong</sa:address-line><sa:address-line>Gwanak-gu</sa:address-line><sa:city>Seoul</sa:city><sa:country>South Korea</sa:country></sa:affiliation></ce:correspondence></ce:author-group><ce:date-received day="19" month="3" year="2014"/><ce:date-revised day="11" month="4" year="2016"/><ce:date-accepted day="13" month="4" year="2016"/><ce:abstract class="author-highlights" id="ab0010" xml:lang="en" view="all"><ce:section-title id="st0010">Highlights</ce:section-title><ce:abstract-sec id="abs0010" view="all"><ce:simple-para id="sp0010" view="all"><ce:list id="ulist0010"><ce:list-item id="u0010"><ce:label>â¢</ce:label><ce:para id="p0010" view="all">The proposed sentence stress feedback system consists of stress prediction, detection and feedback provision models.</ce:para></ce:list-item><ce:list-item id="u0015"><ce:label>â¢</ce:label><ce:para id="p0015" view="all">The accuracy of the stress prediction and detection models was 96.6% and 84.1% respectively.</ce:para></ce:list-item><ce:list-item id="u0020"><ce:label>â¢</ce:label><ce:para id="p0020" view="all">The stress feedback provision model provides non-native learners with sentence stress errors.</ce:para></ce:list-item><ce:list-item id="u0025"><ce:label>â¢</ce:label><ce:para id="p0025" view="all">Any sentence can be used for practice in the proposed system.</ce:para></ce:list-item><ce:list-item id="u0030"><ce:label>â¢</ce:label><ce:para id="p0030" view="all">Students trained with this system improved their accentedness and rhythm significantly more than those in the control group.</ce:para></ce:list-item></ce:list></ce:simple-para></ce:abstract-sec></ce:abstract><ce:abstract class="author" id="ab0015" xml:lang="en" view="all"><ce:section-title id="st0015">Abstract</ce:section-title><ce:abstract-sec id="abs0015" view="all"><ce:simple-para id="sp0015" view="all">This paper proposes a sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined. This system provides non-native learners with feedback on sentence stress errors so that they can improve their English rhythm and fluency in a self-study setting. The sentence stress feedback system was devised to predict and detect the sentence stress of any practice sentence. The accuracy of the prediction and detection models was 96.6% and 84.1%, respectively. The stress feedback provision model offers positive or negative stress feedback for each spoken word by comparing the probability of the predicted stress pattern with that of the detected stress pattern. In an experiment that evaluated the educational effect of the proposed system incorporated in our CALL system, significant improvements in accentedness and rhythm were seen with the students who trained with our system but not with those in the control group.</ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords class="keyword" id="kwd0010" xml:lang="en" view="all"><ce:section-title id="st0020">Keywords</ce:section-title><ce:keyword id="kw0010"><ce:text id="te0015">Sentence stress</ce:text></ce:keyword><ce:keyword id="kw0015"><ce:text id="te0020">Sentence stress feedback system</ce:text></ce:keyword><ce:keyword id="kw0020"><ce:text id="te0025">Stress prediction model</ce:text></ce:keyword><ce:keyword id="kw0025"><ce:text id="te0030">Stress detection model</ce:text></ce:keyword><ce:keyword id="kw0030"><ce:text id="te0035">Stress feedback provision model</ce:text></ce:keyword><ce:keyword id="kw0035"><ce:text id="te0040">CALL</ce:text></ce:keyword></ce:keywords></head><body view="all"><ce:sections><ce:section id="s0010" role="introduction" view="all"><ce:label>1</ce:label><ce:section-title id="st0025">Introduction</ce:section-title><ce:para id="p0035" view="all">In English, every word has one or more lexical stresses<ce:cross-ref id="crf0050" refid="fn0010"><ce:sup loc="post">1</ce:sup></ce:cross-ref><ce:footnote id="fn0010"><ce:label>1</ce:label><ce:note-para id="np0010" view="all">Each word has one primary stress.</ce:note-para></ce:footnote> depending on the structure of the word and the number of syllables, but not all word stresses are phonetically realized in utterance. Content words, which deliver major semantic information and therefore require listeners' attention, normally receive stress on the utterance level whereas function words do not (cf. <ce:cross-ref id="crf0055" refid="bib0155">Kingdon, 1958</ce:cross-ref>, for a detailed discussion about content and function words). Stress imposed on the utterance level has been traditionally called âsentence stressâ (<ce:cross-refs id="crfs0010" refid="bib0095 bib0140">Gimson, 1980; Jones, 1972</ce:cross-refs>).</ce:para><ce:para id="p0040" view="all">The major function of sentence stress is to highlight semantically important words and to form the rhythmic pattern of the utterance. It has been known that sentence stress occurs at regular intervals and unstressed syllables between consecutive stressed syllables are reduced, causing the impression of âstress-timed rhythmâ. Stress-timed rhythm has been traditionally distinguished from âsyllable-timed rhythmâ in which syllables are pronounced with similar duration without vowel reduction, as in French and Italian (<ce:cross-refs id="crfs0015" refid="bib0015 bib0195 bib0225">Abercrombie, 1967; Lloyd James, 1940; Pike, 1945</ce:cross-refs>). However, the idea that the units of rhythm (i.e., feet in stress-timed rhythm and syllables in syllable-timed rhythm) are of equal duration has been negated in several empirical studies (e.g., <ce:cross-refs id="crfs0020" refid="bib0070 bib0240 bib0275">Dauer, 1983; Roach, 1982; Wenk and Wiolland, 1982</ce:cross-refs>) and for many languages, the rhythmic classification is not as clear-cut as previously believed. Nonetheless, the rhythm classes are generally regarded as reflecting the different rhythmic characteristics.</ce:para><ce:para id="p0045" view="all">Sentence stress is distinguished from pitch accent that carries pitch prominence caused by an important intonation event (<ce:cross-refs id="crfs0025" refid="bib0035 bib0220">Bolinger, 1958; Pierrehumbert, 1980</ce:cross-refs>) as well as rhythmic prominence caused by sentence stress. While all pitch accents are imposed on stressed syllables, some sentence stresses do not involve pitch prominence and only affect the rhythmic pattern of a sentence. Thus pitch accent can be considered to be ranked higher than sentence stress in the prosodic hierarchy.</ce:para><ce:para id="p0050" view="all">It has been recognized in some previous research that prosody plays an equal or greater role than segments in the judgment of comprehensibility and/or accentedness of non-native speech (e.g. <ce:cross-refs id="crfs0030" refid="bib0020 bib0040 bib0100 bib0260 bib0280">Anderson-Hsieh etÂ al., 1992; Boula de MareÃ¼il and Vieru-Dimulescu, 2006; Hahn, 2004; Tajima etÂ al., 1997; Wennerstrom, 2000</ce:cross-refs>). As for Korean learners of English, great difficulties have been observed in rhythm and fluency. Low proficiency learners tend to place sentence stress on most of the words in a sentence, even on function words (<ce:cross-ref id="crf0060" refid="bib0170">Lee, 2011</ce:cross-ref>). They tend to use strong vowels even in unstressed syllables, giving the impression of syllable-timed rhythm to native English listeners.</ce:para><ce:para id="p0055" view="all">Previous research shows that teaching only the pronunciation of segments does not significantly improve comprehensibility in non-native spontaneous speech (<ce:cross-ref id="crf0065" refid="bib0090">Elliott, 1997</ce:cross-ref>), whereas prosody teaching does (<ce:cross-refs id="crfs0035" refid="bib0080 bib0085">Derwing and Rossiter, 2003; Derwing etÂ al., 1998</ce:cross-refs>). Hence this paper aims to propose an automatic sentence stress feedback system designed to provide non-native English learners, especially Koreans, with feedback on their sentence stress errors. It is hoped that this system will help non-native learners correct their errors and thereby improve their English rhythm and fluency, ultimately resulting in an increase in their oral proficiency.</ce:para><ce:para id="p0060" view="all">Automatic prosody scoring systems have been proposed in previous literature to evaluate the English prosody (i.e., stress, rhythm and/or intonation) of both native and non-native speakers (<ce:cross-refs id="crfs0040" refid="bib0065 bib0110 bib0120 bib0185 bib0205 bib0210 bib0255 bib0270 bib0290">Cheng, 2011; HÃ¶nig etÂ al., 2010; Ito etÂ al., 2009; Liscombe, 2007; Maier etÂ al., 2009; Mostow and Duong, 2009; Suzuki etÂ al., 2008; Tepperman etÂ al., 2010; Yamashita etÂ al., 2005</ce:cross-refs>). Several automated speech assessment systems with a prosody evaluation component have also been developed (<ce:cross-refs id="crfs0045" refid="bib0055 bib0060 bib0265 bib0295">Chandel etÂ al., 2007; Chen and Zechner, 2011; Teixeira etÂ al., 2000; Zechner etÂ al., 2011</ce:cross-refs>). These automatic prosody or speech assessment systems are useful for stimulating and encouraging English learners, but their educational effect is somewhat limited because corrective feedback is not offered and learners are left without knowing what to correct. Hence, there have been a few studies concerning systems that offer automated feedback on prosody or speech (<ce:cross-refs id="crfs0050" refid="bib0025 bib0250">Bang etÂ al., 2013; Sitaram etÂ al., 2011</ce:cross-refs>).</ce:para><ce:para id="p0065" view="all"><ce:cross-ref id="crf0070" refid="bib0115">Imoto etÂ al. (2002)</ce:cross-ref> proposed a sentence stress detection model that provides diagnostic information to learners by comparing native speakers' reference speech with Japanese learners' speech. This study focused only on stress detection. Our proposed system, however, focuses on the integration of sentence stress prediction, detection, and error feedback technologies to help non-native learners effectively improve their English rhythm and fluency.</ce:para><ce:para id="p0070" view="all">While several previous studies including <ce:cross-ref id="crf0075" refid="bib0115">Imoto etÂ al. (2002)</ce:cross-ref> used native speech as a reference for direct comparison with non-native speech, this study uses automatically predicted sentence stresses generated by our sentence stress prediction model as a reference for sentence stress detection. This allows the proposed system to evaluate and process any given utterance.</ce:para><ce:para id="p0075" view="all">Frequent and repetitive practice is necessary for non-native learners to improve their English rhythm and fluency. Since traditional face-to-face language learning opportunities are costly due to time and space barriers, CALL (Computer-Assisted Language Learning), which overcomes these barriers and offers non-native learners easy access to computer-based practice programs, has received much attention since <ce:cross-ref id="crf0080" refid="bib0180">Levy (1997)</ce:cross-ref> and <ce:cross-ref id="crf0085" refid="bib0285">Witt and Young (1997)</ce:cross-ref>. <ce:cross-ref id="crf0090" refid="bib0175">Lee etÂ al. (2011)</ce:cross-ref> showed that a CALL system based on dialog management technologies (cf. <ce:cross-ref id="crf0095" refid="bib0165">Lee etÂ al., 2010</ce:cross-ref>) significantly improved learners' spoken language skills. To evaluate our sentence stress feedback system, we set up a CALL system with which our sentence stress prediction, detection, and feedback provision models were incorporated.</ce:para><ce:para id="p0080" view="all">The remainder of this paper is structured as follows: <ce:cross-ref id="crf0100" refid="s0015">Section 2</ce:cross-ref> describes the materials used for the proposed system. <ce:cross-ref id="crf0105" refid="s0030">Section 3</ce:cross-ref> proposes and describes the sentence stress prediction, detection, and feedback provision models. <ce:cross-ref id="crf0110" refid="s0055">Section 4</ce:cross-ref> reports the results of some validation experiments designed to evaluate the performance of these models as well as the usefulness of our CALL system in a real learning environment. A conclusion is given in <ce:cross-ref id="crf0115" refid="s0075">Section 5</ce:cross-ref>.</ce:para></ce:section><ce:section id="s0015" view="all"><ce:label>2</ce:label><ce:section-title id="st0030">Materials</ce:section-title><ce:section id="s0020" view="all"><ce:label>2.1</ce:label><ce:section-title id="st0035">The Aix-MARSEC database for sentence stress prediction</ce:section-title><ce:para id="p0085" view="all">In <ce:cross-ref id="crf0120" refid="bib0150">Kang etÂ al. (2012)</ce:cross-ref>, we used the BURNC (Boston University Radio News Corpus), where pitch accents are labeled according to the ToBI system (<ce:cross-ref id="crf0125" refid="bib0245">Silverman etÂ al., 1992</ce:cross-ref>) to produce a pitch accent prediction model. The accuracy of sentence stress prediction with this model was only 83.7%, and thus not satisfactory enough to be used for teaching stress-timed English rhythm to non-native learners because pitch accent is imposed on some, but not all, stressed words in a sentence.<ce:cross-ref id="crf0130" refid="fn0015"><ce:sup loc="post">2</ce:sup></ce:cross-ref><ce:footnote id="fn0015"><ce:label>2</ce:label><ce:note-para id="np0015" view="all">Although the pitch accent prediction model is not suitable for teaching English rhythm, it may be used to build an intonation assessment and correction system.</ce:note-para></ce:footnote> Since we wanted to drastically improve the accuracy of the sentence stress prediction, we built a sentence stress prediction model using the Aix-MARSEC (Aix-Machine Readable Spoken English Corpus) database where sentence stress is reliably annotated (cf. <ce:cross-ref id="crf0135" refid="bib0105">Hirst etÂ al., 2009</ce:cross-ref>).</ce:para><ce:para id="p0090" view="all">The Aix-MARSEC database consists of over five hours of natural-sounding British English speech data collected from 53 different speakers (17 males and 36 females). The corpus includes approximately 55,000 orthographically transcribed words. In this corpus, rhythm units and stress feet, both of which are demarcated by sentence stress, are annotated.</ce:para><ce:para id="p0095" view="all">The rhythm unit annotation is based on <ce:cross-ref id="crf0140" refid="bib0125">Jassem's (1952)</ce:cross-ref> notion of Anacrusis (ANA) and Narrow Rhythm Unit (NRU) and the stress foot annotation on <ce:cross-ref id="crf0145" refid="bib0010">Abercrombie's (1964)</ce:cross-ref> notion of Foot (F) (cf. the <ce:italic>Read Me</ce:italic> file available in <ce:cross-ref id="crf0150" refid="bib0105">Hirst etÂ al., 2009</ce:cross-ref>). Narrow Rhythm Unit begins with a stressed syllable and ends at the following word boundary.<ce:cross-ref id="crf0155" refid="fn0020"><ce:sup loc="post">3</ce:sup></ce:cross-ref><ce:footnote id="fn0020"><ce:label>3</ce:label><ce:note-para id="np0020" view="all">It should be noted that <ce:cross-ref id="crf0160" refid="bib0130">Jassem (1999)</ce:cross-ref> defined âstressâ as âthe potential for accentâ and used the term âaccentâ to refer to both rhythm accent (tertiary accent) and pitch accent (primary and secondary pitch accents). âAccentâ in <ce:cross-ref id="crf0165" refid="bib0130">Jassem (1999)</ce:cross-ref> corresponds to âstressâ in <ce:cross-ref id="crf0170" refid="bib0125">Jassem (1952)</ce:cross-ref>.</ce:note-para></ce:footnote> Foot begins with a stressed syllable or an intonation boundary and ends before the following stressed syllable or at the next intonation boundary. Hence the right boundary of the Narrow Rhythm Unit coincides with the word boundary whereas that of the Foot is often placed inside a word before the next stressed syllable. Unstressed syllables or words preceding the first stressed syllable are regarded as Anacruses, which are âpronounced extremely rapidlyâ according to <ce:cross-ref id="crf0175" refid="bib0125">Jassem (1952)</ce:cross-ref>. But in <ce:cross-ref id="crf0180" refid="bib0010">Abercrombie (1964)</ce:cross-ref>, utterance-initial unstressed syllables or words preceding the first stressed syllable form a separate Foot. The difference between the two theories of rhythm is well demonstrated in <ce:cross-ref id="crf0185" refid="bib0105">Hirst etÂ al. (2009)</ce:cross-ref> as in <ce:cross-ref id="crf0190" refid="t0010">Table 1</ce:cross-ref><ce:float-anchor refid="t0010"/>.</ce:para><ce:para id="p0100" view="all">If we regard the first syllable in NRU as a stressed syllable, we can easily extract words where sentence stress is imposed. But if we use Abercrombie's stress feet annotation, we have to exclude utterance-initial feet without a stressed syllable, which requires extra effort. Hence we used Jassem's rhythm unit annotations to implement our sentence stress prediction model. By using the Aix-MARSEC database, we were able to acquire a much more accurate sentence stress prediction model, which works for any sentence.</ce:para></ce:section><ce:section id="s0025" view="all"><ce:label>2.2</ce:label><ce:section-title id="st0040">The KLEAC database for sentence stress detection</ce:section-title><ce:para id="p0105" view="all">Non-native English learners have their own prosodic habits and characteristics depending on their language background. To increase the accuracy of sentence stress detection from learners' speech, we needed to adapt the prosodic characteristics of non-native learners' speech to our sentence stress detection model. In this model, we focused on sentence stresses imposed by Korean learners when they read English sentences. We used the KLEAC (KoreanLearners' English Accentuation Corpus) database, where sentence stresses imposed by Korean learners are annotated (cf. <ce:cross-ref id="crf0195" refid="bib0170">Lee, 2011</ce:cross-ref>) as displayed in <ce:cross-ref id="crf0200" refid="f0010">Fig.â1</ce:cross-ref><ce:float-anchor refid="f0010"/>. Naturally, some sentence stresses were found to be incorrectly placed, especially on function words.</ce:para><ce:para id="p0110" view="all">The KLEAC consists of six hours of speech with 5500 English sentences produced by 75 native Korean speakers (middle school students aged 13â14 years). In this corpus, sentence stress labels, but not RNU/ANA labels, were manually annotated by seven Korean phonetic experts using the Praat program (<ce:cross-ref id="crf0205" refid="bib0030">Boersma and Weenink, 2009</ce:cross-ref>). A set of annotation principles were established; most importantly, sentence stress was marked on syllables produced with pitch prominence, longer duration, and unreduced vowel quality. The labelers were instructed to pay greater attention to function words because imposing sentence stress on them is one of the major errors made by Korean learners of English. To improve the inter-transcriber agreement, the annotations were partly cross-checked between the labelers in the beginning stage. The inter-rater agreement was calculated for 524 sentences that were randomly selected from the corpus using Fless' Kappa; there was very strong inter-rater agreement among the labelers [Îºâ=â.868 (95% CI: .866 to .870), pâ&lt;â.001].</ce:para><ce:para id="p0115" view="all">To verify the reliability of the KLEAC labelers, five of them took part in an additional labeling task;<ce:cross-ref id="crf0210" refid="fn0025"><ce:sup loc="post">4</ce:sup></ce:cross-ref><ce:footnote id="fn0025"><ce:label>4</ce:label><ce:note-para id="np0025" view="all">Two labelers could not participate in this task because of personal reasons.</ce:note-para></ce:footnote> they were asked to mark sentence stresses imposed by English native speakers (not by Korean learners) on 50 utterances arbitrarily selected from the Aix-MARSEC database. Inter-rater agreements were then calculated between the Korean transcribers and the annotator(s) of the Aix-MARSEC database. Specifically, sentence stress labels made by each Korean labeler were compared with those made by the transcriber(s) who annotated the 50 chosen Aix-MARSEC utterances. The results of Cohen's Kappa analyses performed between each of the labelers and the Aix-MARSEC labeler(s) are shown in <ce:cross-ref id="crf0215" refid="t0015">Tableâ2</ce:cross-ref><ce:float-anchor refid="t0015"/>. Kappa values ranged between 0.61 and 0.83, which suggests that the agreement between the Korean labelers and the annotator(s) of the Aix-MARSEC database was substantial. Furthermore, a Fless' Kappa analysis demonstrated consistency among the Korean labelers [Îºâ=â.721 (95% CI: .713 to .729), pâ&lt;â.001]. The results of this labeling task indicate that the Korean labelers also transcribe native English sentence stress in a consistent manner.</ce:para></ce:section></ce:section><ce:section id="s0030" role="methods" view="all"><ce:label>3</ce:label><ce:section-title id="st0045">Methods</ce:section-title><ce:section id="s0035" view="all"><ce:label>3.1</ce:label><ce:section-title id="st0050">Overview of the sentence stress feedback system</ce:section-title><ce:para id="p0120" view="all">As mentioned above, our sentence stress feedback system aims to provide non-native learners with corrective feedback on English stress errors. This will allow learners to check and correct their own stress errors, which would eventually lead to the improvement of their English rhythm and fluency. To achieve this goal, we built an integrated sentence stress feedback system consisting of sentence stress prediction, detection, and feedback provision models, as in <ce:cross-ref id="crf0220" refid="f0015">Fig.â2</ce:cross-ref><ce:float-anchor refid="f0015"/>. The prediction and detection models were established using two different speech databases: the Aix-MARSEC database was used for the prediction model and the KLEAC for the detection model. In addition we developed a CRF (Conditional Random Field) classifier based on <ce:cross-ref id="crf0225" refid="bib0160">Lafferty etÂ al. (2001)</ce:cross-ref>.</ce:para><ce:para id="p0125" view="all">If a practice sentence is given to the system, the prediction model produces a reference sentence stress pattern that is generated on the basis of some syntactic and lexical features. These features will be discussed in <ce:cross-ref id="crf0230" refid="s0040">Section 3.2</ce:cross-ref>. When a non-native learner reads the practice sentence, the detection model analyses, extracts, and utilizes acoustic features to detect sentence stresses imposed on the utterance. By comparing the predicted sentence stresses with the detected ones, the feedback provision model informs the learner whether each word is stressed or unstressed correctly. In the proposed system, we carefully set up a feedback provision model so that it offers positive or negative stress feedback for every spoken word by comparing the probability of the predicted sentence stress pattern with that of the detected stress pattern.</ce:para><ce:para id="p0130" view="all">The proposed system is designed to detect sentence stress errors on the word level, but not on the syllable level, because sentence stress detection processed on the syllable level considerably lowers the accuracy of the system. Hence this system cannot detect lexical stress errors.</ce:para><ce:para id="p0135" view="all">The rates of sentence and lexical stress errors were measured as part of another study using 525 sentences arbitrarily chosen from the KLEAC corpus. The rate of lexical stress errors was only 0.4% (21 words out of total 4795 words) whereas the rate of sentence stress errors of function words was 20.7% (483 words out of 2333 function words). It follows that the inability of our system to detect lexical stress errors does not pose a serious problem in teaching English rhythm to Korean learners.</ce:para><ce:para id="p0140" view="all">Although the proposed system is designed to detect sentence stress errors of both content and function words, sentence stress errors of content words are rarely detected unless a content word is pronounced very weakly. This system is also designed to detect sentence stress errors in utterances of broad focus. Hence it is very likely that our system will judge sentence stress placed on a narrowly focused function word as an error.</ce:para></ce:section><ce:section id="s0040" view="all"><ce:label>3.2</ce:label><ce:section-title id="st0055">Sentence stress prediction model</ce:section-title><ce:para id="p0145" view="all">To construct a stress prediction model that automatically generates a reference stress pattern for any input sentence, we used the Aix-MARSEC database. The first syllable of each NRU in this database was regarded as having attracted sentence stress on the utterance level.</ce:para><ce:para id="p0150" view="all">In building our sentence stress feedback system for teaching English rhythm and fluency, we wanted the prediction model to accurately predict sentence stress placement when any new sentence was given without additional information. We also noted that the prediction of a word's sentence stress label was determined by the labels of neighboring words, not those of distant words. Hence we adopted the linear-chain CRF model which has been widely used in the previous studies on the automatic prediction of pitch accent, sentence stress, and boundary tones (<ce:cross-refs id="crfs0055" refid="bib0135 bib0230 bib0235">Jeon and Liu, 2009; Qian etÂ al., 2010; Rangarajan Sridhar etÂ al., 2008</ce:cross-refs>).</ce:para><ce:para id="p0155" view="all">The linear-chain CRF defines a conditional probability distribution of a label sequence <ce:italic>y</ce:italic>, given an observation sequence <ce:italic>x</ce:italic>. The distribution follows the relation between the labels encoded in a linear-chain structure. A linear-chain CRF is defined as follows:<ce:display><ce:formula id="e0010"><ce:label>(1)</ce:label><mml:math altimg="si1.gif" display="block" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>â</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>Î»</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mtext>â</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>â</mml:mtext><mml:mo>,</mml:mo><mml:mtext>â</mml:mtext><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:mo>+</mml:mo><mml:msub><mml:mi>Î¼</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mtext>â</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mtext>â</mml:mtext><mml:mo>,</mml:mo><mml:mtext>â</mml:mtext><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></ce:formula></ce:display>where <ce:bold>Z</ce:bold>(<ce:italic><ce:bold>x</ce:bold></ce:italic>) is a normalization constant that is computed by summing over all possible label sequences <ce:italic><ce:bold>y</ce:bold></ce:italic> of the observation sequence <ce:italic><ce:bold>x</ce:bold></ce:italic>. <ce:italic>f<ce:inf loc="post">k</ce:inf></ce:italic>(<ce:italic>y<ce:inf loc="post">tâ</ce:inf></ce:italic><ce:inf loc="post">1</ce:inf>, <ce:italic>y<ce:inf loc="post">t</ce:inf></ce:italic>) encodes a transition score for state transition <ce:italic>y<ce:inf loc="post">t</ce:inf></ce:italic><ce:inf loc="post">â1</ce:inf>â<ce:italic>y<ce:inf loc="post">t</ce:inf></ce:italic> and <ce:italic>g<ce:inf loc="post">k</ce:inf></ce:italic>(<ce:italic>y<ce:inf loc="post">t</ce:inf>, <ce:bold>x</ce:bold></ce:italic>) encodes the observation feature, <ce:italic><ce:bold>x</ce:bold></ce:italic>, centered at the current time, <ce:italic>t</ce:italic>. <ce:italic>Î»<ce:inf loc="post">k</ce:inf></ce:italic> and <ce:italic>Âµ<ce:inf loc="post">k</ce:inf></ce:italic> are trained parameters associated with the features <ce:italic>f<ce:inf loc="post">k</ce:inf></ce:italic> and <ce:italic>g<ce:inf loc="post">k</ce:inf></ce:italic> for a given feature <ce:italic>k</ce:italic>.</ce:para><ce:para id="p0160" view="all">Since content words tend to be stressed on the sentence level whereas function words are not, the word class, i.e. content or function word, was considered to be the most important feature among the several syntactic and lexical features that were generated based on POS (Part-of-Speech) tags automatically obtained by using our own POS tagger. This tagger was implemented following <ce:cross-ref id="crf0235" refid="bib0050">Brill (1992)</ce:cross-ref> and showed an accuracy of 96.3%.</ce:para><ce:para id="p0165" view="all">Based on the POS tags, we generated new syntactic and lexical features by combining them with a syntactic or lexical feature like word class, word identity, number of vowels, and number of syllables. These new features are WORD_CLASS+POS_TAG, WORD_IDENTITY+POS_TAG, VOWEL_ NUMBER+POS_TAG, and SYLLABLE_NUMBER+POS_TAG.</ce:para><ce:para id="p0170" view="all">The linear-chain CRF needs contextual information to appropriately predict the labels (stressed or unstressed) of individual words in a practice sentence. The two words preceding a target word and the three words following it were chosen as its contextual information. The optimal window size for this contextual information was empirically determined in the experiment.</ce:para></ce:section><ce:section id="s0045" view="all"><ce:label>3.3</ce:label><ce:section-title id="st0060">Sentence stress detection model</ce:section-title><ce:para id="p0175" view="all">Like the sentence stress prediction model, the sentence stress detection model was constructed with the same CRF classifier. While the prediction model was trained with the Aix-MARSEC database, we used the KLEAC database to train the detection model. This was done to reflect the acoustically unique characteristics of Korean learners' English so that the detection model could detect sentence stress in Korean English more accurately.</ce:para><ce:para id="p0180" view="all">Much like the classification task of the prediction model, an important part in developing the detection model was determining the necessary features. Acoustic features were essential in detecting imposed sentence stresses from learners' utterances. The acoustic features used for stress detection included syllable duration, vowel duration, the normalized mean pitch of the syllable, the normalized mean intensity of the syllable, and the duration of silence at the word boundary.</ce:para><ce:para id="p0185" view="all">According to previous research into prosody detection (<ce:cross-refs id="crfs0060" refid="bib0135 bib0230 bib0235">Jeon and Liu, 2009; Qian etÂ al., 2010; Rangarajan Sridhar etÂ al., 2008</ce:cross-refs>), appropriate syntactic and lexical features are also necessary to achieve high accuracy of prosody detection. Hence the same syntactic and lexical features used for the prediction model were used to develop the detection model.</ce:para><ce:para id="p0190" view="all">In the proposed stress detection model, the normalized pitch and intensity means were extracted from the mean pitch and intensity of lexically stressed vowels because sentence stress is normally imposed on syllables receiving primary word stress. To determine the timeline of the words in a given utterance, a forced alignment procedure was used.</ce:para><ce:para id="p0195" view="all">However, non-native English has various acoustic characteristics that prevent the stress detection model from detecting sentence stress accurately. To reduce undesirable fluctuations, acoustic features were normalized using the <ce:italic>z</ce:italic>-score. The <ce:italic>z</ce:italic>-score was calculated using the mean Âµ and the standard deviation Ï of the feature value <ce:italic>x</ce:italic>, as shown in the following sentence: <ce:italic>z</ce:italic>â=â(<ce:italic>x</ce:italic>Â âÂ Âµ) / Ï.</ce:para><ce:para id="p0200" view="all">Generally, the acoustic features were measured with continuous values. Before applying the continuous values to the CRF classifier, we discretized the continuous feature variables. To conduct quantile discretization, we introduced 10 bins where each bin received an equal number of data values. Through the quantile discretization procedure, we were able to alleviate the data sparseness problem.</ce:para></ce:section><ce:section id="s0050" view="all"><ce:label>3.4</ce:label><ce:section-title id="st0065">Sentence stress feedback provision model</ce:section-title><ce:para id="p0205" view="all">The probability scores of a word's predicted and detected stress levels were produced by the CRF classifier used in the stress prediction and detection models. These scores, which range from 0 to 1, reflect how confidently the models assess the stress level of each word. Sentence stress feedback signs are determined by comparing the stress probability scores produced by the prediction and detection models, word by word. As the absolute difference between the predicted and detected probability values of a word decreases, the detected stress level is assumed to correspond better to the predicted one. For example, for a word in a practice sentence, if the output probability score of the stress prediction model is 0.8 (likely to be stressed) and that of the stress detection model 0.2 (unlikely to be stressed), then the absolute difference is 0.6 (=|0.8Â âÂ 0.2|) and therefore the learner's stress placement on the word is likely to be different from the predicted standard stress pattern.</ce:para><ce:para id="p0210" view="all">Two feedback signs were used: âOâ was a positive feedback sign, meaning ârightâ, and âXâ was a negative feedback sign, meaning âwrongâ, as shown in <ce:cross-ref id="crf0240" refid="f0020">Fig.â3</ce:cross-ref><ce:float-anchor refid="f0020"/>. Positive feedback was used to encourage the learners' good performance. Negative feedback was used to inform the learners of their stress errors that need correction. Since both prediction and detection models can make errors in automatically judging word stress level, we had to consider the possibility of incorrect feedback signs being given to the learners. To avoid this problem and achieve reliability in the proposed system, the feedback model was designed so that no signs are offered if the absolute difference between predicted and detected probability scores falls within the range where neither positive nor negative feedback can be given confidently to the learners.</ce:para><ce:para id="p0215" view="all">The formula for the feedback is as follows:<ce:display><ce:formula id="e0015"><ce:label>(2)</ce:label><mml:math altimg="si2.gif" display="block" overflow="scroll"><mml:mrow><mml:mtext>Feedback</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Positive</mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>â</mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>Î¸</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>NoÂ sign</mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>â</mml:mtext><mml:msub><mml:mi>Î¸</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>â¤</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>â¤</mml:mo><mml:msub><mml:mi>Î¸</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Negative</mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext>â</mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>Ï</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>Î¸</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></ce:formula></ce:display></ce:para><ce:para id="p0220" view="all">The <ce:italic>Ï</ce:italic><ce:inf loc="post">pre</ce:inf> and <ce:italic>Ï</ce:italic><ce:inf loc="post">det</ce:inf> in (2) are output probability values derived by the prediction and detection models, respectively, and <ce:italic>Î¸</ce:italic><ce:inf loc="post">1</ce:inf> and <ce:italic>Î¸</ce:italic><ce:inf loc="post">2</ce:inf> are decision boundaries for the feedback signs.</ce:para></ce:section></ce:section><ce:section id="s0055" role="results" view="all"><ce:label>4</ce:label><ce:section-title id="st0070">Results</ce:section-title><ce:section id="s0060" view="all"><ce:label>4.1</ce:label><ce:section-title id="st0075">Evaluation of the sentence stress prediction and detection models</ce:section-title><ce:para id="p0225" view="all">To validate the performance of the stress prediction and detection models, the utterances in the Aix-MARSEC and KLEAC databases were divided either into a test or a training set, respectively, in a ratio of approximately 4:1. Using the datasets, we conducted five-fold cross validation for the prediction and detection models to calculate the accuracy, precision, recall, and F-measure values of these models, as in <ce:cross-ref id="crf0245" refid="t0020">Tableâ3</ce:cross-ref><ce:float-anchor refid="t0020"/>, and verified how accurately the proposed models predict and detect sentence stress. These values had been checked first because the proposed system manipulated the binary classification problem in which moderate accuracy could be achieved by selecting either stressed or unstressed words as holding a majority. Since the Aix-MARSEC database used for the prediction model showed an accuracy of 60.5% when selecting stressed words as holding a majority and the KLEAC database used for the detection model was 58.6% accurate under the same condition, we selected stressed words as holding a majority. To improve the accuracy of the prediction and detection models from the baseline accuracy, we adopted the machine learning method using a linear-chain CRF. As a result, the accuracy of both models was greatly improved as can be seen in <ce:cross-ref id="crf0250" refid="t0020">Tableâ3</ce:cross-ref>.</ce:para><ce:para id="p0230" view="all">Compared to our previous pitch accent prediction model (<ce:cross-ref id="crf0255" refid="bib0150">Kang etÂ al., 2012</ce:cross-ref>), with which the accuracy of sentence stress prediction was only 83.7% <ce:cross-ref id="crf0260" refid="fn0030"><ce:sup loc="post">5</ce:sup></ce:cross-ref><ce:footnote id="fn0030"><ce:label>5</ce:label><ce:note-para id="np0030" view="all">The accuracy and F-measure of pitch accent prediction were respectively 87.3% and 88.3%, based on the BURNC.</ce:note-para></ce:footnote>, sentence stress prediction accuracy greatly improved because the Aix-MARSEC database offered much more appropriate information about sentence stress placement. Although the accuracy of the detection model was much lower than that of the prediction model, the accuracy and F-measure of the detection model reached 84.1% and 86.7%, respectively. It led us to believe that the detection model could be effectively used in a real teaching environment if the decision boundaries were appropriately assigned in the feedback provision model.</ce:para><ce:para id="p0235" view="all"><ce:cross-ref id="crf0265" refid="t0025">Tableâ4</ce:cross-ref><ce:float-anchor refid="t0025"/> shows the confusion matrices of the prediction and detection results. It shows the number of words âlabeledâ as âstressedâ or âunstressedâ in the KLEAC and the number of words âpredictedâ or âdetectedâ as âstressedâ or âunstressedâ with our prediction and detection models.</ce:para><ce:para id="p0240" view="all">We can confirm from <ce:cross-ref id="crf0270" refid="t0025">Tableâ4</ce:cross-ref> that the prediction model results in higher accuracy than the detection model and that the error rate of stressed word detection is reasonably low, i.e. 11.2% (=Â 626Â /Â (626Â +Â 4959)). It follows that reliable sentence stress prediction and detection results are attainable with our prediction and detection models.</ce:para></ce:section><ce:section id="s0065" view="all"><ce:label>4.2</ce:label><ce:section-title id="st0080">Determining feedback decision boundaries</ce:section-title><ce:para id="p0245" view="all">Unlike the prediction and detection models, a validation of the feedback provision model should be based on how optimally the decision boundaries for feedback are decided so that it can help non-native learners correct sentence stress errors. Iterative calculations to find optimum boundaries were conducted until the correlation between the learners' feedback scores and their proficiency levels were maximized. If the feedback given to a learner was highly correlated with this learner' proficiency level based on the corresponding feedback decision boundaries, we verified that the feedback provision model with these boundaries were reliable and hence useful in a real learning environment.</ce:para><ce:para id="p0250" view="all">To measure the appropriateness of the feedback provision model, we calculated Pearson's correlation coefficients between feedback-derived scores (Eq. <ce:cross-ref id="crf0275" refid="e0020">3</ce:cross-ref>) and the proficiency levels of the Korean learners that participated in the KLEAC database recordings.<ce:cross-ref id="crf0280" refid="fn0035"><ce:sup loc="post">6</ce:sup></ce:cross-ref><ce:footnote id="fn0035"><ce:label>6</ce:label><ce:note-para id="np0035" view="all">The proficiency levels were assessed by 7 native speakers. They were graduate students majoring in linguistics.</ce:note-para></ce:footnote><ce:display><ce:formula id="e0020"><ce:label>(3)</ce:label><mml:math altimg="si3.gif" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">Feedback</mml:mi><mml:mtext>â</mml:mtext><mml:mi mathvariant="italic">driven</mml:mi><mml:mtext>â</mml:mtext><mml:mi mathvariant="italic">score</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Number</mml:mi><mml:mtext>â</mml:mtext><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mtext>â</mml:mtext><mml:mo>"</mml:mo><mml:mi mathvariant="italic">positive</mml:mi><mml:mo>"</mml:mo><mml:mtext>â</mml:mtext><mml:mtext>Â </mml:mtext><mml:mi mathvariant="italic">feedback</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Total</mml:mi><mml:mtext>â</mml:mtext><mml:mi mathvariant="italic">number</mml:mi><mml:mtext>â</mml:mtext><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mtext>â</mml:mtext><mml:mo>"</mml:mo><mml:mi mathvariant="italic">positive</mml:mi><mml:mo>"</mml:mo><mml:mtext>â</mml:mtext><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mtext>â</mml:mtext><mml:mo>"</mml:mo><mml:mi mathvariant="italic">negative</mml:mi><mml:mo>"</mml:mo><mml:mtext>â</mml:mtext><mml:mi mathvariant="italic">feedback</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></ce:formula></ce:display></ce:para><ce:para id="p0255" view="all">The utterances in the KLEAC database were split into a ratio of 4:1. Four fifths of all utterances were used to construct a new detection model and this model was then applied to the remaining one fifth of the utterances to derive the feedback-derived scores.</ce:para><ce:para id="p0260" view="all">To measure the correlation between feedback scores and the proficiency levels of the Korean learners recorded in the KLEAC database, a five-fold cross validation process was conducted cyclically 5 times to avoid overfitting. Through the 5 iterative processes, each utilizing a new detection model, the feedback scores eventually took all of the KLEAC utterances into consideration; these scores were then correlated to the speakers' proficiency levels.</ce:para><ce:para id="p0265" view="all">An iterative search for sentence stress was performed to find the <ce:italic>Î¸</ce:italic><ce:inf loc="post">1</ce:inf> and <ce:italic>Î¸</ce:italic><ce:inf loc="post">2</ce:inf> values that yielded the highest correlation between feedback scores and speaker proficiency levels. This was done by changing the <ce:italic>Î¸</ce:italic><ce:inf loc="post">1</ce:inf> and <ce:italic>Î¸</ce:italic><ce:inf loc="post">2</ce:inf> values in increments of 0.01, between 0 and 1. As a result, the search found that the highest correlation value of 0.5855 was achieved when <ce:italic>Î¸</ce:italic><ce:inf loc="post">1</ce:inf> and <ce:italic>Î¸</ce:italic><ce:inf loc="post">2</ce:inf> were respectively set as 0.18 and 0.61, as can be seen in <ce:cross-ref id="crf0285" refid="f0025">Fig.â4</ce:cross-ref><ce:float-anchor refid="f0025"/>.</ce:para><ce:para id="p0270" view="all"><ce:cross-ref id="crf0290" refid="f0030">Fig.â5</ce:cross-ref><ce:float-anchor refid="f0030"/> displays the distribution of the feedback scores over the proficiency levels, based on the optimum boundaries. This figure shows that the feedback scores increase with the proficiency levels. This leads us to the assertion that our stress feedback provision model offers reasonably good sentence stress feedback, with which non-native learners can correct their stress errors and thereby improve their English rhythm and fluency.</ce:para></ce:section><ce:section id="s0070" view="all"><ce:label>4.3</ce:label><ce:section-title id="st0085">Educational effect of the proposed system</ce:section-title><ce:para id="p0275" view="all">To evaluate the educational effect of the proposed sentence stress feedback system, we conducted an additional training experiment by building an online CALL system in which our sentence stress feedback system was embedded. 40 Korean elementary school students consisting of 19 boys and 21 girls participated in this experiment. 38 of the students were approximately 12 years old (6th graders) and 2 of them were 11 years old (5th graders). They were enrolled in an English course at a private English institute in Seoul, Korea. Most of the students were assessed by the institute to be between lower and upper intermediate proficiency levels and their experience living in an English speaking environment was limited; only two students had experience residing in an English speaking community and their length of residence was less than six months.</ce:para><ce:para id="p0280" view="all">During the training period, all 40 students took English classes both at school and at the institute. As a part of their homework for the institute course, the students were requested to solve listening problems and repeat after native speakers' read sentences. These exercises were completed online. In addition, they received High Variability Phonetic Training.</ce:para><ce:para id="p0285" view="all">In High Variability Phonetic Training, listeners are exposed to highly variable sound stimuli produced by multiple talkers in multiple phonetic contexts. Since learners can learn the relevant phonetic cues that apply to different listening situations, High Variability Phonetic Training has been proven to be very effective in improving not only learners' sound discrimination ability (cf. <ce:cross-refs id="crfs0065" refid="bib0190 bib0200">Lively etÂ al., 1993; Logan etÂ al., 1991</ce:cross-refs>) but also their pronunciation accuracy (<ce:cross-ref id="crf0295" refid="bib0045">Bradlow etÂ al., 1995</ce:cross-ref>). In this training, the participants listened to English minimal pairs or triplets that differed only in a vowel sound or a consonant sound, such as â<ce:italic>f<ce:underline>ee</ce:underline>t-f<ce:underline>i</ce:underline>t</ce:italic>â and â<ce:italic><ce:underline>t</ce:underline>in-<ce:underline>th</ce:underline>in</ce:italic>â. These word pairs or triplets were presented both in isolation and embedded in sentences. 13 vowels and 17 consonants were included and each pair or triplet was made of phonemes that were thought to be confusable to Korean learners of English. In an attempt to assess the educational effect of our sentence stress feedback system, 20 participants were classified as the experiment group and they took part in additional English rhythm practice sessions using our online CALL system where the proposed sentence stress feedback system was incorporated. The 20 subjects in the control group were not given any conventional rhythm training because the purpose of this training experiment was not to prove that our method is superior to conventional teaching methods, but to observe whether the proposed system is effective in teaching English rhythm without teachers' instruction.</ce:para><ce:para id="p0290" view="all">These training sessions were conducted 5 days a week, for 4 weeks. One High Variability Phonetic Training session took approximately 10 to 15 minutes and the average time required to complete one rhythm practice session with our system was 8.8 minutes. They also participated in a pretest and posttest during the first and last training session, respectively.</ce:para><ce:para id="p0295" view="all">For the pretest and posttest, participants were asked to read the âRainbowâ passage, which is one of the most common standard reading passages used in speech evaluation tests, and their read speech was recorded. The recordings obtained from the pretest and posttest were randomized in order and presented to three native English speakers majoring in linguistics. They assessed the comprehensibility, accentedness, and rhythm of each participant's recordings on a 9 point scale, where 1 stood for âimpossible to understand or not at all native-likeâ and 9 stood for âextremely easy to understand or native-likeâ (cf. <ce:cross-ref id="crf0300" refid="bib0215">Munro and Derwing, 1995</ce:cross-ref> for an assessment of this type). The listeners were instructed to assess the speech rhythm of each participant based on sentence stress placement and the degree of stress-timing.</ce:para><ce:para id="p0300" view="all">The results of the assessment in <ce:cross-ref id="crf0305" refid="t0030">Tableâ5</ce:cross-ref><ce:float-anchor refid="t0030"/> suggest that only the participants in the experiment group, who used our sentence stress feedback system, showed significant improvements in rhythm and accentedness scores. Paired t-tests revealed that the accentedness and rhythm scores of each participant's post-test recording (accentedness: Mean: 4.95, SD: 1.69; rhythm: Mean: 5.05, SD: 2.04) were significantly different from those of their pre-test recording (accentedness: M: 4.33, SD: 1.85; rhythm: M: 4.37, SD: 2.03) in the experiment group (accentedness: t(dfâ=â59)â=ââ3.43, pâ&lt;â.05; rhythm: t(dfâ=â59)â=ââ3.84, pâ&lt;â.001), but not in the control group. Specifically, the participants in the experiment group received significantly higher accentedness and rhythm scores after the training, which suggests that their speech was rated as âless foreign-accentedâ and âhaving more native-like rhythmâ after the training was received.</ce:para><ce:para id="p0305" view="all">Additionally, the changes in the scores were converted into ratios to compare the degree of improvement between the two groups more precisely. The improvement ratio was calculated by subtracting a pre-test score from a post-test score, dividing the difference by the pre-test score, and then multiplying the ratio by 100, so that the value is converted into a percentage. As shown in <ce:cross-ref id="crf0320" refid="t0035">Tableâ6</ce:cross-ref><ce:float-anchor refid="t0035"/>, the improvement ratios of their accentedness and rhythm scores were 14.3% and 15.5%, respectively, whereas those of the participants in the control group were only 4.7% and 5.5%, respectively. This suggests that our system was successful in increasing learners' accentedness and rhythm scores by 10%.</ce:para><ce:para id="p0310" view="all">This training experiment was conducted to evaluate the educational effect of our CALL system. The results demonstrate that learners who receive sentence stress training with our CALL system can improve their use of English rhythm to a greater extent than those who do not in comparison. However, their comprehensibility scores did not improve after the training. This result is not surprising because the correlation between comprehensibility and accentedness is not strong (<ce:cross-refs id="crfs0070" refid="bib0075 bib0215">Derwing and Munro, 1997; Munro and Derwing, 1995</ce:cross-refs>); comprehensibility is affected by a wider range of factors than accentedness, including those such as speech rate and word recognition (<ce:cross-ref id="crf0325" refid="bib0145">Jun and Li, 2010</ce:cross-ref>). Since our training focused on improving learners' English rhythm in a short period of time, it appears that our training was not sufficient to improve learners' overall spoken English performance. Nonetheless, the native listeners' ratings clearly show that our sentence stress feedback system helps non-native learners produce English rhythm in a more native-like way.</ce:para></ce:section></ce:section><ce:section id="s0075" role="conclusion" view="all"><ce:label>5</ce:label><ce:section-title id="st0090">Discussion and conclusion</ce:section-title><ce:para id="p0315" view="all">So far, we have described our unique sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined to offer feedback on sentence stress errors. The objective of the system is to provide non-native learners with corrective feedback so that they can improve their English rhythm and fluency.</ce:para><ce:para id="p0320" view="all">The reference stress pattern of an input sentence is extracted from a stress prediction model trained with the Aix-MARSEC database. By using the Aix-MARSEC database, in which sentence stress is explicitly annotated, we were able to achieve an accuracy of 96.6% in sentence stress prediction. This accuracy is much higher than the accuracy of the pitch accent prediction model trained with the BURNC in our previous study (<ce:cross-ref id="crf0330" refid="bib0150">Kang etÂ al., 2012</ce:cross-ref>).</ce:para><ce:para id="p0325" view="all">To reflect the characteristic prosodic properties of Korean learners' English in the detection model, the KLEAC database was used to train the stress detection model; in the KLEAC database, the sentence stresses imposed by Korean learners are marked. As a result, the accuracy of the detection model was 84.1%. The proposed stress detection model detects the sentence stress of any input sentence with this accuracy whereas the detection model developed by <ce:cross-ref id="crf0335" refid="bib0115">Imoto etÂ al. (2002)</ce:cross-ref> requires an existing reference utterance of a native speaker, which makes their system limited in the number of the utterances that can be input.</ce:para><ce:para id="p0330" view="all">To automatically provide non-native learners with appropriate feedback on their sentence stress errors, a feedback provision model was constructed. This model was constructed based on the output deviations of the prediction and detection models and the optimum feedback decision boundaries that showed maximum correlation between learners' feedback scores and the KLEAC database speakers' proficiency levels. The decision boundaries <ce:italic>Î¸</ce:italic><ce:inf loc="post">1</ce:inf> and <ce:italic>Î¸</ce:italic><ce:inf loc="post">2</ce:inf> determined for the feedback provision model were 0.18 and 0.61, respectively.</ce:para><ce:para id="p0335" view="all">In an experiment that evaluated the usefulness of the proposed sentence stress feedback system, we found that the students who trained with our system significantly improved their accentedness and rhythm, and improvement ratios were approximately 10% higher in the experiment than in the control group. Therefore, we believe that our sentence stress feedback system will be useful in helping non-native learners improve their English rhythm and fluency in a non-classroom environment.</ce:para><ce:para id="p0340" view="all">The problem with our sentence stress feedback system is that it inevitably produces errors in sentence stress prediction, detection, and feedback because of the imperfection of sentence stress labels in the Aix-MARSEC and KLEAC databases as well as the POS tagger we used. We believe, however, that non-native learners will be more satisfied with the current version of this system if this system is combined with good learning contents in an appropriate way.</ce:para></ce:section></ce:sections><ce:acknowledgment id="ac0010" view="all"><ce:section-title id="st0095">Acknowledgments</ce:section-title><ce:para id="p0345" view="all">This work was supported by Basic Science Research Program through the <ce:grant-sponsor id="gsp0010" sponsor-id="http://dx.doi.org/10.13039/501100003725" xlink:type="simple" xlink:role="http://www.elsevier.com/xml/linking-roles/grant-sponsor">National Research Foundation of Korea</ce:grant-sponsor> (NRF) funded by the Ministry of Education, Science and Technology (<ce:grant-number id="gnum0010" refid="gsp0010">2012-0008835</ce:grant-number>). We are grateful to GLEC Institute for kindly helping us conduct the training experiment at their facility. We are also grateful to anonymous reviewers for their invaluable comments and to Sylvia Cho for proofreading the draft of this paper.</ce:para></ce:acknowledgment></body><tail view="all"><ce:bibliography id="bb0010" view="all"><ce:section-title id="st0100">References</ce:section-title><ce:bibliography-sec id="bs0010" view="all"><ce:bib-reference id="bib0010"><ce:label>Abercrombie, 1964</ce:label><sb:reference id="sr0010"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Abercrombie</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Syllable quantity and enclitics in English</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:editors><sb:editor><ce:given-name>D.</ce:given-name><ce:surname>Abercrombie</ce:surname></sb:editor><sb:editor><ce:given-name>D.</ce:given-name><ce:surname>Jones</ce:surname></sb:editor></sb:editors><sb:title><sb:maintitle>In Honour of Daniel Jones</sb:maintitle></sb:title><sb:date>1964</sb:date><sb:publisher><sb:name>Longmans</sb:name><sb:location>London</sb:location></sb:publisher></sb:edited-book><sb:pages><sb:first-page>216</sb:first-page><sb:last-page>222</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0015"><ce:label>Abercrombie, 1967</ce:label><sb:reference id="sr0015"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Abercrombie</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Elements of General Phonetics</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1967</sb:date><sb:publisher><sb:name>Edinburgh University Press</sb:name><sb:location>Edinburgh</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0020"><ce:label>Anderson-Hsieh et al, 1992</ce:label><sb:reference id="sr0020"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Anderson-Hsieh</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Johnson</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Koehler</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The relationship between native speaker judgment of non-native pronunciation and deviance in segments, prosody, and syllable structure</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Lang. Learn</sb:maintitle></sb:title><sb:volume-nr>42</sb:volume-nr></sb:series><sb:date>1992</sb:date></sb:issue><sb:pages><sb:first-page>529</sb:first-page><sb:last-page>555</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0025"><ce:label>Bang et al, 2013</ce:label><sb:reference id="sr0025"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Bang</ce:surname></sb:author><sb:author><ce:surname>Kang</ce:surname><ce:given-name>S.</ce:given-name></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Lee</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>An automatic feedback system for English speaking integrating pronunciation and prosody assessments</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of SLaTE 2013</sb:maintitle></sb:title><sb:date>2013</sb:date></sb:edited-book><sb:pages><sb:first-page>83</sb:first-page><sb:last-page>89</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0030"><ce:label>Boersma, Weenink, 2009</ce:label><sb:reference id="sr0030"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Boersma</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Weenink</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Praat: doing phonetics by computer (Version 5.1.05)</sb:maintitle></sb:title></sb:contribution><sb:comment>[Computer program]</sb:comment><sb:host><sb:e-host><ce:inter-ref id="iw0010" xlink:href="http://www.praat.org/" xlink:type="simple">http://www.praat.org/</ce:inter-ref><sb:date>2009</sb:date></sb:e-host></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0035"><ce:label>Bolinger, 1958</ce:label><sb:reference id="sr0035"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.L.M.</ce:given-name><ce:surname>Bolinger</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A theory of pitch accent in English</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Word</sb:maintitle></sb:title><sb:volume-nr>14</sb:volume-nr></sb:series><sb:issue-nr>2â3</sb:issue-nr><sb:date>1958</sb:date></sb:issue><sb:pages><sb:first-page>109</sb:first-page><sb:last-page>149</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0040"><ce:label>Boula de MareÃ¼il, Vieru-Dimulescu, 2006</ce:label><sb:reference id="sr0040"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Boula de MareÃ¼il</ce:surname></sb:author><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Vieru-Dimulescu</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The contribution of prosody to the perception of foreign accent</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Phonetica</sb:maintitle></sb:title><sb:volume-nr>63</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>2006</sb:date></sb:issue><sb:pages><sb:first-page>247</sb:first-page><sb:last-page>267</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0045"><ce:label>Bradlow et al, 1995</ce:label><sb:reference id="sr0045"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.R.</ce:given-name><ce:surname>Bradlow</ce:surname></sb:author><sb:author><ce:given-name>D.B.</ce:given-name><ce:surname>Pisoni</ce:surname></sb:author><sb:author><ce:given-name>R.A.</ce:given-name><ce:surname>Yamada</ce:surname></sb:author><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Tohkura</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Training Japanese listeners to identify English /r/ and /l/: IV. Some effects of perceptual learning on speech production</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. Acoust. Soc. Am</sb:maintitle></sb:title><sb:volume-nr>101</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>1995</sb:date></sb:issue><sb:pages><sb:first-page>2299</sb:first-page><sb:last-page>2310</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0050"><ce:label>Brill, 1992</ce:label><sb:reference id="sr0050"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Brill</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>A simple rule-based part of speech tagger</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the third conference on Applied Natural Language Processing, Morristown</sb:maintitle></sb:title><sb:date>1992</sb:date></sb:edited-book><sb:pages><sb:first-page>112</sb:first-page><sb:last-page>116</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0055"><ce:label>Chandel et al, 2007</ce:label><sb:reference id="sr0055"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Chandel</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Parate</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Madathingal</ce:surname></sb:author><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Pant</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Rajput</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Ikbal</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>Sensei: spoken language assessment for call center agents</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>2007 IEEE Workshop on Automatic Speech Recognition &amp; Understanding</sb:maintitle></sb:title><sb:date>2007</sb:date></sb:edited-book><sb:pages><sb:first-page>711</sb:first-page><sb:last-page>716</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0060"><ce:label>Chen, Zechner, 2011</ce:label><sb:reference id="sr0060"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Chen</ce:surname><ce:given-name>L.</ce:given-name></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Zechner</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Applying rhythm features to automatically assess non-native speech</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of INTERSPEECH 2011</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book><sb:pages><sb:first-page>1861</sb:first-page><sb:last-page>1864</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0065"><ce:label>Cheng, 2011</ce:label><sb:reference id="sr0065"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Cheng</ce:surname><ce:given-name>J.</ce:given-name></sb:author></sb:authors><sb:title><sb:maintitle>Automatic assessment of prosody in high-stakes English tests</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of INTERSPEECH 2011</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book><sb:pages><sb:first-page>1589</sb:first-page><sb:last-page>1592</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0070"><ce:label>Dauer, 1983</ce:label><sb:reference id="sr0070"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>R.M.</ce:given-name><ce:surname>Dauer</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Stress-timing and syllable-timing reanalyzed</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. Phon</sb:maintitle></sb:title><sb:volume-nr>11</sb:volume-nr></sb:series><sb:date>1983</sb:date></sb:issue><sb:pages><sb:first-page>51</sb:first-page><sb:last-page>62</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0075"><ce:label>Derwing, Munro, 1997</ce:label><sb:reference id="sr0075"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>T.M.</ce:given-name><ce:surname>Derwing</ce:surname></sb:author><sb:author><ce:given-name>M.J.</ce:given-name><ce:surname>Munro</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Accent, intelligibility, and comprehensibility: evidence from four L1s</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Stud. Second Lang. Acquis</sb:maintitle></sb:title><sb:volume-nr>20</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>1997</sb:date></sb:issue><sb:pages><sb:first-page>1</sb:first-page><sb:last-page>16</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0080"><ce:label>Derwing, Rossiter, 2003</ce:label><sb:reference id="sr0080"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>T.M.</ce:given-name><ce:surname>Derwing</ce:surname></sb:author><sb:author><ce:given-name>M.J.</ce:given-name><ce:surname>Rossiter</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The effects of pronunciation instruction on the accuracy, fluency, and complexity of L2 accented speech</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Appl. Lang. Learn</sb:maintitle></sb:title><sb:volume-nr>13</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2003</sb:date></sb:issue><sb:pages><sb:first-page>1</sb:first-page><sb:last-page>18</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0085"><ce:label>Derwing et al, 1998</ce:label><sb:reference id="sr0085"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>T.M.</ce:given-name><ce:surname>Derwing</ce:surname></sb:author><sb:author><ce:given-name>M.J.</ce:given-name><ce:surname>Munro</ce:surname></sb:author><sb:author><ce:given-name>G.</ce:given-name><ce:surname>Wiebe</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Evidence in favor of a broad framework for pronunciation instruction</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Lang. Learn</sb:maintitle></sb:title><sb:volume-nr>48</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>1998</sb:date></sb:issue><sb:pages><sb:first-page>393</sb:first-page><sb:last-page>410</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0090"><ce:label>Elliott, 1997</ce:label><sb:reference id="sr0090"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.R.</ce:given-name><ce:surname>Elliott</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>On the teaching and acquisition of pronunciation within a communicative approach</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Hispania</sb:maintitle></sb:title><sb:volume-nr>80</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>1997</sb:date></sb:issue><sb:pages><sb:first-page>95</sb:first-page><sb:last-page>108</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0095"><ce:label>Gimson, 1980</ce:label><sb:reference id="sr0095"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.C.</ce:given-name><ce:surname>Gimson</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>An Introduction to the Pronunciation of English</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:edition>third ed.</sb:edition><sb:date>1980</sb:date><sb:publisher><sb:name>Edward Arnold</sb:name><sb:location>London</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0100"><ce:label>Hahn, 2004</ce:label><sb:reference id="sr0100"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>L.D.</ce:given-name><ce:surname>Hahn</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Primary stress and intelligibility: research to motivate the teaching of suprasegmentals</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>TESOL Quart</sb:maintitle></sb:title><sb:volume-nr>38</sb:volume-nr></sb:series><sb:issue-nr>2</sb:issue-nr><sb:date>2004</sb:date></sb:issue><sb:pages><sb:first-page>201</sb:first-page><sb:last-page>223</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0105"><ce:label>Hirst et al, 2009</ce:label><sb:reference id="sr0105"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Hirst</ce:surname></sb:author><sb:author><ce:given-name>C.</ce:given-name><ce:surname>De Looze</ce:surname></sb:author><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Auran</ce:surname></sb:author><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Bouzon</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Aix-MARSEC: a database for the analysis of the prosody of British English</sb:maintitle></sb:title></sb:contribution><sb:comment>Database available from the Speech &amp; Language Data Repository</sb:comment><sb:host><sb:e-host><ce:inter-ref id="iw0015" xlink:href="http://sldr.org/sldr000033/en" xlink:type="simple">http://sldr.org/sldr000033/en</ce:inter-ref><sb:date>2009</sb:date></sb:e-host></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0110"><ce:label>HÃ¶nig et al, 2010</ce:label><sb:reference id="sr0110"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>F.</ce:given-name><ce:surname>HÃ¶nig</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Batliner</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Weilhammer</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>NÃ¶th</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Automatic assessment of nonnative prosody for English as L2</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of Speech Prosody</sb:maintitle></sb:title><sb:date>2010</sb:date></sb:edited-book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0115"><ce:label>Imoto et al, 2002</ce:label><sb:reference id="sr0115"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Imoto</ce:surname></sb:author><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Tsubota</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Raux</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Kawahara</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Dantsuji</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Modeling and automatic detection of English sentence stress for computer-assisted English prosody learning system</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of ICSLP, Denver</sb:maintitle></sb:title><sb:date>2002</sb:date></sb:edited-book><sb:pages><sb:first-page>749</sb:first-page><sb:last-page>752</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0120"><ce:label>Ito et al, 2009</ce:label><sb:reference id="sr0120"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Ito</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Konno</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Ito</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Makino</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Evaluation of English intonation based on combination of multiple evaluation scores</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of INTERSPEECH 2009</sb:maintitle></sb:title><sb:date>2009</sb:date></sb:edited-book><sb:pages><sb:first-page>596</sb:first-page><sb:last-page>599</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0125"><ce:label>Jassem, 1952</ce:label><sb:reference id="sr0125"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>W.</ce:given-name><ce:surname>Jassem</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Stress in modern English</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Bull. de la Soc. Polonaise de Linguistique</sb:maintitle></sb:title><sb:volume-nr>12</sb:volume-nr></sb:series><sb:date>1952</sb:date></sb:issue><sb:pages><sb:first-page>189</sb:first-page><sb:last-page>194</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0130"><ce:label>Jassem, 1999</ce:label><sb:reference id="sr0130"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>W.</ce:given-name><ce:surname>Jassem</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>English stress, accent and intonation revisited</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Speech and Lang. Technol</sb:maintitle></sb:title><sb:volume-nr>3</sb:volume-nr></sb:series><sb:date>1999</sb:date></sb:issue><sb:pages><sb:first-page>33</sb:first-page><sb:last-page>50</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0135"><ce:label>Jeon, Liu, 2009</ce:label><sb:reference id="sr0135"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.H.</ce:given-name><ce:surname>Jeon</ce:surname></sb:author><sb:author><ce:surname>Liu</ce:surname><ce:given-name>Y.</ce:given-name></sb:author></sb:authors><sb:title><sb:maintitle>Automatic prosodic events detection using syllable-based acoustic and syntactic features</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing 2009</sb:maintitle></sb:title><sb:date>2009</sb:date></sb:edited-book><sb:pages><sb:first-page>4565</sb:first-page><sb:last-page>4568</sb:last-page></sb:pages></sb:host><sb:comment>Taipei</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0140"><ce:label>Jones, 1972</ce:label><sb:reference id="sr0140"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Jones</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>An outline of English phonetics</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:edition>ninth ed.</sb:edition><sb:date>1972</sb:date><sb:publisher><sb:name>Heffer</sb:name><sb:location>Cambridge</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0145"><ce:label>Jun, Li, 2010</ce:label><sb:reference id="sr0145"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>H.G.</ce:given-name><ce:surname>Jun</ce:surname></sb:author><sb:author><ce:surname>Li</ce:surname><ce:given-name>J.</ce:given-name></sb:author></sb:authors><sb:title><sb:maintitle>Factors in raters' perceptions of comprehensibility and accentedness</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 1st Pronunciation in Second Language Learning and Teaching Conference</sb:maintitle></sb:title><sb:date>2010</sb:date><sb:publisher><sb:name>Iowa State University</sb:name></sb:publisher></sb:edited-book><sb:pages><sb:first-page>53</sb:first-page><sb:last-page>66</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0150"><ce:label>Kang et al, 2012</ce:label><sb:reference id="sr0150"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Kang</ce:surname><ce:given-name>S.</ce:given-name></sb:author><sb:author><ce:given-name>G.G.</ce:given-name><ce:surname>Lee</ce:surname></sb:author><sb:author><ce:given-name>H.Y.</ce:given-name><ce:surname>Lee</ce:surname></sb:author><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Kim</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>An automatic pitch accent feedback system for English learners with adaptation of an English corpus spoken by Koreans</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of IEEE Workshop on Spoken Language Technology (SLT 2012), Miami</sb:maintitle></sb:title><sb:date>2012</sb:date></sb:edited-book><sb:pages><sb:first-page>432</sb:first-page><sb:last-page>437</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0155"><ce:label>Kingdon, 1958</ce:label><sb:reference id="sr0155"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Kingdon</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The Groundwork of English Stress</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1958</sb:date><sb:publisher><sb:name>Longmans</sb:name><sb:location>London</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0160"><ce:label>Lafferty et al, 2001</ce:label><sb:reference id="sr0160"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Lafferty</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>McCallum</ce:surname></sb:author><sb:author><ce:given-name>F.C.N.</ce:given-name><ce:surname>Pereira</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Conditional random fields: probabilistic models for segmenting and labeling sequence data</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the Eighteenth International Conference on Machine Learning 2001</sb:maintitle></sb:title><sb:date>2001</sb:date></sb:edited-book><sb:pages><sb:first-page>282</sb:first-page><sb:last-page>289</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0165"><ce:label>Lee et al, 2010</ce:label><sb:reference id="sr0165"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Lee</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Jung</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Kim</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Lee</ce:surname></sb:author><sb:author><ce:given-name>G.G.</ce:given-name><ce:surname>Lee</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Recent approaches to dialog management for spoken dialog systems</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. Comput. Sci. Eng</sb:maintitle></sb:title><sb:volume-nr>4</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2010</sb:date></sb:issue><sb:pages><sb:first-page>1</sb:first-page><sb:last-page>22</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0170"><ce:label>Lee, 2011</ce:label><sb:reference id="sr0170"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>H.Y.</ce:given-name><ce:surname>Lee</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Evaluation of Korean Learners' English Accentuation</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 16th National Conference of the English Phonetic Society of Japan and the Second International Congress of Phoneticians of English</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book><sb:pages><sb:first-page>23</sb:first-page><sb:last-page>25</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0175"><ce:label>Lee et al, 2011</ce:label><sb:reference id="sr0175"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Lee</ce:surname></sb:author><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Noh</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Lee</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Lee</ce:surname></sb:author><sb:author><ce:given-name>G.G.</ce:given-name><ce:surname>Lee</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Sagong</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>On the effectiveness of robot-assisted language learning</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>ReCALL J.</sb:maintitle></sb:title><sb:volume-nr>23</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>2011</sb:date></sb:issue><sb:pages><sb:first-page>25</sb:first-page><sb:last-page>58</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0180"><ce:label>Levy, 1997</ce:label><sb:reference id="sr0180"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Levy</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>CALL: Context and Conceptualisation</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1997</sb:date><sb:publisher><sb:name>Oxford University Press</sb:name><sb:location>Oxford</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0185"><ce:label>Liscombe, 2007</ce:label><sb:reference id="sr0185"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Liscombe</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Prosody and speaker state: paralinguistics, pragmatics, and proficiency</sb:maintitle></sb:title></sb:contribution><sb:comment>PhD Dissertation; Columbia University</sb:comment><sb:host><sb:book><sb:date>2007</sb:date></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0190"><ce:label>Lively et al, 1993</ce:label><sb:reference id="sr0190"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.E.</ce:given-name><ce:surname>Lively</ce:surname></sb:author><sb:author><ce:given-name>J.S.</ce:given-name><ce:surname>Logan</ce:surname></sb:author><sb:author><ce:given-name>D.B.</ce:given-name><ce:surname>Pisoni</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Training Japanese listeners to identify English /r/ and /l/. II: The role of phonetic environment and talker variability in learning new perceptual categories</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. Acoust. Soc. Am</sb:maintitle></sb:title><sb:volume-nr>94</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>1993</sb:date></sb:issue><sb:pages><sb:first-page>1242</sb:first-page><sb:last-page>1255</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0195"><ce:label>Lloyd James, 1940</ce:label><sb:reference id="sr0195"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Lloyd James</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Speech Signals in Telephony</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1940</sb:date><sb:publisher><sb:name>Pitman &amp; Sons</sb:name><sb:location>London</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0200"><ce:label>Logan et al, 1991</ce:label><sb:reference id="sr0200"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.S.</ce:given-name><ce:surname>Logan</ce:surname></sb:author><sb:author><ce:given-name>S.E.</ce:given-name><ce:surname>Lively</ce:surname></sb:author><sb:author><ce:given-name>D.B.</ce:given-name><ce:surname>Pisoni</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Training Japanese listeners to identify English /r/ and /l/: a first report</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. Acoust. Soc. Am</sb:maintitle></sb:title><sb:volume-nr>89</sb:volume-nr></sb:series><sb:issue-nr>2</sb:issue-nr><sb:date>1991</sb:date></sb:issue><sb:pages><sb:first-page>874</sb:first-page><sb:last-page>886</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0205"><ce:label>Maier et al, 2009</ce:label><sb:reference id="sr0205"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Maier</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>HÃ¶nig</ce:surname></sb:author><sb:author><ce:given-name>V.</ce:given-name><ce:surname>ZeiÃler</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Batliner</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>KÃ¶rner</ce:surname></sb:author><sb:author><ce:given-name>N.</ce:given-name><ce:surname>Yamanaka</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>A language-independent feature set for the automatic evaluation of prosody</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of INTERSPEECH 2009</sb:maintitle></sb:title><sb:date>2009</sb:date></sb:edited-book><sb:pages><sb:first-page>600</sb:first-page><sb:last-page>603</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0210"><ce:label>Mostow, Duong, 2009</ce:label><sb:reference id="sr0210"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Mostow</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Duong</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Automated assessment of oral reading prosody</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the 14th International Conference on Artificial Intelligence in Education (AIED 2009)</sb:maintitle></sb:title><sb:date>2009</sb:date></sb:edited-book><sb:pages><sb:first-page>189</sb:first-page><sb:last-page>196</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0215"><ce:label>Munro, Derwing, 1995</ce:label><sb:reference id="sr0215"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.J.</ce:given-name><ce:surname>Munro</ce:surname></sb:author><sb:author><ce:given-name>T.M.</ce:given-name><ce:surname>Derwing</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Foreign accent, comprehensibility and intelligibility in the speech of second language learners</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>Lang. Learn</sb:maintitle></sb:title><sb:volume-nr>45</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>1995</sb:date></sb:issue><sb:pages><sb:first-page>73</sb:first-page><sb:last-page>97</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0220"><ce:label>Pierrehumbert, 1980</ce:label><sb:reference id="sr0220"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.B.</ce:given-name><ce:surname>Pierrehumbert</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The Phonology and Phonetics of English Intonation</sb:maintitle></sb:title></sb:contribution><sb:comment>Ph.D Dissertation; MIT</sb:comment><sb:host><sb:book><sb:date>1980</sb:date></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0225"><ce:label>Pike, 1945</ce:label><sb:reference id="sr0225"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Pike</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The Intonation of American English</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>1945</sb:date><sb:publisher><sb:name>University of Michigan Press</sb:name><sb:location>Ann Arbor</sb:location></sb:publisher></sb:book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0230"><ce:label>Qian et al, 2010</ce:label><sb:reference id="sr0230"><sb:contribution langtype="en"><sb:authors><sb:author><ce:surname>Qian</ce:surname><ce:given-name>Y.</ce:given-name></sb:author><sb:author><ce:surname>Wu</ce:surname><ce:given-name>Z.</ce:given-name></sb:author><sb:author><ce:surname>Ma</ce:surname><ce:given-name>X.</ce:given-name></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Soong</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Automatic prosody prediction and detection with Conditional Random Field (CRF) models</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of ISCSLP 2010</sb:maintitle></sb:title><sb:date>2010</sb:date></sb:edited-book><sb:pages><sb:first-page>135</sb:first-page><sb:last-page>138</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0235"><ce:label>Rangarajan Sridhar et al, 2008</ce:label><sb:reference id="sr0235"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>V.</ce:given-name><ce:surname>Rangarajan Sridhar</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Bangalore</ce:surname></sb:author><sb:author><ce:given-name>S.S.</ce:given-name><ce:surname>Narayanan</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Exploiting acoustic and syntactic features for automatic prosody labeling in a maximum entropy framework</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEEE Trans. Audio Speech Lang. Process</sb:maintitle></sb:title><sb:volume-nr>16</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>797</sb:first-page><sb:last-page>811</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0240"><ce:label>Roach, 1982</ce:label><sb:reference id="sr0240"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>P.</ce:given-name><ce:surname>Roach</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>On the distinction between âstress-timedâ and âsyllable-timedâ languages</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:editors><sb:editor><ce:given-name>D.</ce:given-name><ce:surname>Crystal</ce:surname></sb:editor></sb:editors><sb:title><sb:maintitle>Linguistic Controversies</sb:maintitle></sb:title><sb:date>1982</sb:date><sb:publisher><sb:name>Edward Arnold</sb:name><sb:location>London</sb:location></sb:publisher></sb:edited-book></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0245"><ce:label>Silverman et al, 1992</ce:label><sb:reference id="sr0245"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Silverman</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Beckman</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Pitrelli</ce:surname></sb:author><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Ostendorf</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Pierrehumbert</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Hirschberg</ce:surname></sb:author><sb:et-al/></sb:authors><sb:title><sb:maintitle>TOBI: a standard scheme for labeling prosody</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of the International Conference on Spoken Language 92</sb:maintitle></sb:title><sb:date>1992</sb:date></sb:edited-book><sb:pages><sb:first-page>563</sb:first-page><sb:last-page>566</sb:last-page></sb:pages></sb:host><sb:comment>Banff</sb:comment></sb:reference></ce:bib-reference><ce:bib-reference id="bib0250"><ce:label>Sitaram et al, 2011</ce:label><sb:reference id="sr0250"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Sitaram</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Mostow</ce:surname></sb:author><sb:author><ce:surname>Li</ce:surname><ce:given-name>Y.</ce:given-name></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Weinstein</ce:surname></sb:author><sb:author><ce:given-name>D.</ce:given-name><ce:surname>Yen</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Valeri</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>What visual feedback should a reading tutor give children on their oral reading prosody?</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of SLaTE 2011</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book><sb:pages><sb:first-page>24</sb:first-page><sb:last-page>26</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0255"><ce:label>Suzuki et al, 2008</ce:label><sb:reference id="sr0255"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>M.</ce:given-name><ce:surname>Suzuki</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Konno</ce:surname></sb:author><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Ito</ce:surname></sb:author><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Makino</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Automatic evaluation system of English prosody based on word importance factor</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. Systemics Cybern. Inform</sb:maintitle></sb:title><sb:volume-nr>6</sb:volume-nr></sb:series><sb:issue-nr>4</sb:issue-nr><sb:date>2008</sb:date></sb:issue><sb:pages><sb:first-page>83</sb:first-page><sb:last-page>90</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0260"><ce:label>Tajima et al, 1997</ce:label><sb:reference id="sr0260"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Tajima</ce:surname></sb:author><sb:author><ce:given-name>R.</ce:given-name><ce:surname>Port</ce:surname></sb:author><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Dalby</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Effects of temporal correction on intelligibility of foreign-accented English</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. Phon</sb:maintitle></sb:title><sb:volume-nr>25</sb:volume-nr></sb:series><sb:issue-nr>1</sb:issue-nr><sb:date>1997</sb:date></sb:issue><sb:pages><sb:first-page>10</sb:first-page><sb:last-page>24</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0265"><ce:label>Teixeira et al, 2000</ce:label><sb:reference id="sr0265"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>C.</ce:given-name><ce:surname>Teixeira</ce:surname></sb:author><sb:author><ce:given-name>H.</ce:given-name><ce:surname>Franco</ce:surname></sb:author><sb:author><ce:given-name>E.</ce:given-name><ce:surname>Shriberg</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Precoda</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>SÃ¶nmez</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Prosodic features for automatic text-independent evaluation of degree of nativeness for language learners</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of ICSLP</sb:maintitle></sb:title><sb:date>2000</sb:date></sb:edited-book><sb:pages><sb:first-page>187</sb:first-page><sb:last-page>190</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0270"><ce:label>Tepperman et al, 2010</ce:label><sb:reference id="sr0270"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>J.</ce:given-name><ce:surname>Tepperman</ce:surname></sb:author><sb:author><ce:given-name>T.</ce:given-name><ce:surname>Stanley</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Hacioglu</ce:surname></sb:author><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Pellom</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Testing suprasegmental English through parroting</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of Speech Prosody</sb:maintitle></sb:title><sb:date>2010</sb:date></sb:edited-book><sb:pages><sb:first-page>2010</sb:first-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0275"><ce:label>Wenk, Wiolland, 1982</ce:label><sb:reference id="sr0275"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>B.</ce:given-name><ce:surname>Wenk</ce:surname></sb:author><sb:author><ce:given-name>F.</ce:given-name><ce:surname>Wiolland</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Is French really syllable-timed?</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>J. Phon</sb:maintitle></sb:title><sb:volume-nr>10</sb:volume-nr></sb:series><sb:date>1982</sb:date></sb:issue><sb:pages><sb:first-page>193</sb:first-page><sb:last-page>216</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0280"><ce:label>Wennerstrom, 2000</ce:label><sb:reference id="sr0280"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>A.</ce:given-name><ce:surname>Wennerstrom</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>The role of Intonation in Second Language Fluency. Perspectives on Fluency</sb:maintitle></sb:title></sb:contribution><sb:host><sb:book><sb:date>2000</sb:date><sb:publisher><sb:name>University of Michigan Press</sb:name><sb:location>Ann Arbor</sb:location></sb:publisher></sb:book><sb:pages><sb:first-page>102</sb:first-page><sb:last-page>127</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0285"><ce:label>Witt, Young, 1997</ce:label><sb:reference id="sr0285"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>S.</ce:given-name><ce:surname>Witt</ce:surname></sb:author><sb:author><ce:given-name>S.J.</ce:given-name><ce:surname>Young</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Language learning based on non-native speech recognition</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of ICASSP 1997</sb:maintitle></sb:title><sb:date>1997</sb:date></sb:edited-book><sb:pages><sb:first-page>633</sb:first-page><sb:last-page>636</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0290"><ce:label>Yamashita et al, 2005</ce:label><sb:reference id="sr0290"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>Y.</ce:given-name><ce:surname>Yamashita</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Kato</ce:surname></sb:author><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Nozawa</ce:surname></sb:author></sb:authors><sb:title><sb:maintitle>Automatic scoring for prosodic proficiency of English sentences spoken by Japanese based on utterance comparison</sb:maintitle></sb:title></sb:contribution><sb:host><sb:issue><sb:series><sb:title><sb:maintitle>IEICE Trans. Inf. &amp; Syst</sb:maintitle></sb:title><sb:volume-nr>E88-D</sb:volume-nr></sb:series><sb:issue-nr>3</sb:issue-nr><sb:date>2005</sb:date></sb:issue><sb:pages><sb:first-page>496</sb:first-page><sb:last-page>501</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference><ce:bib-reference id="bib0295"><ce:label>Zechner et al, 2011</ce:label><sb:reference id="sr0295"><sb:contribution langtype="en"><sb:authors><sb:author><ce:given-name>K.</ce:given-name><ce:surname>Zechner</ce:surname></sb:author><sb:author><ce:surname>Xi</ce:surname><ce:given-name>X.</ce:given-name></sb:author><sb:author><ce:surname>Chen</ce:surname><ce:given-name>L.</ce:given-name></sb:author></sb:authors><sb:title><sb:maintitle>Evaluating prosodic features for automated scoring of non-native read speech</sb:maintitle></sb:title></sb:contribution><sb:host><sb:edited-book><sb:title><sb:maintitle>Proceedings of 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</sb:maintitle></sb:title><sb:date>2011</sb:date></sb:edited-book><sb:pages><sb:first-page>461</sb:first-page><sb:last-page>466</sb:last-page></sb:pages></sb:host></sb:reference></ce:bib-reference></ce:bibliography-sec></ce:bibliography></tail></article></xocs:serial-item></xocs:doc></originalText></full-text-retrieval-response>