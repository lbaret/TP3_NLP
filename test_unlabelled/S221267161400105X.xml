<full-text-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/article/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:bk="http://www.elsevier.com/xml/bk/dtd"><coredata><prism:url>http://api.elsevier.com/content/article/pii/S221267161400105X</prism:url><dc:identifier>doi:10.1016/j.aasri.2014.09.005</dc:identifier><eid>1-s2.0-S221267161400105X</eid><prism:doi>10.1016/j.aasri.2014.09.005</prism:doi><pii>S2212-6716(14)00105-X</pii><dc:title>Comparison of SIFT and SURF Methods for Use on Hand Gesture Recognition based on Depth Map </dc:title><prism:publicationName>AASRI Procedia</prism:publicationName><prism:aggregationType>Journal</prism:aggregationType><prism:issn>22126716</prism:issn><prism:volume>9</prism:volume><prism:startingPage>19</prism:startingPage><prism:endingPage>24</prism:endingPage><prism:pageRange>19-24</prism:pageRange><dc:format>text/xml</dc:format><prism:coverDate>2014-12-31</prism:coverDate><prism:coverDisplayDate>2014</prism:coverDisplayDate><prism:copyright>Copyright © 2014 The Authors. Published by Elsevier B.V.</prism:copyright><prism:publisher>The Authors. Published by Elsevier B.V.</prism:publisher><prism:issueName>2014 AASRI Conference on Circuit and Signal Processing (CSP 2014)</prism:issueName><dc:creator>Sykora, Peter</dc:creator><dc:creator>Kamencay, Patrik</dc:creator><dc:creator>Hudec, Robert</dc:creator><dc:description>AbstractIn this paper a comparison between two popular feature extraction methods is presented. Scale-invariant feature transform (or SIFT) is the first method. The Speeded up robust features (or SURF) is presented as second. These two methods are tested on set of depth maps. Ten defined gestures of left hand are in these depth maps. The Microsoft Kinect camera is used for capturing the images [1]. The Support vector machine (or SVM) is used as classification method. The results are accuracy of SVM prediction on selected images.</dc:description><openaccess>1</openaccess><openaccessArticle>true</openaccessArticle><openaccessType>Full</openaccessType><openArchiveArticle>false</openArchiveArticle><openaccessSponsorName/><openaccessSponsorType>ElsevierWaived</openaccessSponsorType><openaccessUserLicense>http://creativecommons.org/licenses/by-nc-nd/3.0/</openaccessUserLicense><dcterms:subject>SIFT</dcterms:subject><dcterms:subject>SURF</dcterms:subject><dcterms:subject>SVM</dcterms:subject><dcterms:subject>Kinect</dcterms:subject><dcterms:subject>depth map</dcterms:subject><dcterms:subject>hand gesture</dcterms:subject><dcterms:subject>recognition</dcterms:subject><link rel="self" href="http://api.elsevier.com/content/article/pii/S221267161400105X"/><link rel="scidir" href="http://www.sciencedirect.com/science/article/pii/S221267161400105X"/></coredata><scopus-id>84937695585</scopus-id><scopus-eid>2-s2.0-84937695585</scopus-eid><link rel="abstract" href="http://api.elsevier.com/content/abstract/scopus_id/84937695585"/><originalText><xocs:doc xmlns:xoe="http://www.elsevier.com/xml/xoe/dtd" xsi:schemaLocation="http://www.elsevier.com/xml/xocs/dtd http://be-prod3a/schema/dtds/document/fulltext/xcr/xocs-article.xsd"><xocs:meta><xocs:content-family>serial</xocs:content-family><xocs:content-type>JL</xocs:content-type><xocs:cid>282179</xocs:cid><xocs:ssids><xocs:ssid type="alllist">291210</xocs:ssid><xocs:ssid type="subj">291791</xocs:ssid><xocs:ssid type="subj">291877</xocs:ssid><xocs:ssid type="subj">291882</xocs:ssid><xocs:ssid type="subj">291883</xocs:ssid><xocs:ssid type="content">31</xocs:ssid><xocs:ssid type="oa">90</xocs:ssid></xocs:ssids><xocs:srctitle>AASRI Procedia</xocs:srctitle><xocs:normalized-srctitle>AASRIPROCEDIA</xocs:normalized-srctitle><xocs:orig-load-date yyyymmdd="20140928">2014-09-28</xocs:orig-load-date><xocs:available-online-date yyyymmdd="20140928">2014-09-28</xocs:available-online-date><xocs:ew-transaction-id>2014-10-13T21:24:07</xocs:ew-transaction-id><xocs:eid>1-s2.0-S221267161400105X</xocs:eid><xocs:pii-formatted>S2212-6716(14)00105-X</xocs:pii-formatted><xocs:pii-unformatted>S221267161400105X</xocs:pii-unformatted><xocs:doi>10.1016/j.aasri.2014.09.005</xocs:doi><xocs:item-stage>S300</xocs:item-stage><xocs:item-version-number>S300.2</xocs:item-version-number><xocs:item-weight>HEAD-AND-TAIL</xocs:item-weight><xocs:hub-eid>1-s2.0-S2212671614X00053</xocs:hub-eid><xocs:timestamp yyyymmdd="20150515">2015-05-15T07:34:50.698394-04:00</xocs:timestamp><xocs:dco>0</xocs:dco><xocs:tomb>0</xocs:tomb><xocs:date-search-begin>20140101</xocs:date-search-begin><xocs:date-search-end>20141231</xocs:date-search-end><xocs:year-nav>2014</xocs:year-nav><xocs:indexeddate epoch="1411937111">2014-09-28T20:45:11.519958Z</xocs:indexeddate><xocs:articleinfo>rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confdate confeditor confloc contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist webpdf webpdfpagecount yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref</xocs:articleinfo><xocs:issns><xocs:issn-primary-formatted>2212-6716</xocs:issn-primary-formatted><xocs:issn-primary-unformatted>22126716</xocs:issn-primary-unformatted></xocs:issns><xocs:crossmark is-crossmark="1">true</xocs:crossmark><xocs:vol-first>9</xocs:vol-first><xocs:volume-list><xocs:volume>9</xocs:volume></xocs:volume-list><xocs:suppl>C</xocs:suppl><xocs:vol-iss-suppl-text>Volume 9</xocs:vol-iss-suppl-text><xocs:sort-order>5</xocs:sort-order><xocs:first-fp>19</xocs:first-fp><xocs:last-lp>24</xocs:last-lp><xocs:pages><xocs:first-page>19</xocs:first-page><xocs:last-page>24</xocs:last-page></xocs:pages><xocs:cover-date-orig><xocs:start-date>2014</xocs:start-date></xocs:cover-date-orig><xocs:cover-date-text>2014</xocs:cover-date-text><xocs:cover-date-start>2014-01-01</xocs:cover-date-start><xocs:cover-date-end>2014-12-31</xocs:cover-date-end><xocs:cover-date-year>2014</xocs:cover-date-year><xocs:title-editors-groups><xocs:title-editors-group><ce:title>2014 AASRI Conference on Circuit and Signal Processing (CSP 2014)</ce:title><xocs:conference-info><xocs:venue>London, UK</xocs:venue><xocs:conference-date-text>21–22 June 2014</xocs:conference-date-text><xocs:date-range><xocs:start-date>20140621</xocs:start-date><xocs:end-date>20140622</xocs:end-date></xocs:date-range></xocs:conference-info><ce:editors><ce:author-group><ce:author><ce:degrees>Dr</ce:degrees><ce:given-name>Wei</ce:given-name><ce:surname>Deng</ce:surname></ce:author><ce:affiliation><ce:textfn>American Applied Sciences Research Institute</ce:textfn></ce:affiliation></ce:author-group></ce:editors></xocs:title-editors-group></xocs:title-editors-groups><xocs:hub-sec><xocs:hub-sec-title>Circuit and Signal Processing</xocs:hub-sec-title></xocs:hub-sec><xocs:document-type>article</xocs:document-type><xocs:document-subtype>fla</xocs:document-subtype><xocs:copyright-line>Copyright © 2014 The Authors. Published by Elsevier B.V.</xocs:copyright-line><xocs:normalized-article-title>COMPARISONSIFTSURFMETHODSFORUSEHANDGESTURERECOGNITIONBASEDDEPTHMAP</xocs:normalized-article-title><xocs:normalized-first-auth-surname>SYKORA</xocs:normalized-first-auth-surname><xocs:normalized-first-auth-initial>P</xocs:normalized-first-auth-initial><xocs:references><xocs:ref-info refid="oref0005"/><xocs:ref-info refid="oref0010"/><xocs:ref-info refid="oref0015"/><xocs:ref-info refid="oref0020"/><xocs:ref-info refid="oref0025"/><xocs:ref-info refid="oref0030"/><xocs:ref-info refid="oref0035"/><xocs:ref-info refid="oref0040"/><xocs:ref-info refid="oref0045"/></xocs:references><xocs:refkeys><xocs:refkey3>SYKORAX2014X19</xocs:refkey3><xocs:refkey4lp>SYKORAX2014X19X24</xocs:refkey4lp><xocs:refkey4ai>SYKORAX2014X19XP</xocs:refkey4ai><xocs:refkey5>SYKORAX2014X19X24XP</xocs:refkey5></xocs:refkeys><xocs:open-access><xocs:oa-article-status is-open-access="1" is-open-archive="0">Full</xocs:oa-article-status><xocs:oa-access-effective-date>2014-09-27T21:03:16Z</xocs:oa-access-effective-date><xocs:oa-sponsor><xocs:oa-sponsor-type>ElsevierWaived</xocs:oa-sponsor-type></xocs:oa-sponsor><xocs:oa-user-license>http://creativecommons.org/licenses/by-nc-nd/3.0/</xocs:oa-user-license><xocs:oa-access-inherited-from winid="http://vtw.elsevier.com/content/oaw/PROC_UNBOUNDED_ESWaived">OA-Window</xocs:oa-access-inherited-from></xocs:open-access><xocs:attachment-metadata-doc><xocs:attachment-set-type>item</xocs:attachment-set-type><xocs:pii-formatted>S2212-6716(14)00105-X</xocs:pii-formatted><xocs:pii-unformatted>S221267161400105X</xocs:pii-unformatted><xocs:eid>1-s2.0-S221267161400105X</xocs:eid><xocs:doi>10.1016/j.aasri.2014.09.005</xocs:doi><xocs:cid>282179</xocs:cid><xocs:timestamp>2014-10-13T17:09:53.934595-04:00</xocs:timestamp><xocs:cover-date-start>2014-01-01</xocs:cover-date-start><xocs:cover-date-end>2014-12-31</xocs:cover-date-end><xocs:attachments><xocs:web-pdf><xocs:attachment-eid>1-s2.0-S221267161400105X-main.pdf</xocs:attachment-eid><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S221267161400105X/MAIN/application/pdf/97b9481bc55e0fa52fca3709049cf97d/main.pdf</xocs:ucs-locator><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S221267161400105X/MAIN/application/pdf/97b9481bc55e0fa52fca3709049cf97d/main.pdf</xocs:ucs-locator><xocs:filename>main.pdf</xocs:filename><xocs:extension>pdf</xocs:extension><xocs:pdf-optimized>true</xocs:pdf-optimized><xocs:filesize>283523</xocs:filesize><xocs:web-pdf-purpose>MAIN</xocs:web-pdf-purpose><xocs:web-pdf-page-count>6</xocs:web-pdf-page-count><xocs:web-pdf-images><xocs:web-pdf-image><xocs:attachment-eid>1-s2.0-S221267161400105X-main_1.png</xocs:attachment-eid><xocs:ucs-locator>https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S221267161400105X/PREVIEW/image/png/53dba2d2c0b23b650b7511d709539da6/main_1.png</xocs:ucs-locator><xocs:ucs-locator>https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S221267161400105X/PREVIEW/image/png/53dba2d2c0b23b650b7511d709539da6/main_1.png</xocs:ucs-locator><xocs:filename>main_1.png</xocs:filename><xocs:extension>png</xocs:extension><xocs:filesize>44541</xocs:filesize><xocs:pixel-height>849</xocs:pixel-height><xocs:pixel-width>656</xocs:pixel-width><xocs:attachment-type>IMAGE-WEB-PDF</xocs:attachment-type><xocs:pdf-page-num>1</xocs:pdf-page-num></xocs:web-pdf-image></xocs:web-pdf-images></xocs:web-pdf></xocs:attachments></xocs:attachment-metadata-doc></xocs:meta><xocs:rawtext> AASRI Procedia   9  ( 2014 )  19 â€“ 24  Available online at www.sciencedirect.com 2212-6716  2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license  (http://creativecommons.org/licenses/by-nc-nd/3.0/). Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute doi: 10.1016/j.aasri.2014.09.005  ScienceDirect 2014 AASRI Conference on Circuits and Signal Processing (CSP 2014)  Comparison of SIFT and SURF Methods for Use on Hand  Gesture Recognition Based on Depth Map  Peter Sykora, Patrik Kamencay, Robert Hudec*  Department of Telecommunications and Multimedia, Faculty of Electrical Engineering, University of Zilina, Univerzitna 8215/1, 01026  Zilina Slovakia    Abstract  In this paper a comparison between two popular feature extraction methods is presented. Scale-invariant feature transform  (or SIFT) is the first method. The Speeded up robust features (or SURF) is presented as second. These two methods are  tested on set of depth maps. Ten defined gestures of left hand are in these depth maps. The Microsoft Kinect camera is  used for capturing the images [1]. The Support vector machine (or SVM) is used as classification method. The results are  accuracy of SVM prediction on selected images.    Â© 2014 Peter Sykora, Patrik Kamencay, Robert Hudec. Published by Elsevier B.V.  Selection and/or peer review under responsibility of American Applied Science Research Institute    Keywords: SIFT; SURF; SVM; Kinect; depth map; hand gesture; recognition  1. Introduction  Gesture recognition is one of the directions in the non verbal machine-human communication. Non verbal  communication can be useful in many life situations (e.i. in situations where human canâ€™t use speech). Several  methods were publicized on topic of gesture recognition [2][3][4]. Many of them focus on feature extraction  from color image of hand and classification as next step. For this, it is important to get the most accurate      * Peter Sykora. Tel.: +421-41-513-2238.  E-mail address: peter.sykora@fel.uniza.sk.   2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license  (http://creativecommons.org/licenses/by-nc-nd/3.0/). Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute 20   Peter Sykora et al. /  AASRI Procedia  9 ( 2014 )  19 â€“ 24  feature vectors. The most common used local visual descriptors are SIFT and SURF [5]. The theory about  these descriptors as well as experiment description is presented in first chapter of this paper. The results of  this experiment are shown next and conclusion at last.  2. Feature extraction and classification methods  In this experiment the database of depth images is created. SIFT and SURF are applied on images of this  database. Resulted feature vectors are divided in to train set and test set. For creation of SVM model the train  set is used in process called training. Next the test set is used for prediction. Results are prediction accuracies  of all tested images for SIFT and SURF.  2.1. SIFT algorithm  Scale invariant feature transform is one of the mostly used local visual descriptors. The method works in  two steps. Detection of feature point as the first step. Feature description as the second step. At the beginning  of procedure the computing of gradient magnitudes and orientations of pixels are computed. This is done in  neighborhood of key point by using the scale of the point. This will make choice on what Gaussian kernel will  be used for blur the image. Feature vector is combination computed from the orientation of histograms within  the sub-regions around the feature point. The feature vector is normalized at least. For more information about  SIFT see [6].  2.2. SURF algorithm  Speeded up robust features descriptor detects feature points by using determinant of the Hessian matrix  H(X, Ïƒ) which is defined as follows                     ,, , xx xy yx yy LX LX HX LX LX Ï‚ Ï‚  Ï‚ â™  â‰¡   â†” â‰ˆ â†� â€¦                                                                                             (1)  Where L is the convolution of the Gaussian second order derivation of image at point X(x,y) in scale Ïƒ and  similarly for L xy  and L yy . For the classification the maxim and minim of the function the discriminant value is  used. The description starts by constructing the window around detected feature point. Orientation of the  window is same as reproducible orientation. From pixel in this region the resulted feature vector is calculated.  For more information about SURF see [7].  2.3. SVM  Support vector machine as part of the model based classifiers uses model for prediction. This model is  created in procedure called training. Model represents each class as pattern in vector space. Each feature  vector is represented as point in feature space. In Fig. 1 only two dimensions are shown for simplicity. Class A  is represented as pattern of gray dots in fig and class B as green dots.     21 Peter Sykora et al. /  AASRI Procedia  9 ( 2014 )  19 â€“ 24    Fig.1. Calculation of best separation.  Separation line is searched in training process. The wider gap between two closes point to line the better  train process. It is seen on fig that line p 2  represents better train process as line p 1  because the gap of line p 2   the v 2  is bigger as gap v 1  of line p 1 . If two patterns canâ€™t be linearly separated, the kernel method is used. This  will transform vector in to higher dimension space in which they are separable. This process will allow  classification of multiple classes [mata6]. In this experiment the RBF kernel method is used. For more  information about SVM see [8].  3. Experimental results  Used input database of depth images and experimental results are described in this chapter. Programming  environment Matlab was used to execute this experiment.  3.1. Image database  Capturing the color image of hand, its followed segmentation can take significant calculation time as well  as processing power. Some methods for segmentation aim for color of skin, as to detect the region of hand.  Results of such process can vary by the light condition or color tone of a particular person. Microsoft Kinect  camera has the advantage that it uses infrared spectrum of light. As such it is invariant to light conditions and  color of skin. Kinect system can track parts of detected human body.     22   Peter Sykora et al. /  AASRI Procedia  9 ( 2014 )  19 â€“ 24    Fig.2. Representation of gestures from cass 1 to 5 (first row) and from 6 to 10 (second row).  Tracked body (or Sceleton) has as its part left and right hand. By locating left hand in the image, the region  of hand can be created. This region is defined as 150x150 pixels square centered on the position of hand. This  acts as simple position segmentation. The resulted depth image (of resolution 150x150 pixels) go throe  threshold filter. Here all pixels with value lover than threshold are set to zero. With this procedure 1500  pictures are captured. There are 150 pictures for 10 classes (see Fig. 2). For the SVM method the database is  divided in to 100 pictures per class for train set and 50 pictures per class for test set. This gives overall 1000  train pictures and 500 test pictures.   3.2. Experimental results  Table 1 contains performance matrix for descriptor SIFT and Table 2 contains data for descriptor SURF.  Each field in this matrix contains the sum of pictures of class represented by its column number, to be  recognized as class represented by row number. For example, in Table 1 for row 8, that is for input images of  class 8, two pictures were wrong recognized as pictures of class 1 and forty-eight pictures were recognized as  pictures of class 8.  Table 1. Performance matrix of resulted accuracy for SIFT descriptor  Output class/Target class 1 2 3 4 5 6 7 8 9 10  1 24 0 0 0 2 0 2 2 1 0  2 0 41 0 0 0 0 25 0 0 0  3 0 2 50 0 0 0 0 0 0 0  4 1 0 0 50 1 0 0 0 0 12  5 21 2 0 0 45 0 3 0 0 1  6 0 0 0 0 0 47 0 0 1 0  7 0 2 0 0 0 0 20 0 0 2  8 2 3 0 0 1 0 0 48 1 0  9 0 0 0 0 1 3 0 0 47 1  10 2 0 0 0 0 0 0 0 0 34    23 Peter Sykora et al. /  AASRI Procedia  9 ( 2014 )  19 â€“ 24  From results it is clear that the invariance to rotation of SIFT descriptor is a disadvantage here. Some hand  gestures, mainly 1 and 5, wave similar shape and occurs as one, only in different orientation. For human mind  it is clear that gesture represented by class 1 has different interpretation that of class 5. Another major error  occurs for class 7 and 2. On both pictures the shape of hand is not the same but it is very similar shape.   For SURF (table 2) the results are similar to the SIFT method. The error occurs in classes with too similar  shape, such as pictures of class 2 and class 7. Another error occurs for classes with the same shape but with  rotation as pictures of class 4 and class 10.   Overall accuracy for SIFT is 81.2% and for SURF it is 82.8%. It is clear, that accuracy can be better if non- invariant to rotation feature extraction method was used.    Table 2. Performance matrix of resulted accuracy for SURF descriptor  Output class/Target class 1 2 3 4 5 6 7 8 9 10  1 40 0 0 0 0 0 0 2 4 0  2 0 46 0 0 0 0 27 0 0 1  3 0 0 48 0 0 0 3 0 0 0  4 0 0 0 50 0 0 0 0 0 16  5 9 1 1 0 50 0 8 0 0 3  6 0 0 0 0 0 50 0 0 4 0  7 0 1 0 0 0 0 12 0 0 0  8 1 2 1 0 0 0 0 48 2 0  9 0 0 0 0 0 0 0 0 40 0  10 0 0 0 0 0 0 0 0 0 30  4. Conclusion  In this paper the comparison between two feature extraction methods was presented. SIFT as the first  method and SURF method as second. They were applied on set of depth map images of left hand gestures.  There were 10 gestures. For capturing these images the Microsoft Kinect camera was used. For image  classification the Support vector machine was used. The experimental results are prediction accuracies of  SVM method on test set images for each descriptor. From the obtained experimental results is evident that  best result using SURF method with accuracy of 82.8% was achieved. Two images capturing the same hand  shape, but with different orientation can be interpreted as two gestures. These methods, SIFT and SURF are  invariant to orientation and thus they are not suited for recognition system of such gestures.  Comparison of others visual descriptors is essential for finding the best candidate for our real-time gesture  recognition system [Sykora2013]. In future work we plan to test these methods on larger test database and  modified feature extraction methods (SIFT, SURF) so they will be non-invariant to orientation.  Acknowledgment  This contribution is the result of the project implementation: Project No. 1/0705/13 "Image elements  classification for semantic image description", supported by the Slovak Science project Grant Agency.  24   Peter Sykora et al. /  AASRI Procedia  9 ( 2014 )  19 â€“ 24  References  [1] Yujie Shen, Zhonghua Hao, Pengfei Wang, Shiwei Ma, Wanquan Liu, A Novel Human Detection  Approach Based on Depth Map via Kinect. Computer Vision and Pattern Recognition Workshops (CVPRW);  2013 IEEE Conference on , vol., no., pp.535,541, 23-28 June 2013.  [2] Panwar  M, Hand gesture recognition based on shape parameters. Computing, Communication and  Applications (ICCCA); 2012 International Conference on , vol., no., pp.1,6, 22-24 Feb. 2012.  [3] Jalal A, Uddin M Z, Kim Tâ€“S, Depth video-based human activity recognition system using translation and  scaling invariant features for life logging at smart home. Consumer Electronics; IEEE Transactions on , vol.58,  no.3, pp.863,871; August 2012.  [4] Wenjun T, Chengdong W, Shuying Z, Li J, Dynamic hand gesture recognition using motion trajectories  and key frames. Advanced Computer Control (ICACC); 2010 2nd International Conference on , vol.3, no.,  pp.163,167, 27-29 March 2010.  [5] S Matuska, R Hudec, M Benco, M Zachariasova, Opponent colour descriptors in object recognition. 15 th   International Conference on Research in Telecommunication Technologies; Senec Slovakia; ISBN 978-80- 227-4026-5; 11-13 Sep. 2013.  [6] Han X, Wenhao H, Kui Y, Feng W, Real-time scene recognition on embedded system with SIFT  keypoints and a new descriptor. Mechatronics and Automation (ICMA); 2013 IEEE International Conference  on , vol., no., pp.1317,1324, 4-7 Aug. 2013.  [7] Zhang H. Hu Q, Fast image matching based-on improved SURF algorithm. Electronics; Communications  and Control (ICECC), 2011 International Conference on , vol., no., pp.1460,1463, 9-11 Sept. 2011.  [8] Soliman O S, Mahmoud A S, A classification system for remote sensing satellite images using support  vector machine with non-linear kernel functions.  Informatics and Systems (INFOS), 2012 8th   International Conference on  , vol., no., pp.BIO-181,BIO-187, 14-16 May 2012.  [9] Sykora P, Hudec R, Benco M, 3D Shape-Motion Detection. TRANSCOM 2013; Zilina; ISBN: 978-80- 554-0692-3; pp.111,114; 24-26 June 2013.   shape but with  rotation as pictures of class 4 and class 10.   Overall accuracy for SIFT is 81.2% and for SURF it is 82.8%. It is clear, that accuracy can be better if non- invariant to rotation feature extraction method was used.    Table 2. Performance matrix of resulted accuracy for SURF descriptor  Output class/Target class 1 2 3 4 5 6 7 8 9 10  1 40 0 0 0 0 0 0 2 4 0  2 0 46 0 0 0 0 27 0 0 1  3 0 0 48 0 0 0 3 0 0 0  4 0 0 0 50 0 0 0 0 0 16  5 9 1 1 0 50 0 8 0 0 3  6 0 0 0 0 0 50 0 0 4 0  7 0 1 0 0 0 0 12 0 0 0  8 1 2 1 0 0 0 0 48 2 0  9 0 0 0 0 0 0 0 0 40 0  10 0 0 0 0 0 0 0 0 0 30  4. Conclusion  In this paper the comparison between two feature extraction methods was presented. SIFT as the first  method and SURF method as second. They were applied on set of depth map images of left hand gestures.  There were 10 gestures. For capturing these images the Microsoft Kinect camera was used. For image  classification the Support vector machine was used. The experimental results are prediction accuracies of  SVM method on test set images for each descriptor. From the obtained experimental results is evident that  best result using SURF method with accuracy of 82.8% was achieved. Two images capturing the same hand  shape, but with different orientation can be interpreted as two gestures. These methods, SIFT and SURF are  invariant to orientation and thus they are not suited for recognition system of such gestures.  Comparison of others visual descriptors is essential for finding the best candidate for our real-time gesture  recognition system [Sykora2013]. In future work we plan to test these methods on larger test database and  modified feature extraction methods (SIFT, SURF) so they will be non-invariant to orientation.  Acknowledgment  This contribution is the result of the project implementation: Project No. 1/0705/13 "Image elements  classification for semantic image description", supported by the Slovak Science project Grant Agency.  24   Peter Sykora et al. /  AASRI Procedia  9 ( 2014 )  19 â€“ 24  References  [1] Yujie Shen, Zhonghua Hao, Pengfei Wang, Shiwei Ma, Wanquan Liu, A Novel Human Detection  Approach Based on Depth Map via Kinect. Computer Vision and Pattern Recognition Workshops (CVPRW);  2013 IEEE Conference on , vol., no., pp.535,541, 23-28 June 2013.  [2] Panwar  M, Hand gesture recognition based on shape parameters. Computing, Communication and  Applications (ICCCA); 2012 International Conference on , vol., no., pp.1,6, 22-24 Feb. 2012.  [3] Jalal A, Uddin M Z, Kim Tâ€“S, Depth video-based human activity recognition system using translation and  scaling invariant features for life logging at smart home. Consumer Electronics; IEEE Transactions on , vol.58,  no.3, pp.863,871; August 2012.  [4] Wenjun T, Chengdong W, Shuying Z, Li J, Dynamic hand gesture recognition using motion trajectories  and key frames. Advanced Computer Control (ICACC); 2010 2nd International Conference on , vol.3, no.,  pp.163,167, 27-29 March 2010.  [5] S Matuska, R Hudec, M Benco, M Zachariasova, Opponent colour descriptors in object recognition. 15 th   International Conference on Research in Telecommunication Technologies; Senec Slovakia; ISBN 978-80- 227-4026-5; 11-13 Sep. 2013.  [6] Han X, Wenhao H, Kui Y, Feng W, Real-time scene recognition on embedded system with SIFT  keypoints and</xocs:rawtext><xocs:serial-item><article xmlns="http://www.elsevier.com/xml/ja/dtd" version="5.2" xml:lang="en" docsubtype="fla"><item-info><jid>AASRI</jid><aid>414</aid><ce:pii>S2212-6716(14)00105-X</ce:pii><ce:doi>10.1016/j.aasri.2014.09.005</ce:doi><ce:copyright type="other" year="2014">The Authors</ce:copyright></item-info><head><ce:article-footnote><ce:label>☆</ce:label><ce:note-para id="npar0005" view="all">Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute.</ce:note-para></ce:article-footnote><ce:title id="tit0005">Comparison of SIFT and SURF Methods for Use on Hand Gesture Recognition based on Depth Map</ce:title><ce:author-group id="aug0005"><ce:author id="aut0005"><ce:given-name>Peter</ce:given-name><ce:surname>Sykora</ce:surname><ce:e-address id="eadd0005" type="email">peter.sykora@fel.uniza.sk</ce:e-address></ce:author><ce:author id="aut0010"><ce:given-name>Patrik</ce:given-name><ce:surname>Kamencay</ce:surname></ce:author><ce:author id="aut0015"><ce:given-name>Robert</ce:given-name><ce:surname>Hudec</ce:surname><ce:cross-ref id="crf0005" refid="cor0005"><ce:sup loc="post">⁎</ce:sup></ce:cross-ref></ce:author><ce:affiliation id="aff0005"><ce:textfn>Department of Telecommunications and Multimedia, Faculty of Electrical Engineering, University of Zilina, Univerzitna 8215/1, 01026 Zilina Slovakia</ce:textfn></ce:affiliation><ce:correspondence id="cor0005"><ce:label>⁎</ce:label><ce:text>Corresponding author. Tel.: +421-41-513-2238.</ce:text></ce:correspondence></ce:author-group><ce:abstract id="abs0005" view="all" class="author"><ce:section-title id="sect0005">Abstract</ce:section-title><ce:abstract-sec id="abst0005" view="all"><ce:simple-para id="spar0005" view="all">In this paper a comparison between two popular feature extraction methods is presented. Scale-invariant feature transform (or SIFT) is the first method. The Speeded up robust features (or SURF) is presented as second. These two methods are tested on set of depth maps. Ten defined gestures of left hand are in these depth maps. The Microsoft Kinect camera is used for capturing the images <ce:cross-ref id="crf0010" refid="bib0005">[1]</ce:cross-ref>. The Support vector machine (or SVM) is used as classification method. The results are accuracy of SVM prediction on selected images.</ce:simple-para></ce:abstract-sec></ce:abstract><ce:keywords id="kwd0005" class="keyword" view="all"><ce:section-title id="sect0010">Keywords</ce:section-title><ce:keyword id="kw0005"><ce:text>SIFT</ce:text></ce:keyword><ce:keyword id="kw0010"><ce:text>SURF</ce:text></ce:keyword><ce:keyword id="kw0015"><ce:text>SVM</ce:text></ce:keyword><ce:keyword id="kw0020"><ce:text>Kinect</ce:text></ce:keyword><ce:keyword id="kw0025"><ce:text>depth map</ce:text></ce:keyword><ce:keyword id="kw0030"><ce:text>hand gesture</ce:text></ce:keyword><ce:keyword id="kw0035"><ce:text>recognition</ce:text></ce:keyword></ce:keywords></head><tail view="all"><ce:bibliography id="bibl0005" view="all"><ce:section-title id="sect0020">References</ce:section-title><ce:bibliography-sec id="bibs0005" view="all"><ce:bib-reference id="bib0005"><ce:label>[1]</ce:label><ce:other-ref id="oref0005"><ce:textref>Yujie Shen, Zhonghua Hao, Pengfei Wang, Shiwei Ma, Wanquan Liu, A Novel Human Detection Approach Based on Depth Map via Kinect. Computer Vision and Pattern Recognition Workshops (CVPRW); 2013 IEEE Conference on , vol., no., pp.535,541, 23-28 June 2013.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0010"><ce:label>[2]</ce:label><ce:other-ref id="oref0010"><ce:textref>Panwar M, Hand gesture recognition based on shape parameters. Computing, Communication and Applications (ICCCA); 2012 International Conference on , vol., no., pp.1,6, 22-24 Feb. 2012.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0015"><ce:label>[3]</ce:label><ce:other-ref id="oref0015"><ce:textref>Jalal A, Uddin M Z, Kim T–S, Depth video-based human activity recognition system using translation and scaling invariant features for life logging at smart home. Consumer Electronics; IEEE Transactions on , vol.58, no.3, pp.863,871; August 2012.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0020"><ce:label>[4]</ce:label><ce:other-ref id="oref0020"><ce:textref>Wenjun T, Chengdong W, Shuying Z, Li J, Dynamic hand gesture recognition using motion trajectories and key frames. Advanced Computer Control (ICACC); 2010 2nd International Conference on , vol.3, no., pp.163,167, 27-29 March 2010.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0025"><ce:label>[5]</ce:label><ce:other-ref id="oref0025"><ce:textref>S Matuska, R Hudec, M Benco, M Zachariasova, Opponent colour descriptors in object recognition. 15th International Conference on Research in Telecommunication Technologies; Senec Slovakia; ISBN 978-80-227-4026-5; 11-13 Sep. 2013.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0030"><ce:label>[6]</ce:label><ce:other-ref id="oref0030"><ce:textref>Han X, Wenhao H, Kui Y, Feng W, Real-time scene recognition on embedded system with SIFT keypoints and a new descriptor. Mechatronics and Automation (ICMA); 2013 IEEE International Conference on , vol., no., pp.1317,1324, 4-7 Aug. 2013.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0035"><ce:label>[7]</ce:label><ce:other-ref id="oref0035"><ce:textref>Zhang H. Hu Q, Fast image matching based-on improved SURF algorithm. Electronics; Communications and Control (ICECC), 2011 International Conference on , vol., no., pp.1460,1463, 9-11 Sept. 2011.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0040"><ce:label>[8]</ce:label><ce:other-ref id="oref0040"><ce:textref>Soliman O S, Mahmoud A S, A classification system for remote sensing satellite images using support vector machine with non-linear kernel functions. Informatics and Systems (INFOS), 2012 8th International Conference on , vol., no., pp.BIO-181,BIO-187, 14-16 May 2012.</ce:textref></ce:other-ref></ce:bib-reference><ce:bib-reference id="bib0045"><ce:label>[9]</ce:label><ce:other-ref id="oref0045"><ce:textref>Sykora P, Hudec R, Benco M, 3D Shape-Motion Detection. TRANSCOM 2013; Zilina; ISBN: 978-80-554-0692-3; pp.111,114; 24-26 June 2013.</ce:textref></ce:other-ref></ce:bib-reference></ce:bibliography-sec></ce:bibliography></tail></article></xocs:serial-item></xocs:doc></originalText></full-text-retrieval-response>